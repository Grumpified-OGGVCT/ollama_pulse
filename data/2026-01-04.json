[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and Python/JS libraries to pull, run, and manage large language models locally.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "self-hosted",
      "REST API",
      "model hub",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama/ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama server.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "npm",
      "Promise-based",
      "chat & embed endpoints"
    ]
  },
  {
    "title": "ollama/ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client with sync/async APIs for chatting, generating, and embedding.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "PyPI",
      "asyncio",
      "streaming",
      "embeddings"
    ]
  },
  {
    "title": "jmorganca/ollama",
    "url": "https://pypi.org/project/ollama/",
    "summary": "PyPI landing page for the official ollama Python package.",
    "source": "pypi",
    "date": "2024-05-15",
    "highlights": [
      "pip install",
      "Python 3.8+",
      "official"
    ]
  },
  {
    "title": "ollama",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "npm landing page for the official ollama JavaScript package.",
    "source": "npm",
    "date": "2024-05-15",
    "highlights": [
      "npm i ollama",
      "TypeScript",
      "ESM/CJS"
    ]
  },
  {
    "title": "langchain-ai/langchain (Ollama integration)",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain LLM and chat components that wrap Ollama endpoints.",
    "source": "github",
    "date": "2024-05-17",
    "highlights": [
      "pip langchain",
      "ChatOllama",
      "embeddings"
    ]
  },
  {
    "title": "ollama-webui/ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Open-source ChatGPT-style web UI for any Ollama-managed model.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "Docker",
      "dark/light mode",
      "multi-model",
      "file upload"
    ]
  },
  {
    "title": "ollama4j/ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Java/Kotlin client library for Ollama with fluent builder API.",
    "source": "github",
    "date": "2024-05-16",
    "highlights": [
      "Maven Central",
      "reactive",
      "embedding support"
    ]
  },
  {
    "title": "richawo/ollama-cli",
    "url": "https://github.com/richawo/ollama-cli",
    "summary": "Enhanced interactive CLI with history and prompt templates for Ollama.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "prompts",
      "history",
      "fzf"
    ]
  },
  {
    "title": "sublime-ollama",
    "url": "https://github.com/b0o/sublime-ollama",
    "summary": "Sublime Text plugin to chat with local Ollama models inside the editor.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "side panel",
      "streaming",
      "snippets"
    ]
  },
  {
    "title": "vscode-ollama",
    "url": "https://github.com/b0o/vscode-ollama",
    "summary": "Visual Studio Code extension that adds an Ollama chat panel.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "sidebar",
      "commands",
      "streaming"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama-rb/ollama-rb",
    "summary": "Community Ruby gem for interacting with Ollama.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "gem install",
      "Faraday",
      "chat & embed"
    ]
  },
  {
    "title": "ollama-cpp",
    "url": "https://github.com/ggerganov/llama.cpp/discussions/ollama",
    "summary": "Discussion thread comparing llama.cpp and Ollama ecosystems.",
    "source": "reddit",
    "date": "2024-05-11",
    "highlights": [
      "performance",
      "quantization",
      "community"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration to use Ollama models as generators or embedders.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip haystack-ai",
      "RAG",
      "pipelines"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ivangreene/ollama-copilot",
    "summary": "GitHub Copilot-like inline suggestions using Ollama models.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "VS Code",
      "inline",
      "local"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/sawa-ko/ollama-discord",
    "summary": "Discord bot that streams Ollama responses into channels or DMs.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "slash commands",
      "streaming",
      "Docker"
    ]
  },
  {
    "title": "ollama-elixir",
    "url": "https://github.com/valian-ca/ollama-elixir",
    "summary": "Elixir client for Ollama with GenServer and streaming support.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Hex.pm",
      "GenServer",
      "LiveView demo"
    ]
  },
  {
    "title": "ollama-go",
    "url": "https://github.com/ollama-go/ollama-go",
    "summary": "Community-maintained Go client for Ollama.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "go get",
      "context",
      "streaming"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official Docker images and compose examples for running Ollama.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "cuda",
      "rocm",
      "compose",
      "rootless"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Helm chart to deploy Ollama on Kubernetes with GPU support.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Helm",
      "GPU nodes",
      "PVC",
      "autoscaling"
    ]
  }
]