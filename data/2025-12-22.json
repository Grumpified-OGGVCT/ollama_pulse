[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 3, and other large language models locally.",
    "source": "github",
    "date": "2024-12-19",
    "highlights": [
      "self-hosted",
      "Docker images",
      "REST/CLI APIs"
    ]
  },
  {
    "title": "langchain-ai/langchain",
    "url": "https://github.com/langchain-ai/langchain",
    "summary": "LangChain\u2019s Ollama integration lets you call locally-running models as standard LangChain LLMs & embeddings.",
    "source": "github",
    "date": "2024-12-15",
    "highlights": [
      "pip install langchain-ollama",
      "streaming support",
      "tool-calling"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://pypi.org/project/ollama/",
    "summary": "Official Python client for Ollama; chat, generate, embed, pull, list, and delete models with a few lines.",
    "source": "pypi",
    "date": "2024-11-30",
    "highlights": [
      "pip install ollama",
      "async support",
      "embeddings"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "Official JavaScript/TypeScript client for Ollama; works in Node, Bun, and modern browsers via fetch.",
    "source": "npm",
    "date": "2024-12-01",
    "highlights": [
      "npm i ollama",
      "Promise & stream APIs",
      "typed"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI for Ollama (now renamed Open WebUI); offers folders, sharing, admin panel, RAG, voice.",
    "source": "github",
    "date": "2024-12-18",
    "highlights": [
      "Docker one-liner",
      "RAG uploads",
      "multi-user"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Clean Java/Kotlin client for Ollama; supports generate, chat, embeddings, model management, and tool calling.",
    "source": "github",
    "date": "2024-12-10",
    "highlights": [
      "Maven Central",
      "reactive streams",
      "modular"
    ]
  },
  {
    "title": "ollama-rs",
    "url": "https://github.com/pepperoni21/ollama-rs",
    "summary": "Async Rust crate for Ollama; provides strongly typed chat, generate, embeddings, and model ops.",
    "source": "github",
    "date": "2024-12-12",
    "highlights": [
      "tokio streams",
      "serde",
      "crates.io"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/pkgs/container/ollama",
    "summary": "Official Ollama Docker image (GPU & CPU) updated nightly; one command to serve any model on port 11434.",
    "source": "github",
    "date": "2024-12-19",
    "highlights": [
      "amd64/arm64",
      "CUDA/ROCm",
      "docker-compose"
    ]
  },
  {
    "title": "chainlit + Ollama",
    "url": "https://github.com/Chainlit/chainlit",
    "summary": "Build shareable LLM apps in minutes; Chainlit has a native Ollama adapter for local model chat & debug UI.",
    "source": "github",
    "date": "2024-12-14",
    "highlights": [
      "pip install chainlit",
      "real-time chat UI",
      "step-by-step tracing"
    ]
  },
  {
    "title": "Ollama discussion: \"What tools are you building?\"",
    "url": "https://www.reddit.com/r/ollama/comments/1hh4e3y/what_tools_are_you_building/",
    "summary": "Community thread where devs share 30+ Ollama projects: CLI helpers, Raycast plugins, Obsidian integrations, etc.",
    "source": "reddit",
    "date": "2024-12-16",
    "highlights": [
      "Raycast extension",
      "Obsidian copilot",
      "Home-Assistant add-on"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ivanfioravivi/ollama-copilot",
    "summary": "VS Code extension that turns Ollama models into a free local GitHub Copilot with inline autocomplete & chat.",
    "source": "github",
    "date": "2024-12-05",
    "highlights": [
      "inline completions",
      "chat sidebar",
      "configurable model"
    ]
  },
  {
    "title": "ollama-workflows",
    "url": "https://github.com/sammcj/ollama-workflows",
    "summary": "GitHub Actions collection to run Ollama-based CI checks (summaries, translations, code-review) on self-hosted runners.",
    "source": "github",
    "date": "2024-12-08",
    "highlights": [
      "GitHub Actions",
      "self-hosted GPU",
      "model cache"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/peterw/ollama-rag",
    "summary": "Minimal Python RAG template using Ollama for local embeddings + chat; includes Streamlit UI and Docker setup.",
    "source": "github",
    "date": "2024-12-11",
    "highlights": [
      "Streamlit UI",
      "Chroma vector DB",
      "pdf ingestion"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/ollama-cli/ollama-cli",
    "summary": "Community-enhanced CLI wrapper (Go) with interactive model picker, conversation history, and prompt templates.",
    "source": "github",
    "date": "2024-12-07",
    "highlights": [
      "fuzzy search models",
      "history sqlite",
      "template library"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with optional GPU node-selector & PVC model cache.",
    "source": "github",
    "date": "2024-12-09",
    "highlights": [
      "Helm",
      "GPU scheduling",
      "model persistence"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration to use Ollama models as generators or embedders in production-grade NLP pipelines.",
    "source": "github",
    "date": "2024-12-13",
    "highlights": [
      "pip install haystack-ai[ollama]",
      "pipeline nodes",
      "RAG ready"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/daethyra/ollama-discord-bot",
    "summary": "Self-hostable Discord bot that streams Ollama replies in chat; supports slash commands, system prompts, threads.",
    "source": "github",
    "date": "2024-12-06",
    "highlights": [
      "slash commands",
      "streaming",
      "thread support"
    ]
  },
  {
    "title": "ollama-n8n-node",
    "url": "https://github.com/Ollama-n8n/ollama-n8n-node",
    "summary": "Custom n8n node to call Ollama chat/generate workflows; enables no-code local LLM automations.",
    "source": "github",
    "date": "2024-12-04",
    "highlights": [
      "n8n community node",
      "no-code",
      "workflow templates"
    ]
  },
  {
    "title": "ollama-huggingface",
    "url": "https://github.com/huggingface/transformers/pull/31500",
    "summary": "Open PR adding Ollama backend to Transformers; lets HF pipelines transparently use Ollama served models.",
    "source": "github",
    "date": "2024-12-03",
    "highlights": [
      "HF integration",
      "pipelines",
      "model cards"
    ]
  },
  {
    "title": "Show HN: I built a Raycast extension for Ollama",
    "url": "https://news.ycombinator.com/item?id=42451234",
    "summary": "HN thread presenting a Raycast extension that offers quick keyboard access to any local Ollama model for summarization or rewrite.",
    "source": "hackernews",
    "date": "2024-12-17",
    "highlights": [
      "Raycast",
      "keyboard shortcut",
      "macOS"
    ]
  }
]