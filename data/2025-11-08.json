[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama project: Get up and running with Llama 3, Mistral, Gemma and other large language models locally.",
    "source": "github",
    "date": "2024-04-19",
    "highlights": [
      "self-hosted LLM runner",
      "Docker-based",
      "REST & CLI APIs",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama. Works in Node.js and modern browsers.",
    "source": "github",
    "date": "2024-04-15",
    "highlights": [
      "npm install ollama",
      "Promise-based API",
      "chat & generate endpoints",
      "streaming support"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client library for Ollama. pip install ollama.",
    "source": "github",
    "date": "2024-04-17",
    "highlights": [
      "sync & async APIs",
      "built-in streaming",
      "Pydantic models",
      "chat & embed endpoints"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration for Ollama. Use any Ollama model as an LLM or chat component inside LangChain.",
    "source": "github",
    "date": "2024-04-10",
    "highlights": [
      "pip install langchain-ollama",
      "ChatOllama & OllamaLLM classes",
      "tool calling support"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "ChatGPT-style web UI for Ollama (now renamed Open WebUI). Features multi-model chats, RAG, admin panel.",
    "source": "github",
    "date": "2024-04-18",
    "highlights": [
      "Docker image",
      "document upload & RAG",
      "user roles",
      "themes & plugins"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama. Maven Central artifact net.ollama4j.",
    "source": "github",
    "date": "2024-04-12",
    "highlights": [
      "Java 8+",
      "async & blocking APIs",
      "chat, generate, embeddings",
      "Spring Boot starter"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/gbaptista/ollama-rb",
    "summary": "Ruby gem for Ollama. gem install ollama.",
    "source": "github",
    "date": "2024-04-08",
    "highlights": [
      "Ruby 3.x",
      "Faraday-based",
      "streaming support",
      "Rails friendly"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sugarforever/ollama-cli",
    "summary": "Rich interactive CLI for chatting with Ollama models, built with Node.js and Ink.",
    "source": "github",
    "date": "2024-04-05",
    "highlights": [
      "tab-autocomplete",
      "conversation history",
      "syntax highlighting",
      "npm install -g ollama-cli"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/fredrikaverpil/ollama-copilot",
    "summary": "GitHub Copilot-like VS Code extension that uses local Ollama models for inline completions.",
    "source": "github",
    "date": "2024-04-14",
    "highlights": [
      "VS Code extension",
      "inline suggestions",
      "configurable model",
      "no cloud required"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama-haystack/ollama-haystack",
    "summary": "Haystack integration by deepset. Use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-04-09",
    "highlights": [
      "pip install ollama-haystack",
      "Generator & Embedder components",
      "RAG pipelines"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/ollama/ollama-cookbook",
    "summary": "Community recipes for Ollama: function calling, vision, RAG, fine-tuning, Docker Compose stacks.",
    "source": "github",
    "date": "2024-04-16",
    "highlights": [
      "Jupyter notebooks",
      "OpenAI-compatible server",
      "vision examples",
      "Kubernetes manifests"
    ]
  },
  {
    "title": "ollama-litellm",
    "url": "https://github.com/BerriAI/litellm/tree/main/cookbook/ollama",
    "summary": "LiteLLM proxy to expose Ollama with OpenAI-compatible endpoints for use in any OpenAI client.",
    "source": "github",
    "date": "2024-04-11",
    "highlights": [
      "drop-in replacement",
      "pip install litellm[proxy]",
      "multi-model gateway",
      "budget tracking"
    ]
  },
  {
    "title": "ollama-vscode",
    "url": "https://github.com/mshd/ollama-vscode",
    "summary": "VS Code extension to chat with Ollama models inside the editor sidebar.",
    "source": "github",
    "date": "2024-04-07",
    "highlights": [
      "sidebar panel",
      "custom prompts",
      "code insertion",
      "multi-model switcher"
    ]
  },
  {
    "title": "ollama-gui",
    "url": "https://github.com/jowilf/ollama-gui",
    "summary": "Cross-platform desktop GUI for Ollama built with Flutter. Windows/macOS/Linux binaries available.",
    "source": "github",
    "date": "2024-04-06",
    "highlights": [
      "Flutter desktop",
      "markdown chat",
      "model manager",
      "offline installer"
    ]
  },
  {
    "title": "ollama-nvim",
    "url": "https://github.com/nomnivore/ollama-nvim",
    "summary": "Neovim plugin for Ollama. Generate, refactor, and chat about code without leaving the editor.",
    "source": "github",
    "date": "2024-04-13",
    "highlights": [
      "Lua config",
      "telescope picker",
      "streaming into buffer",
      "custom prompts"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama-docker-compose",
    "summary": "Official Docker Compose examples: CPU-only, NVIDIA GPU, AMD ROCm, and Open WebUI stack.",
    "source": "github",
    "date": "2024-04-18",
    "highlights": [
      "one-command stacks",
      "GPU passthrough",
      "volume mounts",
      "production ready"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community Helm chart for deploying Ollama on Kubernetes with GPU support and autoscaling.",
    "source": "github",
    "date": "2024-04-04",
    "highlights": [
      "Helm 3",
      "GPU node selector",
      "ingress & PVC",
      "custom models init container"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Rust crate for Ollama. Async client using reqwest. crates.io/ollama-rs.",
    "source": "github",
    "date": "2024-04-03",
    "highlights": [
      "async/await",
      "tokio runtime",
      "chat & generate",
      "embeddings support"
    ]
  },
  {
    "title": "ollama-go",
    "url": "https://github.com/ollama/ollama-go",
    "summary": "Official Go client for Ollama. go get github.com/ollama/ollama/go.",
    "source": "github",
    "date": "2024-04-16",
    "highlights": [
      "idiomatic Go",
      "streaming responses",
      "context cancellation",
      "embed package"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/mxyng/ollama-discord-bot",
    "summary": "Discord bot that lets servers chat with local Ollama models. Slash commands and thread support.",
    "source": "github",
    "date": "2024-04-02",
    "highlights": [
      "slash commands",
      "thread isolation",
      "role-based access",
      "Docker image"
    ]
  }
]