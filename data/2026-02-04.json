[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and Python/JavaScript libraries for downloading and running Llama 2, Mistral, Gemma, and other large language models locally with GPU/CPU acceleration.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "CLI",
      "Docker",
      "macOS/Linux/Windows",
      "REST API",
      "GGUF support"
    ]
  },
  {
    "title": "ollama/ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama server; chat, generate, embed, pull, and manage models from Node or the browser.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "npm install ollama",
      "Promise-based",
      "streaming",
      "ESM/CJS"
    ]
  },
  {
    "title": "ollama/ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client library that wraps the Ollama REST API; supports sync/async, streaming, embeddings, and tool calling.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "pip install ollama",
      "asyncio",
      "Pydantic models",
      "Jupyter friendly"
    ]
  },
  {
    "title": "jmorganca/ollama",
    "url": "https://pypi.org/project/ollama/",
    "summary": "PyPI landing page for the official ollama Python package; same codebase as ollama/ollama-python repo.",
    "source": "pypi",
    "date": "2024-05-18",
    "highlights": [
      "pip installable",
      "0.1.x series",
      "wheel available"
    ]
  },
  {
    "title": "ollama on npm",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "Official npm package for the JavaScript client; zero native dependencies, works in Node \u226518 and modern browsers.",
    "source": "npm",
    "date": "2024-05-15",
    "highlights": [
      "TypeScript types",
      "ESM default",
      "0.5.x series"
    ]
  },
  {
    "title": "langchain-ai/langchain-community : Ollama integration",
    "url": "https://github.com/langchain-ai/langchain-community/tree/master/libs/community/langchain_community/llms/ollama",
    "summary": "LangChain LLM and chat wrappers that connect to a local Ollama server; supports streaming, JSON mode, and tool use.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "pip install langchain-community",
      "ChatOllama",
      "OllamaEmbeddings"
    ]
  },
  {
    "title": "langchain-ai/langchainjs : Ollama integration",
    "url": "https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-ollama",
    "summary": "LangChain.js package providing ChatOllama and OllamaEmbeddings classes for Node and edge runtimes.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "npm @langchain/ollama",
      "streaming",
      "tool calling",
      "Vercel Edge"
    ]
  },
  {
    "title": "ollama-webui/ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Open-source ChatGPT-style web interface for Ollama; manage models, chat in threads, upload docs, and use plugins.",
    "source": "github",
    "date": "2024-05-17",
    "highlights": [
      "Docker image",
      "file upload",
      "RAG",
      "admin panel",
      "dark/light themes"
    ]
  },
  {
    "title": "otiai10/ollama-dl",
    "url": "https://github.com/otiai10/ollama-dl",
    "summary": "Lightweight Go CLI to download GGUF models from Hugging Face and import them into Ollama without running the server.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Go binary",
      "resume partial",
      "HF hub",
      "no Docker needed"
    ]
  },
  {
    "title": "sammcj/ollama-runner",
    "url": "https://github.com/sammcj/ollama-runner",
    "summary": "GitHub Action that spins up Ollama service inside CI pipelines so you can run model-based tests or benchmarks.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "GitHub Actions",
      "GPU runners",
      "model caching",
      "matrix testing"
    ]
  },
  {
    "title": "mxyng/ollama-copilot",
    "url": "https://github.com/mxyng/ollama-copilot",
    "summary": "Experimental GitHub Copilot proxy that routes completion requests to a local Ollama model instead of OpenAI.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Copilot proxy",
      "VS Code",
      "self-hosted",
      "FIM completions"
    ]
  },
  {
    "title": "thomas-michael-wallace/ollama-express",
    "url": "https://github.com/thomas-michael-wallace/ollama-express",
    "summary": "Express.js middleware that exposes an OpenAI-compatible REST endpoint backed by Ollama for drop-in replacement.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "/v1/chat/completions",
      "streaming",
      "Docker",
      "env config"
    ]
  },
  {
    "title": "continuedev/continue : Ollama provider",
    "url": "https://github.com/continuedev/continue/tree/main/core/llm/ollama.ts",
    "summary": "Continue VS Code/JetBrains plugin supports Ollama as a local provider for inline code generation and chat.",
    "source": "github",
    "date": "2024-05-16",
    "highlights": [
      "IDE extension",
      "inline autocomplete",
      "chat sidebar",
      "config.json"
    ]
  },
  {
    "title": "r2d4/ollama-model-manager",
    "url": "https://github.com/r2d4/ollama-model-manager",
    "summary": "Kubernetes operator that automatically downloads and caches Ollama models into cluster storage for scalable serving.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "K8s CRD",
      "Helm chart",
      "model versioning",
      "PVC cache"
    ]
  },
  {
    "title": "nickwalton/ollama-rag",
    "url": "https://github.com/nickwalton/ollama-rag",
    "summary": "Simple Python repo showing how to build a RAG pipeline with Ollama embeddings and Chroma vector store.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "PDF ingestion",
      "ChromaDB",
      "streamlit UI",
      "sentence splitting"
    ]
  },
  {
    "title": "ollama-r/ollama",
    "url": "https://github.com/ollama-r/ollama",
    "summary": "Unofficial R client for Ollama REST API; chat, generate, and pull models from R scripts or Shiny apps.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "CRAN-style",
      "httr2",
      "Shiny demo",
      "RMarkdown"
    ]
  },
  {
    "title": "pepperoni21/ollama-rs",
    "url": "https://github.com/pepperoni21/ollama-rs",
    "summary": "Community Rust crate providing async wrappers for the Ollama API with Tokio and Serde.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "crates.io",
      "async/await",
      "streaming",
      "examples"
    ]
  },
  {
    "title": "tgbyte/ollama-grpc",
    "url": "https://github.com/tgbyte/ollama-grpc",
    "summary": "gRPC bridge that translates Ollama REST calls into protobuf services for internal micro-service usage.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "protobuf",
      "Go server",
      "code generation",
      "mutual TLS"
    ]
  },
  {
    "title": "defense-unicorns/uds-ollama",
    "url": "https://github.com/defense-unicorns/uds-ollama",
    "summary": "Defense Unicorn\u2019s secure Kubernetes package bundling Ollama with Chainguard images and Zarf deployment.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Zarf package",
      "Chainguard",
      "SBOM",
      "air-gap install"
    ]
  },
  {
    "title": "ollama discussion: \u2018Show HN: Self-hosted Copilot with Ollama\u2019",
    "url": "https://news.ycombinator.com/item?id=40351234",
    "summary": "Hacker News thread where developers share tools and tips for replacing GitHub Copilot with local Ollama models.",
    "source": "hackernews",
    "date": "2024-05-08",
    "highlights": [
      "community tips",
      "VS Code setup",
      "latency benchmarks"
    ]
  }
]