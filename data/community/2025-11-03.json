[
  {
    "title": "Ollama \u2013 Run Llama 2 and other LLMs locally with one command",
    "url": "https://news.ycombinator.com/item?id=37443013",
    "summary": "HN thread covering first impressions, GPU/VRAM usage, model download speeds, and comparisons to llama.cpp and text-generation-webui.",
    "source": "hackernews",
    "date": "2023-08-22",
    "highlights": [
      "single-binary install",
      "macOS + Linux support",
      "Modelfile syntax",
      "GPU off-load"
    ]
  },
  {
    "title": "r/LocalLLaMA - Ollama beginner experience + benchmark numbers",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/15x6k9s/ollama_beginner_experience_benchmark_numbers/",
    "summary": "User shares first-day setup on M1 Pro, 7B/13B token/s metrics, and tips for keeping GGUF files cached.",
    "source": "reddit",
    "date": "2023-07-21",
    "highlights": [
      "M1 GPU utilisation 95%",
      "13B model at 28 tokens/s",
      "docker-compose snippet"
    ]
  },
  {
    "title": "r/ollama - What tools are you integrating with Ollama?",
    "url": "https://www.reddit.com/r/ollama/comments/17t8p2q/what_tools_are_you_integrating_with_ollama/",
    "summary": "Community thread listing Obsidian, Emacs, Raycast, Alfred, OpenAI-compatible proxies, and LangChain integrations.",
    "source": "reddit",
    "date": "2023-11-02",
    "highlights": [
      "OpenAI REST drop-in",
      "Obsidian Copilot plugin",
      "Raycast extension"
    ]
  },
  {
    "title": "r/LocalLLaMA - Ollama vs text-generation-webui feature showdown",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/16b0q5r/ollama_vs_textgenerationwebui_which_do_you_use/",
    "summary": "Side-by-side discussion of API ergonomics, extensibility, GPU scheduling, and update cadence.",
    "source": "reddit",
    "date": "2023-08-30",
    "highlights": [
      "REST API simplicity",
      "smaller memory footprint",
      "lacks built-in chat UI"
    ]
  },
  {
    "title": "Ollama Python & JavaScript SDKs walkthrough - YouTube",
    "url": "https://www.youtube.com/watch?v=Kq6N0x3pWzA",
    "summary": "15-min tutorial showing pip install ollama, streaming chat completions, and building a Slack bot.",
    "source": "youtube",
    "date": "2023-09-12",
    "highlights": [
      "ollama.chat() streaming",
      "custom Modelfiles",
      "Slack Bolt integration"
    ]
  },
  {
    "title": "r/ollama - Show-off your custom Modelfiles",
    "url": "https://www.reddit.com/r/ollama/comments/18mxw7u/showoff_your_custom_modelfiles/",
    "summary": "Users share tuned prompts, system messages and parameter sets for CodeLlama, Llava, and Mistral.",
    "source": "reddit",
    "date": "3 Dec 2023",
    "highlights": [
      "temperature scheduling",
      "template variables",
      "multi-line system prompts"
    ]
  },
  {
    "title": "Hacker News - Ollama 0.1.14 adds multi-model GPU scheduling",
    "url": "https://news.ycombinator.com/item?id=38472911",
    "summary": "Discussion of new multi-model support, concurrent loading, and VRAM budgeting.",
    "source": "hackernews",
    "date": "2023-10-19",
    "highlights": [
      "concurrent model serving",
      "VRAM budgeting",
      "zero-copy model switching"
    ]
  },
  {
    "title": "r/selfhosted - Ollama + Open-WebUI = private ChatGPT alternative",
    "url": "https://www.reddit.com/r/selfhosted/comments/18q5vte/ollama_openwebui_a_private_chatgpt_alternative/",
    "summary": "Step-by-step docker-compose for local network deployment with user auth and model management UI.",
    "source": "reddit",
    "date": "2023-12-07",
    "highlights": [
      "one-line docker-compose",
      "Open-WebUI frontend",
      "local network only"
    ]
  },
  {
    "title": "YouTube - Run Llava vision models locally with Ollama",
    "url": "https://www.youtube.com/watch?v=5tMFI3eRLew",
    "summary": "10-min demo of pulling llava:7b-v1.5, sending base64 images, and getting captions via CLI.",
    "source": "youtube",
    "date": "2023-11-15",
    "highlights": [
      "vision model support",
      "CLI base64 input",
      "caption generation"
    ]
  },
  {
    "title": "r/ollama - Benchmark request: Apple M3 vs M1 token throughput",
    "url": "https://www.reddit.com/r/ollama/comments/17r2xyz/benchmark_request_apple_m3_vs_m1_token_throughput/",
    "summary": "Users post side-by-side benchmarks of 7B and 13B models on M1, M2 and M3 chips.",
    "source": "reddit",
    "date": "2023-10-31",
    "highlights": [
      "M3 15% faster",
      "uniform 7B ~45 tokens/s",
      "temperature throttling notes"
    ]
  },
  {
    "title": "Hacker News - Show HN: Ollama-WebUI \u2013 self-hosted chat interface",
    "url": "https://news.ycombinator.com/item?id=38749022",
    "summary": "Developer launches open-source chat UI that proxies to Ollama, gaining 400 GitHub stars in 48h.",
    "source": "hackernews",
    "date": "2023-11-05",
    "highlights": [
      "OpenAI-compatible endpoints",
      "dark/light themes",
      "import/export chats"
    ]
  },
  {
    "title": "r/LocalLLaMA - Tips for running Ollama on CPU-only cloud boxes",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/16zj9k5/tips_for_running_ollama_on_cpu_only_cloud_boxes/",
    "summary": "Users share 7B quant choices, num_thread tuning, and 4-bit matrix multi-thread gains.",
    "source": "reddit",
    "date": "2023-10-02",
    "highlights": [
      "4-bit quant 3GB RAM",
      "num_thread 8 optimal",
      "AVX2 speed-up"
    ]
  },
  {
    "title": "YouTube - Building a private coding assistant with Ollama and Continue",
    "url": "https://www.youtube.com/watch?v=2eD9D9XwM9s",
    "summary": "Live-coding session integrating CodeLlama inside VS Code via Continue extension.",
    "source": "youtube",
    "date": "2023-10-26",
    "highlights": [
      "Continue.dev setup",
      "/ollama command",
      "inline code completion"
    ]
  },
  {
    "title": "r/ollama - Windows preview is out (experimental)",
    "url": "https://www.reddit.com/r/ollama/comments/18w2a7z/windows_preview_is_out_experimental/",
    "summary": "Early adopters report WSL2-less exe, GPU passthrough issues, and workaround scripts.",
    "source": "reddit",
    "date": "2023-12-15",
    "highlights": [
      "native Windows exe",
      "CUDA support",
      "WSL no longer required"
    ]
  },
  {
    "title": "Hacker News - Ollama adds OpenAI-compatible REST endpoints",
    "url": "https://news.ycombinator.com/item?id=39012345",
    "summary": "Thread on dropping in Ollama as a private backend for apps written for OpenAI.",
    "source": "hackernews",
    "date": "2023-12-01",
    "highlights": [
      "/v1/chat/completions",
      "drop-in replacement",
      "function calling preview"
    ]
  },
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: the easiest way to run Llama 2, Mistral, Gemma and other LLMs locally with one command.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "self-contained binary",
      "Docker-style CLI",
      "built-in model registry",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-webui/ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich ChatGPT-style web UI for Ollama that supports multi-model chats, code highlighting, RAG and admin panel.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "React+FastAPI",
      "file upload & RAG",
      "multi-user support",
      "dark/light themes"
    ]
  },
  {
    "title": "jmorganca/ollama-python",
    "url": "https://github.com/jmorganca/ollama-python",
    "summary": "Official Python client library for Ollama with async support and streaming responses.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip install ollama",
      "async/await",
      "streaming JSON",
      "embedding endpoints"
    ]
  },
  {
    "title": "ollama/ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node and browsers to talk with the Ollama API.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "npm install ollama",
      "TypeScript types",
      "browser compatible",
      "streaming support"
    ]
  },
  {
    "title": "langchain-ollama integration",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain documentation page showing how to use Ollama as an LLM backend for chains, agents and retrieval.",
    "source": "blog",
    "date": "2024-05-08",
    "highlights": [
      "one-line swap",
      "streaming tokens",
      "native async",
      "callback handlers"
    ]
  },
  {
    "title": "ollama-ai/ollama-ai",
    "url": "https://github.com/ollama-ai/ollama-ai",
    "summary": "Unofficial Rust crate that wraps the Ollama REST API with strongly-typed models and Tokio async.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "async-first",
      "Serde types",
      "streaming via SSE",
      "crates.io published"
    ]
  },
  {
    "title": "ollama-hf-cli: push/pull models from Hugging Face",
    "url": "https://github.com/elliotmarks/ollama-hf-cli",
    "summary": "Small CLI that lets you upload and download GGUF quantized models directly between Hugging Face and Ollama registries.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "GGUF auto-convert",
      "HF hub sync",
      "private repo support",
      "CI friendly"
    ]
  },
  {
    "title": "ollama-rag: private RAG pipeline",
    "url": "https://github.com/dmahurin/ollama-rag",
    "summary": "Minimal retrieval-augmented-generation example that ingests PDFs into Chroma and queries via Ollama embeddings.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Chroma vector DB",
      "sentence-transformers",
      "PDF ingestion",
      "streamlit UI"
    ]
  },
  {
    "title": "ollama-discord: Discord bot",
    "url": "https://github.com/ollama-discord/ollama-discord",
    "summary": "Self-hostable Discord bot that brings local Ollama models into any server with slash commands and thread support.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "slash commands",
      "per-thread context",
      "rate limiting",
      "mod roles"
    ]
  },
  {
    "title": "ollama-vscode: Local LLM Copilot",
    "url": "https://github.com/ollama-vscode/ollama-vscode",
    "summary": "VS Code extension that adds inline code completion and chat side-panel powered by any Ollama model.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "inline completions",
      "chat panel",
      "multi-model switch",
      "custom prompts"
    ]
  },
  {
    "title": "HuggingFace GGUF library for Ollama",
    "url": "https://huggingface.co/TheBloke/Llama-2-7B-GGUF",
    "summary": "Popular Hugging Face repo offering Llama-2-7B GGUF quants that can be imported into Ollama with a single Modelfile line.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "Q4_K_M quant",
      "official GGUF",
      "Modelfile example",
      "download stats"
    ]
  },
  {
    "title": "Running Ollama on Jetson Orin",
    "url": "https://developer.nvidia.com/blog/run-ollama-local-llms-on-jetson/",
    "summary": "NVIDIA developer blog post detailing how to build Ollama with CUDA for ARM64 and run 7B-13B models on Jetson edge devices.",
    "source": "blog",
    "date": "2024-04-30",
    "highlights": [
      "CUDA arm64",
      "docker build",
      "13B benchmark",
      "power measurements"
    ]
  },
  {
    "title": "ollama-k8s: Helm chart for Kubernetes",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart that deploys Ollama with GPU node selection, PVC storage and horizontal pod autoscaling.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "GPU node pool",
      "PVC caching",
      "HPA scaling",
      "Prometheus metrics"
    ]
  },
  {
    "title": "ollama-slack: Slack bot integration",
    "url": "https://github.com/alexkim/ollama-slack",
    "summary": "Lightweight Slack bot server that listens for @ mentions and replies using any local Ollama model.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "socket-mode",
      "mention trigger",
      "thread context",
      "env var config"
    ]
  },
  {
    "title": "ollama-cli-chat: Terminal UI",
    "url": "https://github.com/frostime/ollama-cli-chat",
    "summary": "Rich terminal chat client built with Textual that supports markdown, syntax highlighting and conversation history.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "Textual TUI",
      "markdown render",
      "conversation save",
      "model switch hotkey"
    ]
  }
]