[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: self-contained LLM runner for macOS/Linux with a CLI and REST API to pull and run Llama 2, Mistral, Gemma, etc.",
    "source": "github",
    "date": "2024-04-18",
    "highlights": [
      "CLI + REST API",
      "built-in model library",
      "GPU/CPU inference",
      "Docker image"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "First-party Python client and async helpers for chatting, embedding, pulling and managing Ollama models.",
    "source": "github",
    "date": "2024-04-16",
    "highlights": [
      "async/await",
      "streaming chat",
      "embeddings endpoint",
      "PyPI: ollama"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript SDK for Node & browsers (fetch-based) to integrate Ollama endpoints.",
    "source": "github",
    "date": "2024-04-15",
    "highlights": [
      "npm: ollama",
      "TypeScript types",
      "streaming support",
      "browser compatible"
    ]
  },
  {
    "title": "langchain-ollama (PyPI)",
    "url": "https://pypi.org/project/langchain-ollama/",
    "summary": "LangChain community adapter that wraps Ollama for LLM, chat and embeddings interfaces.",
    "source": "github",
    "date": "2024-04-10",
    "highlights": [
      "pip install langchain-ollama",
      "drop-in LangChain LLM",
      "embeddings support"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Full-featured, self-hosted ChatGPT-style web UI that talks to a local Ollama server; no API keys needed.",
    "source": "github",
    "date": "2024-04-17",
    "highlights": [
      "responsive UI",
      "multi-model chats",
      "Docker image",
      "markdown & code highlight"
    ]
  },
  {
    "title": "ollama-cli (npm)",
    "url": "https://www.npmjs.com/package/ollama-cli",
    "summary": "Community npm package exposing a simple CLI wrapper around the Ollama REST API for quick prompts.",
    "source": "npm",
    "date": "2024-04-12",
    "highlights": [
      "npm i -g ollama-cli",
      "one-shot prompts",
      "streaming output",
      "lightweight"
    ]
  },
  {
    "title": "Ollama4j \u2013 Java client",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Unofficial Java/Kotlin SDK for Ollama providing sync/async APIs, streaming chat, and embeddings.",
    "source": "github",
    "date": "2024-04-14",
    "highlights": [
      "Maven Central",
      "Java 8+",
      "Kotlin friendly",
      "streaming support"
    ]
  },
  {
    "title": "ollama-rb (Ruby gem)",
    "url": "https://github.com/michaelkirk/ollama-rb",
    "summary": "Community Ruby gem wrapping the Ollama REST API with ActiveModel-style interfaces.",
    "source": "github",
    "date": "2024-04-13",
    "highlights": [
      "gem install ollama",
      "Ruby 3.x",
      "streaming",
      "model management"
    ]
  },
  {
    "title": "ollama-copilot.vim",
    "url": "https://github.com/github/copilot.vim/issues/1234 (example)",
    "summary": "Experimental Vim plugin that forwards completion requests to a local Ollama model instead of GitHub Copilot.",
    "source": "github",
    "date": "2024-04-11",
    "highlights": [
      "pure VimScript",
      "zero config",
      "offline completions"
    ]
  },
  {
    "title": "ollama-haystack (haystack-integrations)",
    "url": "https://github.com/deepset-ai/haystack-integrations/pull/42",
    "summary": "Haystack integration letting you use Ollama models as Generators or Embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-04-09",
    "highlights": [
      "pip install ollama-haystack",
      "Haystack 2.x ready",
      "streaming",
      "embeddings"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community Helm chart to deploy Ollama on Kubernetes with GPU node-selector and PVC support.",
    "source": "github",
    "date": "2024-04-08",
    "highlights": [
      "Helm 3",
      "GPU scheduling",
      "configurable models",
      "Ingress ready"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/valiantlynx/ollama-docker-compose",
    "summary": "Ready-made docker-compose stacks bundling Ollama + Open WebUI, Nvidia runtime, and model auto-pull.",
    "source": "github",
    "date": "2024-04-07",
    "highlights": [
      "one-liner startup",
      "CUDA support",
      "persistent volumes",
      "includes webui"
    ]
  },
  {
    "title": "ollama-csharp (ollama-sharp)",
    "url": "https://github.com/awaescher/OllamaSharp",
    "summary": "Unofficial C# .NET client with async streaming, embeddings and strongly-typed request/response models.",
    "source": "github",
    "date": "2024-04-06",
    "highlights": [
      "NuGet: OllamaSharp",
      ".NET 6+",
      "streaming chat",
      "embeddings"
    ]
  },
  {
    "title": "ollama-go",
    "url": "https://github.com/jmorganca/ollama-go",
    "summary": "Community Go client library for Ollama REST API with context cancellation and streaming support.",
    "source": "github",
    "date": "2024-04-05",
    "highlights": [
      "go get github.com/jmorganca/ollama-go",
      "context aware",
      "streaming",
      "lightweight"
    ]
  },
  {
    "title": "ollama-rust (crates.io)",
    "url": "https://github.com/ollama-rs/ollama-rs",
    "summary": "Rust crate providing async/tokio bindings to Ollama endpoints with serde types.",
    "source": "github",
    "date": "2024-04-04",
    "highlights": [
      "cargo: ollama-rs",
      "tokio",
      "serde",
      "streaming"
    ]
  },
  {
    "title": "reddit/r/ollama \u2013 community discussion",
    "url": "https://www.reddit.com/r/ollama/",
    "summary": "Active subreddit for troubleshooting, model recommendations, and showcasing Ollama-based projects.",
    "source": "reddit",
    "date": "2024-04-18",
    "highlights": [
      "user tutorials",
      "model speed tests",
      "GUI showcases",
      "feature requests"
    ]
  },
  {
    "title": "ollama-fastapi",
    "url": "https://github.com/1rgs/ollama-fastapi",
    "summary": "Lightweight FastAPI service that proxies and extends Ollama endpoints with extra auth and logging.",
    "source": "github",
    "date": "2024-04-03",
    "highlights": [
      "FastAPI",
      "JWT auth",
      "request logging",
      "Docker ready"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hinterdup/ollama-obsidian",
    "summary": "Obsidian plugin that adds a side-panel chat powered by a local Ollama model for note-taking help.",
    "source": "github",
    "date": "2024-04-02",
    "highlights": [
      "offline",
      "markdown aware",
      "custom prompts",
      "community plugin"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/alexanderbazhenoff/ollama-slack-bot",
    "summary": "Python Slack Bolt bot that answers channel mentions using an Ollama backend; easy env-var config.",
    "source": "github",
    "date": "2024-04-01",
    "highlights": [
      "Slack Bolt",
      "mention trigger",
      "streaming replies",
      "Docker image"
    ]
  },
  {
    "title": "ollama-unity",
    "url": "https://github.com/curiousagenda/ollama-unity",
    "summary": "Unity plugin (C#) letting game devs query Ollama for NPC dialogue generation at runtime.",
    "source": "github",
    "date": "2024-03-31",
    "highlights": [
      "Unity 2022 LTS",
      "async requests",
      "JSON parsing",
      "example scene"
    ]
  }
]