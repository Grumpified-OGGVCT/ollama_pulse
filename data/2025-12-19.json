[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Ollama repository providing a lightweight, extensible framework for running large language models locally with a simple API and CLI.",
    "source": "github",
    "date": "2023-07-15",
    "highlights": [
      "CLI-first",
      "macOS/Linux/Windows",
      "Docker images",
      "Modelfile syntax",
      "REST API"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama, allowing easy integration of local LLMs into Python applications with async support.",
    "source": "github",
    "date": "2023-08-20",
    "highlights": [
      "pip install ollama",
      "async/await",
      "streaming responses",
      "type hints"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript SDK for Ollama, enabling browser and Node.js applications to interact with local LLMs.",
    "source": "github",
    "date": "2023-09-10",
    "highlights": [
      "npm install ollama",
      "TypeScript definitions",
      "browser & Node",
      "streaming"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain integration for Ollama, providing LLM, embeddings, and chat model wrappers for building complex LLM pipelines.",
    "source": "github",
    "date": "2023-10-05",
    "highlights": [
      "LLM wrapper",
      "ChatOllama",
      "OllamaEmbeddings",
      "RAG support"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat interface for Ollama with multi-model support, code highlighting, and RAG via document uploads.",
    "source": "github",
    "date": "2023-11-12",
    "highlights": [
      "React frontend",
      "file upload",
      "conversation history",
      "admin panel"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java SDK for Ollama, offering synchronous and asynchronous APIs for integrating local LLMs into JVM applications.",
    "source": "github",
    "date": "2023-12-01",
    "highlights": [
      "Maven Central",
      "async callbacks",
      "Kotlin-friendly",
      "Spring Boot starter"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ivanfioravanti/ollama-rag",
    "summary": "Ready-to-use RAG template combining Ollama, ChromaDB, and LangChain for private document Q&A.",
    "source": "github",
    "date": "2024-01-08",
    "highlights": [
      "Docker Compose",
      "PDF ingestion",
      "ChromaDB",
      "streamlit UI"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Experimental GitHub Copilot-like plugin that uses Ollama models for local code completion in VS Code.",
    "source": "github",
    "date": "2024-01-20",
    "highlights": [
      "VS Code extension",
      "FIM templates",
      "multi-model",
      "offline"
    ]
  },
  {
    "title": "ollama-cli-chat",
    "url": "https://github.com/sugarforever/ollama-cli-chat",
    "summary": "Terminal UI chat client for Ollama built with Python and Rich, supporting multi-session history and markdown rendering.",
    "source": "github",
    "date": "2024-02-03",
    "highlights": [
      "Rich TUI",
      "session save/load",
      "syntax highlighting",
      "pipx install"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration providing OllamaGenerator and OllamaChatGenerator nodes for building LLM pipelines.",
    "source": "github",
    "date": "2024-02-15",
    "highlights": [
      "Haystack 2.x",
      "generator nodes",
      "pipeline YAML",
      "RAG ready"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/ollama/ollama-cookbook",
    "summary": "Community-driven collection of recipes and examples for deploying and integrating Ollama in various scenarios.",
    "source": "github",
    "date": "2024-02-28",
    "highlights": [
      "Kubernetes",
      "GPU notes",
      "Function calling",
      "Fine-tuning hints"
    ]
  },
  {
    "title": "ollama-nim",
    "url": "https://github.com/ire4ever1190/ollama-nim",
    "summary": "Nim language client for Ollama with async support and native JSON mapping.",
    "source": "github",
    "date": "2024-03-10",
    "highlights": [
      "Nimble package",
      "asyncdispatch",
      "sum types",
      "zero deps"
    ]
  },
  {
    "title": "ollama-rs",
    "url": "https://github.com/pepperoni21/ollama-rs",
    "summary": "Rust crate offering strongly-typed async APIs for Ollama with serde serialization.",
    "source": "github",
    "date": "2024-03-18",
    "highlights": [
      "crates.io",
      "tokio runtime",
      "serde",
      "examples"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/mxyng/ollama-discord",
    "summary": "Discord bot that brings local LLM chat to servers using Ollama backends with per-user quota support.",
    "source": "github",
    "date": "2024-03-25",
    "highlights": [
      "Slash commands",
      "threading",
      "rate limits",
      "Docker"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/seratch/ollama-slack",
    "summary": "Slack Bolt app that plugs Ollama models into Slack channels with streaming replies and mention triggers.",
    "source": "github",
    "date": "2024-04-02",
    "highlights": [
      "Socket Mode",
      "streaming blocks",
      "mention trigger",
      "app home"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU node selection and autoscaling.",
    "source": "github",
    "date": "2024-04-10",
    "highlights": [
      "GPU tolerations",
      "PVC models",
      "HPA",
      "Ingress"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/dagger/dagger/tree/main/sdk/python/src/dagger/client/gen_ollama.py",
    "summary": "Dagger module that wraps Ollama for CI/CD pipelines, enabling model testing and containerized LLM steps.",
    "source": "github",
    "date": "2024-04-18",
    "highlights": [
      "Dagger SDK",
      "CI caching",
      "model tests",
      "containerized"
    ]
  },
  {
    "title": "ollama-elixir",
    "url": "https://github.com/michaelguarino/ollama-elixir",
    "summary": "Elixir OTP client for Ollama with GenServer-backed connection pooling and telemetry events.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "Hex.pm",
      "Finch HTTP",
      "telemetry",
      "Supervisor"
    ]
  },
  {
    "title": "ollama-rust-embeddings",
    "url": "https://github.com/joshua-mo-143/ollama-rust-embeddings",
    "summary": "Rust utility that exposes Ollama embedding models via a fast Axum HTTP API with batched requests.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Axum",
      "batched embeddings",
      "simd",
      "OpenAPI"
    ]
  },
  {
    "title": "ollama-express",
    "url": "https://github.com/ollama-express/ollama-express",
    "summary": "Express.js starter kit that adds Ollama middleware, session chat history, and Swagger docs for local LLM backends.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Express 5",
      "middleware",
      "Redis sessions",
      "Swagger"
    ]
  }
]