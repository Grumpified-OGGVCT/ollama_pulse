[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and Python/JS libraries to pull and run Llama 2, Mistral, Gemma, etc. locally with GPU acceleration.",
    "source": "github",
    "date": "2024-04-15",
    "highlights": [
      "self-contained binaries",
      "model library",
      "REST & streaming APIs"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official TypeScript/JavaScript client for the Ollama server; works in Node & browsers.",
    "source": "github",
    "date": "2024-04-12",
    "highlights": [
      "npm install ollama",
      "Promise/async API",
      "chat & embed helpers"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client; pip install ollama gives synchronous & async APIs for chat, generate, pull, etc.",
    "source": "github",
    "date": "2024-04-14",
    "highlights": [
      "sync/async",
      "embeddings endpoint",
      "built-in model listing"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package letting you plug any Ollama model into chains, agents, RAG, etc.",
    "source": "github",
    "date": "2024-04-10",
    "highlights": [
      "ChatOllama",
      "OllamaEmbeddings",
      "native streaming"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Full-featured, self-hosted ChatGPT-style web UI for Ollama models (themes, multi-user, RAG, plugins).",
    "source": "github",
    "date": "2024-04-13",
    "highlights": [
      "Docker ready",
      "OpenAI-compatible API",
      "document upload"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Unofficial Java/Kotlin client for Ollama with fluent builder API and Spring Boot starters.",
    "source": "github",
    "date": "2024-04-11",
    "highlights": [
      "Maven Central",
      "async options",
      "Spring Boot auto-config"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/gbaptista/ollama-rb",
    "summary": "Community Ruby gem wrapping Ollama\u2019s REST endpoints for generate, chat, pull, etc.",
    "source": "github",
    "date": "2024-04-09",
    "highlights": [
      "gem install ollama",
      "Faraday-based",
      "streaming support"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/salty-flower/ollama-cli",
    "summary": "Enhanced interactive CLI (fuzzy model search, syntax highlighting, chat sessions) built on top of ollama/ollama.",
    "source": "github",
    "date": "2024-03-28",
    "highlights": [
      "fuzzy finder",
      "conversation persistence",
      "markdown rendering"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/fcsonline/ollama-copilot",
    "summary": "Neovim plugin that turns any Ollama model into a GitHub Copilot-style code-completion engine.",
    "source": "github",
    "date": "2024-04-08",
    "highlights": [
      "inline suggestions",
      "configurable shortcuts",
      "Lua config"
    ]
  },
  {
    "title": "ollama-codellama-vscode",
    "url": "https://github.com/Fbrisset/ollama-codellama-vscode",
    "summary": "VS Code extension connecting CodeLlama (or any Ollama model) to editor autocomplete and chat sidebar.",
    "source": "github",
    "date": "2024-04-06",
    "highlights": [
      "inline completions",
      "chat panel",
      "custom prompts"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official Docker image and compose snippets for CPU & GPU deployments (AMD/NVIDIA).",
    "source": "github",
    "date": "2024-04-14",
    "highlights": [
      "single-command up",
      "GPU flags",
      "volume caching"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community Helm chart to run Ollama on Kubernetes with autoscaling and PVC model cache.",
    "source": "github",
    "date": "2024-04-07",
    "highlights": [
      "GPU node-selector",
      "model pre-load list",
      "ingress ready"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/davidmoshal/ollama-rag",
    "summary": "Minimal RAG template using Ollama embeddings + Chroma + LangChain to chat with local PDFs.",
    "source": "github",
    "date": "2024-04-05",
    "highlights": [
      "PDF loader",
      "Chroma vector store",
      "Gradio UI"
    ]
  },
  {
    "title": "ollama-chat-streamlit",
    "url": "https://github.com/rain-1/ollama-chat-streamlit",
    "summary": "One-file Streamlit app for multi-model chat sessions, markdown export, and parameter sliders.",
    "source": "github",
    "date": "2024-04-04",
    "highlights": [
      "session memory",
      "temperature slider",
      "shareable URL"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama-haystack/ollama-haystack",
    "summary": "Haystack integration providing OllamaGenerator and OllamaEmbedder nodes for pipelines.",
    "source": "github",
    "date": "2024-04-03",
    "highlights": [
      "YAML pipeline support",
      "batch inference",
      "open source"
    ]
  },
  {
    "title": "ollama-node",
    "url": "https://www.npmjs.com/package/ollama-node",
    "summary": "Zero-dependency Node.js wrapper around the Ollama REST API (generate, chat, embeddings).",
    "source": "npm",
    "date": "2024-04-11",
    "highlights": [
      "TypeScript types",
      "streaming",
      "ESM/CommonJS"
    ]
  },
  {
    "title": "ollama-py",
    "url": "https://pypi.org/project/ollama-py",
    "summary": "Lightweight third-party Python client (alternative to official) with pydantic models.",
    "source": "pypi",
    "date": "2024-04-10",
    "highlights": [
      "pydantic validation",
      "asyncio support",
      "MIT license"
    ]
  },
  {
    "title": "ollama-rs",
    "url": "https://crates.io/crates/ollama-rs",
    "summary": "Rust crate providing strongly-typed async client for Ollama\u2019s API.",
    "source": "github",
    "date": "2024-04-08",
    "highlights": [
      "tokio/async",
      "serde models",
      "cargo install"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Community Rust SDK with examples for chat, generate, pull, and embeddings.",
    "source": "github",
    "date": "2024-04-07",
    "highlights": [
      "examples folder",
      "tokio runtime",
      "MIT"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/CooperCorona/ollama-discord-bot",
    "summary": "Discord.py bot that exposes Ollama models via slash commands with per-guild model selection.",
    "source": "github",
    "date": "2024-04-06",
    "highlights": [
      "slash commands",
      "typing indicators",
      "guild configs"
    ]
  }
]