[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: lightweight, extensible framework for running Llama 2, Mistral, Gemma and other LLMs locally with a single command.",
    "source": "github",
    "date": "2024-05-28",
    "highlights": [
      "Docker-style CLI",
      "model library",
      "REST & Go APIs",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama server\u2014chat, generate, pull, push, list, delete models.",
    "source": "github",
    "date": "2024-05-23",
    "highlights": [
      "npm install ollama",
      "Promise-based",
      "full API coverage",
      "browser & Node"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama\u2014chat, generate, embeddings, tool calling, streaming support.",
    "source": "github",
    "date": "2024-05-24",
    "highlights": [
      "pip install ollama",
      "sync & async",
      "Pydantic models",
      "Jupyter friendly"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain-ollama",
    "summary": "LangChain integration for Ollama models.",
    "source": "github",
    "date": "2024-06-01",
    "highlights": [
      "LangChain adapter",
      "Chat & embeddings",
      "Tool calling"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama\u2014chat UI, model management, multi-user, OpenAI-compatible endpoint, RAG uploads.",
    "source": "github",
    "date": "2024-05-28",
    "highlights": [
      "Docker image",
      "PWA",
      "document QA",
      "role-based auth",
      "dark/light mode"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client library for Ollama.",
    "source": "github",
    "date": "2024-05-30",
    "highlights": [
      "Maven Central",
      "Kotlin coroutines",
      "Spring Boot starter"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Official Ruby gem for Ollama\u2014chat, generate, pull, delete, list, copy, show, create models.",
    "source": "github",
    "date": "2024-05-21",
    "highlights": [
      "gem install ollama",
      "streaming blocks",
      "Ruby 3.x",
      "zero deps"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/saltyorg/ollama-cli",
    "summary": "Enhanced CLI wrapper around Ollama with prompts and utilities.",
    "source": "github",
    "date": "2024-05-28",
    "highlights": [
      "Bash scripts",
      "Prompt templates",
      "Model management"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "VS Code extension that turns Ollama models into inline code copilot with autocomplete and chat pane.",
    "source": "github",
    "date": "2024-05-26",
    "highlights": [
      "inline suggestions",
      "chat panel",
      "configurable model",
      "status bar"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration to use Ollama models for RAG pipelines.",
    "source": "github",
    "date": "2024-06-04",
    "highlights": [
      "Haystack nodes",
      "Embedding support",
      "Pipeline examples"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/ollama/ollama-csharp",
    "summary": ".NET SDK for Ollama with async streaming, chat, embeddings and tool-calling support.",
    "source": "github",
    "date": "2024-05-24",
    "highlights": [
      "NuGet OllamaSharp",
      "net6+",
      "IAsyncEnumerable",
      "dependency injection"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama-docker",
    "summary": "Official Docker image and compose examples for Ollama.",
    "source": "github",
    "date": "2024-06-06",
    "highlights": [
      "GPU support",
      "Compose stacks",
      "CUDA images"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Helm chart for deploying Ollama server and models on Kubernetes with GPU and persistence support.",
    "source": "github",
    "date": "2024-05-22",
    "highlights": [
      "GPU nodeSelector",
      "model PVC cache",
      "autoscaling",
      "ingress"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama/ollama-rag",
    "summary": "Template repository for building RAG apps with Ollama and Chroma.",
    "source": "github",
    "date": "2024-06-03",
    "highlights": [
      "Streamlit UI",
      "ChromaDB",
      "PDF ingestion"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama/ollama-discord",
    "summary": "Discord bot that lets users chat with Ollama models in servers.",
    "source": "github",
    "date": "2024-06-01",
    "highlights": [
      "Slash commands",
      "Thread support",
      "Rate limiting"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/ollama/ollama-slack",
    "summary": "Slack bot integration for Ollama with streaming replies.",
    "source": "github",
    "date": "2024-05-27",
    "highlights": [
      "Bolt framework",
      "Socket Mode",
      "App mentions"
    ]
  },
  {
    "title": "ollama-fastapi",
    "url": "https://github.com/ollama/ollama-fastapi",
    "summary": "FastAPI service exposing Ollama with OpenAI-compatible endpoints.",
    "source": "github",
    "date": "2024-06-05",
    "highlights": [
      "OpenAI drop-in",
      "/v1/chat/completions",
      "Streaming"
    ]
  },
  {
    "title": "ollama-terraform",
    "url": "https://github.com/ollama/ollama-terraform",
    "summary": "Terraform module to provision Ollama on AWS EC2 with GPU.",
    "source": "github",
    "date": "2024-05-26",
    "highlights": [
      "g5.xlarge GPU",
      "Cloud-init",
      "Spot instances"
    ]
  },
  {
    "title": "ollama-macos-swift",
    "url": "https://github.com/ollama/ollama-macos-swift",
    "summary": "SwiftUI Mac app that wraps Ollama for native UI interactions.",
    "source": "github",
    "date": "2024-06-07",
    "highlights": [
      "Swift Package",
      "Menu-bar app",
      "Keyboard shortcuts"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama/ollama-rust",
    "summary": "Rust crate providing an async client for Ollama.",
    "source": "github",
    "date": "2024-06-04",
    "highlights": [
      "Tokio runtime",
      "Serde models",
      "Crates.io"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package exposing Ollama models as LLM, chat, embeddings and tool-calling components.",
    "source": "github",
    "date": "2024-05-27",
    "highlights": [
      "pip install langchain-ollama",
      "LCEL support",
      "tool calling",
      "streaming"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Java/Kotlin client for Ollama with fluent API, streaming, embeddings, tool calls and Android support.",
    "source": "github",
    "date": "2024-05-25",
    "highlights": [
      "Maven Central",
      "Kotlin coroutines",
      "Android tested",
      "fluent builder"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/technomancy/ollama-cli",
    "summary": "Rust-based interactive CLI for Ollama with readline, syntax highlighting and conversation history.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "cargo install ollama-cli",
      "readline",
      "themes",
      "conversation save/load"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/ollama/ollama-nix",
    "summary": "Nix flake providing Ollama server, clients and pre-packaged models for reproducible deployments.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "flakes",
      "CUDA & ROCm",
      "model derivations",
      "module"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Fast RAG example using Ollama + llama.cpp embeddings, FAISS index and Streamlit frontend.",
    "source": "github",
    "date": "2024-05-23",
    "highlights": [
      "PDF ingestion",
      "FAISS",
      "Streamlit",
      "GPU fallback"
    ]
  },
  {
    "title": "ollama-dify",
    "url": "https://github.com/langgenius/dify/tree/main/api/core/model_runtime/ollama",
    "summary": "Dify.ai integration that registers Ollama as a local model provider for chat, completion and embeddings.",
    "source": "github",
    "date": "2024-05-27",
    "highlights": [
      "model runtime",
      "config UI",
      "streaming",
      "embeddings"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama-discord/ollama-discord",
    "summary": "Self-host Discord bot bringing Ollama models to servers with slash commands, threads and moderation.",
    "source": "github",
    "date": "2024-05-25",
    "highlights": [
      "slash commands",
      "thread isolation",
      "rate limiting",
      "mod roles"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/ollama-slack/ollama-slack",
    "summary": "Slack Bolt app that exposes Ollama models via @ mentions or app home with conversation history.",
    "source": "github",
    "date": "2024-05-26",
    "highlights": [
      "@ollama mention",
      "app home",
      "history",
      "socket mode"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama-obsidian/ollama-obsidian",
    "summary": "Obsidian plugin for local AI assistance\u2014summarize notes, generate text, Q&A with vault context.",
    "source": "github",
    "date": "2024-05-24",
    "highlights": [
      "command palette",
      "vault context",
      "templates",
      "streaming"
    ]
  },
  {
    "title": "ollama-ray",
    "url": "https://github.com/ray-project/ray/tree/master/python/ray/llm/examples/ollama",
    "summary": "Ray Serve example deploying Ollama models for scalable inference and micro-batch serving.",
    "source": "github",
    "date": "2024-05-23",
    "highlights": [
      "Ray Serve",
      "autoscaling",
      "micro-batching",
      "GPU"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Community Rust crate for Ollama with Tokio, streaming, serde and full API coverage.",
    "source": "github",
    "date": "2024-05-22",
    "highlights": [
      "crates.io",
      "async",
      "streaming",
      "examples"
    ]
  },
  {
    "title": "ollama-zig",
    "url": "https://github.com/ollama-zig/ollama-zig",
    "summary": "Zig client library for Ollama with build.zig integration and cross-compilation support.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "Zig package manager",
      "no libc",
      "cross-compile",
      "static binary"
    ]
  }
]