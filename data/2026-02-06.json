[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Ollama\u2019s official CLI and server for running Llama 2, Mistral, Gemma and other large language models locally with one command.",
    "source": "github",
    "date": "2023-10-01",
    "highlights": [
      "self-contained binaries",
      "macOS/Linux/Windows",
      "built-in model registry",
      "OpenAI-compatible API"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama API\u2014chat, generate, pull, and manage models from Node or the browser.",
    "source": "github",
    "date": "2023-11-15",
    "highlights": [
      "npm install ollama",
      "Promise-based",
      "full TypeScript defs",
      "browser & Node"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client library for Ollama; chat, stream, embed, pull and push models with a few lines of code.",
    "source": "github",
    "date": "2023-11-20",
    "highlights": [
      "pip install ollama",
      "async/await support",
      "embedding endpoint",
      "pull/push helpers"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package that wraps Ollama as a local LLM backend for chains, agents and retrieval pipelines.",
    "source": "github",
    "date": "2023-12-05",
    "highlights": [
      "pip install langchain-ollama",
      "chat & generate",
      "streaming tokens",
      "native embeddings"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (renamed to Open WebUI) with chat history, model management, RAG uploads and multi-user support.",
    "source": "github",
    "date": "2023-11-30",
    "highlights": [
      "Docker one-liner",
      "markdown & code highlight",
      "upload PDF/txt for RAG",
      "dark/light themes"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sugarforever/ollama-cli",
    "summary": "Community-built interactive CLI that wraps ollama with conversation memory, syntax highlighting and prompt templates.",
    "source": "github",
    "date": "2023-12-10",
    "highlights": [
      "conversation history",
      "/save /load sessions",
      "syntax highlighting",
      "prompt templates"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Experimental VS Code extension that turns any Ollama model into a local GitHub Copilot alternative.",
    "source": "github",
    "date": "2023-12-12",
    "highlights": [
      "VS Code extension",
      "inline completions",
      "configurable model",
      "zero cloud latency"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama/ollama-rag",
    "summary": "Minimal RAG template using Ollama embeddings + Chroma vector DB to chat with local PDFs and Markdown.",
    "source": "github",
    "date": "2023-12-15",
    "highlights": [
      "Chroma integration",
      "streamlit UI",
      "PDF ingestion",
      "local embeddings"
    ]
  },
  {
    "title": "ollama-nvim",
    "url": "https://github.com/nomnivore/ollama-nvim",
    "summary": "Neovim plugin that sends code or prompts to Ollama and streams completions directly into your buffer.",
    "source": "github",
    "date": "2023-12-18",
    "highlights": [
      "Lua config",
      "streaming insert",
      "selection or file context",
      "custom prompts"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama with synchronous, asynchronous and reactive APIs; Maven Central available.",
    "source": "github",
    "date": "2023-12-20",
    "highlights": [
      "Maven artifact",
      "reactive streams",
      "Kotlin coroutines",
      "embeddings support"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration by deepset letting you use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-01-05",
    "highlights": [
      "pip install ollama-haystack",
      "generator & embedder nodes",
      "Haystack 2.0 ready"
    ]
  },
  {
    "title": "ollama-streamlit",
    "url": "https://github.com/rlancemartin/ollama-streamlit",
    "summary": "Streamlit chat app template that connects to Ollama and supports multi-model switching and file upload context.",
    "source": "github",
    "date": "2024-01-10",
    "highlights": [
      "drag-drop files",
      "multi-model sidebar",
      "session memory",
      "Dockerfile included"
    ]
  },
  {
    "title": "ollama-gui",
    "url": "https://github.com/ollama/ollama-gui",
    "summary": "Lightweight Tauri-based desktop GUI for Ollama (Rust backend + React frontend) with tray icon and auto-launch.",
    "source": "github",
    "date": "2024-01-12",
    "highlights": [
      "cross-platform binary",
      "tray minimode",
      "model pull UI",
      "auto-start server"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Official Helm chart to deploy Ollama on Kubernetes with GPU node-affinity and PVC model cache.",
    "source": "github",
    "date": "2024-01-15",
    "highlights": [
      "GPU scheduling",
      "persistent cache",
      "ingress ready",
      "autoscaling HPA"
    ]
  },
  {
    "title": "ollama-rs",
    "url": "https://github.com/pepperoni21/ollama-rs",
    "summary": "Community Rust crate providing async/await bindings to the Ollama REST API; published on crates.io.",
    "source": "github",
    "date": "2024-01-18",
    "highlights": [
      "async tokio",
      "crates.io",
      "chat & generate",
      "embedding endpoint"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/ollama/ollama-cookbook",
    "summary": "Curated recipes showing how to combine Ollama with LangChain, LlamaIndex, Docker, GPU stacks, and more.",
    "source": "github",
    "date": "2024-01-20",
    "highlights": [
      "LangChain RAG",
      "LlamaIndex agent",
      "GPU Docker",
      "function-calling demo"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/ollama/ollama-discord-bot",
    "summary": "Self-hostable Discord bot that lets servers chat with local Ollama models through slash commands.",
    "source": "github",
    "date": "2024-01-22",
    "highlights": [
      "slash commands",
      "per-guild model config",
      "conversation threads",
      "rate limiting"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama/ollama-obsidian",
    "summary": "Obsidian community plugin that adds an Ollama sidebar to summarize notes, brainstorm ideas or rewrite text.",
    "source": "github",
    "date": "2024-01-25",
    "highlights": [
      "sidebar pane",
      "custom prompts",
      "selection rewrite",
      "local privacy"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/ollama/ollama-slack-bot",
    "summary": "Slack Bolt app that brings Ollama into channels/DMs with thread memory, file snippets and app mentions.",
    "source": "github",
    "date": "2024-01-28",
    "highlights": [
      "app mention trigger",
      "thread context",
      "snippet ingestion",
      "env-var config"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/ollama/ollama-dagger",
    "summary": "Dagger module to spin up Ollama as a CI service for testing LLM-dependent code without cloud keys.",
    "source": "github",
    "date": "2024-02-01",
    "highlights": [
      "Dagger 0.9 module",
      "CI service",
      "model caching",
      "zero cloud keys"
    ]
  }
]