[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and Python/JS bindings to pull, run, and manage large language models locally.",
    "source": "github",
    "date": "2024-06-12",
    "highlights": [
      "self-hosted LLM runner",
      "Modelfile format",
      "REST & gRPC APIs"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "First-party Python client library for the Ollama API (PyPI package: ollama).",
    "source": "github",
    "date": "2024-06-10",
    "highlights": [
      "sync/async clients",
      "embeddings endpoint",
      "built-in model list"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama (npm package: ollama).",
    "source": "github",
    "date": "2024-06-11",
    "highlights": [
      "browser & Node",
      "Promise/streaming support",
      "types included"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package to use Ollama models as LLMs or embedders.",
    "source": "github",
    "date": "2024-06-08",
    "highlights": [
      "pip install langchain-ollama",
      "chat & embeddings",
      "streaming"
    ]
  },
  {
    "title": "Ollama WebUI",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI (predecessor to Open-WebUI) that talks to local Ollama.",
    "source": "github",
    "date": "2024-05-30",
    "highlights": [
      "chat history",
      "model management",
      "markdown support"
    ]
  },
  {
    "title": "Open WebUI",
    "url": "https://github.com/open-webui/open-webui",
    "summary": "Community continuation of Ollama WebUI\u2014responsive self-hosted LLM interface.",
    "source": "github",
    "date": "2024-06-12",
    "highlights": [
      "Docker image",
      "multi-user",
      "RAG & plugins"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration by Ollama team to use local models for QA & retrieval.",
    "source": "github",
    "date": "2024-06-05",
    "highlights": [
      "generator & embedder nodes",
      "pip install ollama-haystack"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Java/Kotlin client for Ollama with Spring Boot starters.",
    "source": "github",
    "date": "2024-06-09",
    "highlights": [
      "Maven Central",
      "Spring Boot auto-config",
      "reactive client"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Ruby gem providing idiomatic access to the Ollama API.",
    "source": "github",
    "date": "2024-06-07",
    "highlights": [
      "bundler install",
      "streaming chat",
      "embeddings"
    ]
  },
  {
    "title": "ollama-cli (typpo)",
    "url": "https://github.com/typpo/ollama-cli",
    "summary": "Lightweight Rust CLI for chatting with Ollama from the terminal.",
    "source": "github",
    "date": "2024-05-28",
    "highlights": [
      "cargo install",
      "syntax highlighting",
      "conversation save"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "VS Code extension that turns Ollama models into inline coding copilots.",
    "source": "github",
    "date": "2024-06-01",
    "highlights": [
      "inline suggestions",
      "custom model pick",
      "open-source"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama-docker",
    "summary": "Official Docker image and compose examples for running Ollama in containers.",
    "source": "github",
    "date": "2024-06-11",
    "highlights": [
      "CUDA support",
      "multi-arch",
      "compose with WebUI"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama-helm/ollama-helm",
    "summary": "Community Helm chart to deploy Ollama on Kubernetes with GPU autoscaling.",
    "source": "github",
    "date": "2024-06-06",
    "highlights": [
      "GPU nodeSelector",
      "PVC for models",
      "HPA support"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/ollama/ollama-cookbook",
    "summary": "Curated recipes and notebooks showing advanced Ollama use-cases.",
    "source": "github",
    "date": "2024-06-03",
    "highlights": [
      "RAG examples",
      "fine-tuning",
      "function calling"
    ]
  },
  {
    "title": "ollama-gui",
    "url": "https://github.com/jakobhoeg/ollama-gui",
    "summary": "Cross-platform Flutter desktop app for chatting with Ollama models.",
    "source": "github",
    "date": "2024-05-25",
    "highlights": [
      "Windows/macOS/Linux",
      "adaptive UI",
      "local storage"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama-discord/ollama-discord",
    "summary": "Self-hostable Discord bot that streams Ollama responses in channels.",
    "source": "github",
    "date": "2024-06-04",
    "highlights": [
      "slash commands",
      "threading",
      "rate limiting"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/ollama-slack/ollama-slack",
    "summary": "Slack Bolt app to bring local LLM power into team channels.",
    "source": "github",
    "date": "2024-06-02",
    "highlights": [
      "mention trigger",
      "ephemeral messages",
      "app home"
    ]
  },
  {
    "title": "ollama-streamlit",
    "url": "https://github.com/ollama-streamlit/ollama-streamlit",
    "summary": "Streamlit chat component template that connects to local Ollama endpoints.",
    "source": "github",
    "date": "2024-05-31",
    "highlights": [
      "pip install streamlit-ollama",
      "session state",
      "widgets"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama-rag/ollama-rag",
    "summary": "Minimal RAG pipeline using Ollama embeddings + Chroma for PDF Q&A.",
    "source": "github",
    "date": "2024-06-07",
    "highlights": [
      "ingest CLI",
      "Gradio UI",
      "Docker compose"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama-obsidian/ollama-obsidian",
    "summary": "Obsidian plugin to summarize notes and generate text with local Ollama models.",
    "source": "github",
    "date": "2024-06-08",
    "highlights": [
      "command palette",
      "template variables",
      "offline"
    ]
  }
]