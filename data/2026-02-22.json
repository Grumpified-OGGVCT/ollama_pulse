[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo\u2014CLI and Go library to pull, run, and manage GGUF models locally with a built-in OpenAI-compatible API server.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "self-contained runtime",
      "OpenAI drop-in API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "First-party JavaScript/TypeScript client for the Ollama HTTP API, published to npm as \u2018ollama\u2019.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "Promise-based",
      "TS typings included",
      "browser & Node"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK on PyPI; chat, generate, embed, and list local models with a few lines of code.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "sync & async APIs",
      "streaming support",
      "PyPI: ollama"
    ]
  },
  {
    "title": "langchain-community/llms Ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/llms/ollama.py",
    "summary": "Built-in LangChain integration letting you swap Ollama models into any chain or agent.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "zero-config",
      "streaming",
      "chat & embed wrappers"
    ]
  },
  {
    "title": "LlamaIndex Ollama integration",
    "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/llms/llama-index-llms-ollama",
    "summary": "LlamaIndex LLM and embedding connectors so you can build RAG pipelines on local Ollama models.",
    "source": "github",
    "date": "2024-05-17",
    "highlights": [
      "embedding support",
      "custom prompt templates",
      "pip: llama-index-llms-ollama"
    ]
  },
  {
    "title": "ollama-webui (now Open WebUI)",
    "url": "https://github.com/open-webui/open-webui",
    "summary": "Full-featured, self-hosted chat interface (React/React-Query) that talks to any Ollama endpoint; supports RAG, multi-user, and plug-ins.",
    "source": "github",
    "date": "2024-05-21",
    "highlights": [
      "Docker image",
      "RAG via LlamaIndex",
      "LDAP/OAuth"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/kevinwatt/ollama-cli",
    "summary": "Rust-based interactive REPL with syntax highlighting, history, and model switching for Ollama.",
    "source": "github",
    "date": "2024-04-10",
    "highlights": [
      "Rust binary",
      "readline",
      "themes"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Experimental GitHub Copilot-style code-completion plugin that uses Ollama models instead of OpenAI.",
    "source": "github",
    "date": "2024-03-28",
    "highlights": [
      "VS Code extension",
      "inline suggestions",
      "local only"
    ]
  },
  {
    "title": "node-ollama",
    "url": "https://github.com/jmorganca/ollama-js",
    "summary": "Community-maintained Node wrapper around the Ollama REST API (npm: ollama).",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "lightweight",
      "zero deps",
      "TypeScript"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client library (Maven Central: ollama4j) exposing sync/async APIs and Spring Boot starter.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "Spring Boot starter",
      "reactive streams",
      "Kotlin DSL"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Official Ruby gem for chatting and generating with local Ollama models.",
    "source": "github",
    "date": "2024-05-16",
    "highlights": [
      "Ruby gem",
      "streaming blocks",
      "Rails ready"
    ]
  },
  {
    "title": "ollama-chatbot-starter",
    "url": "https://github.com/vercel-labs/ollama-chatbot-starter",
    "summary": "Next.js starter template that wires Ollama to Vercel AI SDK for streaming chat UIs.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Vercel AI SDK",
      "edge runtime",
      "Dockerfile"
    ]
  },
  {
    "title": "haystack-ollama",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Deepset Haystack integration providing OllamaGenerator and OllamaChatGenerator nodes.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Haystack 2.x",
      "pipeline YAML",
      "embed nodes"
    ]
  },
  {
    "title": "semantic-kernel-ollama",
    "url": "https://github.com/microsoft/semantic-kernel/tree/main/dotnet/src/Connectors/Connectors.Ollama",
    "summary": "Microsoft Semantic Kernel connector letting you orchestrate Ollama models in C# pipelines.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "NuGet package",
      "planners",
      "native functions"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU autoscaling.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "GPU node selector",
      "PVC caching",
      "Prometheus metrics"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official Docker image and compose samples (CPU & GPU) for running Ollama in containers.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "official image",
      "CUDA/ROCm",
      "rootless mode"
    ]
  },
  {
    "title": "chatbox (Chatbox AI)",
    "url": "https://github.com/Bin-Huang/chatbox",
    "summary": "Cross-platform desktop chat client (Electron) that supports Ollama endpoints out of the box.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "Windows/Mac/Linux",
      "Markdown render",
      "local storage"
    ]
  },
  {
    "title": "ollama-model-registry",
    "url": "https://github.com/technovangelist/ollama-model-registry",
    "summary": "Simple registry UI to browse, upload, and version GGUF models for internal Ollama fleets.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "upload GGUF",
      "version tags",
      "basic auth"
    ]
  },
  {
    "title": "r/ollama - Reddit community",
    "url": "https://www.reddit.com/r/ollama/",
    "summary": "Active subreddit for sharing tips, custom models, and troubleshooting Ollama setups.",
    "source": "reddit",
    "date": "2024-05-22",
    "highlights": [
      "~25k members",
      "model showcases",
      "weekly Q&A"
    ]
  },
  {
    "title": "Ollama on Hacker News",
    "url": "https://news.ycombinator.com/item?id=40234567",
    "summary": "Discussion thread comparing Ollama to llama.cpp and local LLM deployment strategies.",
    "source": "hackernews",
    "date": "2024-05-20",
    "highlights": [
      "performance benchmarks",
      "M-series Mac optimizations",
      "Docker tips"
    ]
  }
]