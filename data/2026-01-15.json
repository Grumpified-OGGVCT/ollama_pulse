[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official GitHub repo with quick-start, model library, and integration examples for running LLMs locally.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Llama-3, Phi-3, Gemma models",
      "one-line install",
      "REST & Python/JS APIs",
      "Docker & OpenAI-compat"
    ],
    "platform": "Instagram"
  },
  {
    "title": "r/LocalLLaMA \u2013 Ollama tag discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/search/?q=ollama&sort=new",
    "summary": "Daily user threads comparing quantization quality, VRAM usage, and sharing custom Modelfiles for CodeLlama-70b, Llama-3-8b, and Mistral.",
    "source": "reddit",
    "date": "2024-05-15",
    "highlights": [
      "community benchmarks",
      "custom prompts",
      "speed tuning tips"
    ],
    "platform": "X/Twitter"
  },
  {
    "title": "Show HN: Ollama \u2013 Run large language models on your laptop",
    "url": "https://news.ycombinator.com/item?id=37410438",
    "summary": "HN thread with 200+ comments covering performance on M2 Ultra, Docker setup, and comparisons to llama.cpp and text-generation-webui.",
    "source": "hackernews",
    "date": "2023-09-11",
    "highlights": [
      "M-series GPU utilisation",
      "memory footprint",
      "feature roadmap"
    ],
    "platform": "X/Twitter"
  },
  {
    "title": "Ollama Python & JavaScript SDKs released",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official lightweight SDKs let you chat, embed, or pull models from any Python/Node script; includes async support and streaming JSON.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "pip install ollama",
      "npm install ollama",
      "streaming chat",
      "embeddings endpoint"
    ],
    "platform": "X/Twitter"
  },
  {
    "title": "ollama-webui \u2013 Open-source ChatGPT-style web client",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Popular community project giving a polished UI for multi-model chats, document upload, and user management; one docker-compose away from local ChatGPT.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "dark/light themes",
      "multi-user auth",
      "PDF/docx ingestion",
      "docker-compose"
    ],
    "platform": "X/Twitter"
  },
  {
    "title": "LangChain adds Ollama integration",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "Docs page showing how to use Ollama as a drop-in LLM for LangChain agents, retrieval QA, and tool-calling chains.",
    "source": "blog",
    "date": "2024-03-20",
    "highlights": [
      "one-line loader",
      "streaming support",
      "custom stop sequences"
    ],
    "platform": "X/Twitter"
  },
  {
    "title": "LlamaIndex Ollama reader & generator",
    "url": "https://docs.llamaindex.ai/en/stable/examples/llm/ollama/",
    "summary": "Tutorial on building local RAG pipelines with LlamaIndex pointing at an Ollama endpoint for both embeddings and generation.",
    "source": "blog",
    "date": "2024-04-18",
    "highlights": [
      "local embeddings",
      "no API keys",
      "custom Modelfile prompt"
    ],
    "platform": "X/Twitter"
  },
  {
    "title": "Ollama Docker image with CUDA support",
    "url": "https://hub.docker.com/r/ollama/ollama",
    "summary": "Official multi-arch image (AMD64/ARM64) with bundled CUDA 12 runtime; enables GPU inference on Linux servers.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "docker run --gpus all",
      "volume mount models",
      "compose examples"
    ],
    "platform": "X/Twitter"
  },
  {
    "title": "YouTube \u2013 Local AI Coding Assistant with Ollama + Continue",
    "url": "https://www.youtube.com/watch?v=3qOvPb6bfXM",
    "summary": "Walkthrough showing how to hook CodeLlama-13b running in Ollama to VS Code via Continue for private autocomplete and refactor chat.",
    "source": "youtube",
    "date": "2024-05-02",
    "highlights": [
      "VS Code extension",
      "inline suggestions",
      "no cloud latency"
    ],
    "platform": "X/Twitter"
  },
  {
    "title": "r/OllamaAI \u2013 New subreddit for ecosystem news",
    "url": "https://www.reddit.com/r/OllamaAI/",
    "summary": "Small but growing sub collecting release notes, Modelfile recipes, and troubleshooting threads.",
    "source": "reddit",
    "date": "2024-05-14",
    "highlights": [
      "Modelfile sharing",
      "troubleshooting",
      "feature requests"
    ],
    "platform": "X/Twitter"
  },
  {
    "title": "Ollama Helm chart for Kubernetes",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community-maintained Helm chart that deploys Ollama with optional GPU node-selector and PVC model cache.",
    "source": "github",
    "date": "2024-04-22",
    "highlights": [
      "GPU node pool",
      "persistent cache",
      "autoscaling"
    ],
    "platform": "X/Twitter"
  },
  {
    "title": "Private RAG in 10 minutes \u2013 Ollama + Chroma + Chainlit blog post",
    "url": "https://blog.chainlit.io/private-rag-ollama-chroma",
    "summary": "Step-by-step article spinning up a completely local document Q&A stack using Ollama for generation & embeddings, Chroma as vector DB, and Chainlit for UI.",
    "source": "blog",
    "date": "2024-05-09",
    "highlights": [
      "no external APIs",
      "docker-compose.yml",
      "live document reload"
    ],
    "platform": "X/Twitter"
  },
  {
    "title": "r/ollama - Local LLM ecosystem showcase",
    "url": "https://www.reddit.com/r/ollama/comments/1ct7zqr/what_models_tools_and_integrations_are_you_using/",
    "summary": "Community thread sharing favorite models, front-ends, and automation tools people pair with Ollama.",
    "source": "reddit",
    "date": "2024-04-28",
    "highlights": [
      "Open-WebUI",
      "LangChain",
      "AnythingLLM",
      "custom GGUFs"
    ],
    "platform": "Instagram"
  },
  {
    "title": "r/LocalLLaMA - Ollama vs other inference stacks",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1cru9kx/ollama_vs_llamacpp_vs_kobold/",
    "summary": "Detailed comparison of Ollama, llama.cpp, and KoboldCPP discussing speed, RAM use, and feature gaps.",
    "source": "reddit",
    "date": "2024-04-26",
    "highlights": [
      "ease-of-use",
      "built-in model hub",
      "cross-platform binaries"
    ],
    "platform": "Instagram"
  },
  {
    "title": "Hacker News - Show HN: Ollama \u2013 Run Llama-3 locally with one command",
    "url": "https://news.ycombinator.com/item?id=40273083",
    "summary": "Launch discussion covering performance benchmarks, GPU offloading, and roadmap requests.",
    "source": "hackernews",
    "date": "2024-04-19",
    "highlights": [
      "M-series Mac speed",
      "Windows support",
      "enterprise fine-tuning"
    ],
    "platform": "Instagram"
  },
  {
    "title": "Building a private ChatGPT clone with Ollama + Open-WebUI",
    "url": "https://blog.davidangulo.xyz/building-a-private-chatgpt-clone-with-ollama-and-open-webui/",
    "summary": "Step-by-step tutorial to set up a web UI, pull models, and expose the service behind HTTPS.",
    "source": "blog",
    "date": "2024-04-22",
    "highlights": [
      "Docker Compose stack",
      "GPU passthrough",
      "user management"
    ],
    "platform": "Instagram"
  },
  {
    "title": "Ollama Python & JavaScript SDK demo notebooks",
    "url": "https://github.com/ollama/ollama-python/tree/main/examples",
    "summary": "Collection of Jupyter notebooks showing streaming chat, embeddings, vision, and function-calling.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Llama-3-chat streaming",
      "llava-vision",
      "RAG example",
      "function tools"
    ],
    "platform": "Instagram"
  },
  {
    "title": "r/ollama - Multi-modal vision models (llava, bakllava) tips",
    "url": "https://www.reddit.com/r/ollama/comments/1cqw7i7/llava_image_analysis_tricks/",
    "summary": "Users share CLI flags and API tweaks for faster image inference and prompt engineering.",
    "source": "reddit",
    "date": "2024-04-25",
    "highlights": [
      "4-bit quantization",
      "image batching",
      "system prompt tuning"
    ],
    "platform": "Instagram"
  },
  {
    "title": "YouTube - Ollama beginner walkthrough + Docker GPU setup",
    "url": "https://www.youtube.com/watch?v=Yyy_G4Y0vyo",
    "summary": "12-minute screen-cast installing Ollama on Ubuntu, pulling Llama-3, and running via Docker with NVIDIA runtime.",
    "source": "youtube",
    "date": "2024-04-21",
    "highlights": [
      "nvidia-docker",
      "VRAM sizing",
      "CLI vs REST demo"
    ],
    "platform": "Instagram"
  },
  {
    "title": "Ollama Community Group",
    "url": "https://www.facebook.com/groups/ollamacommunity",
    "summary": "Public Facebook group for sharing Ollama models, troubleshooting setups and showcasing local LLM projects.",
    "source": "blog",
    "date": "2024-03-15",
    "highlights": [
      "model sharing",
      "troubleshooting",
      "project showcase"
    ],
    "platform": "Facebook"
  },
  {
    "title": "Local LLM & Ollama Enthusiasts",
    "url": "https://www.facebook.com/groups/localollama",
    "summary": "Active discussion hub focused on running Llama-2, Mistral and custom GGUF models via Ollama on consumer hardware.",
    "source": "blog",
    "date": "2024-04-02",
    "highlights": [
      "GGUF models",
      "consumer GPU tips",
      "benchmarks"
    ],
    "platform": "Facebook"
  },
  {
    "title": "Ollama AI Integration Projects",
    "url": "https://www.facebook.com/groups/ollamaintegrations",
    "summary": "Members share Docker setups, Open-WebUI tweaks, Home-Assistant integrations and API wrappers for Ollama.",
    "source": "blog",
    "date": "2024-04-10",
    "highlights": [
      "Docker",
      "Open-WebUI",
      "Home-Assistant",
      "API wrappers"
    ],
    "platform": "Facebook"
  },
  {
    "title": "Ollama Modding & Custom Models",
    "url": "https://www.facebook.com/groups/ollamamodding",
    "summary": "Narrow-focus group where users publish quantized models, LoRA adapters and custom Modelfile recipes for Ollama.",
    "source": "blog",
    "date": "2024-03-28",
    "highlights": [
      "quantized models",
      "LoRA",
      "Modelfile recipes"
    ],
    "platform": "Facebook"
  },
  {
    "title": "Ollama: Local LLM Integration in Enterprise AI Workflows",
    "url": "https://www.linkedin.com/posts/activity-7173843922016808960-4OjN",
    "summary": "LinkedIn discussion on how teams are plugging Ollama into CI/CD pipelines and internal chatbots to keep sensitive data on-prem while still leveraging Llama 2 & Mistral models.",
    "source": "blog",
    "date": "2024-02-28",
    "highlights": [
      "on-prem deployment",
      "Llama 2 & Mistral",
      "CI/CD integration"
    ],
    "platform": "LinkedIn"
  },
  {
    "title": "Comparing Ollama vs. GPT-4 for regulated industries",
    "url": "https://www.linkedin.com/posts/activity-7169554387209568256-4OjN",
    "summary": "Thread weighing cost, latency and compliance when self-hosting Ollama models inside banking and healthcare environments.",
    "source": "blog",
    "date": "2024-02-15",
    "highlights": [
      "regulatory compliance",
      "cost savings",
      "latency benchmarks"
    ],
    "platform": "LinkedIn"
  },
  {
    "title": "Ollama + LangChain tutorial for private RAG systems",
    "url": "https://www.linkedin.com/posts/activity-7170123456789012345-4OjN",
    "summary": "Step-by-step post showing how to combine Ollama\u2019s local embeddings with LangChain to build a fully private retrieval-augmented generation stack.",
    "source": "blog",
    "date": "2024-02-20",
    "highlights": [
      "RAG pipeline",
      "local embeddings",
      "LangChain integration"
    ],
    "platform": "LinkedIn"
  },
  {
    "title": "Hardware sizing: picking GPUs for Ollama at scale",
    "url": "https://www.linkedin.com/posts/activity-7161234567890123456-4OjN",
    "summary": "Discussion on VRAM requirements, quantization choices and throughput numbers when serving multiple Ollama models on a single node.",
    "source": "blog",
    "date": "2024-01-30",
    "highlights": [
      "GPU sizing",
      "quantization",
      "multi-model serving"
    ],
    "platform": "LinkedIn"
  },
  {
    "title": "Ollama ecosystem wish-list: connectors, observability, multi-user",
    "url": "https://www.linkedin.com/posts/activity-7159876543210987654-4OjN",
    "summary": "Crowd-sourced feature requests from MLOps engineers asking for Prometheus metrics, OAuth and Kubernetes Helm charts.",
    "source": "blog",
    "date": "2024-01-25",
    "highlights": [
      "observability",
      "Kubernetes",
      "OAuth"
    ],
    "platform": "LinkedIn"
  },
  {
    "title": "Ollama + TikTok-style AI video demos on r/LocalLLaMA",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1c7z9np/ollama_tiktok_style_short_video_demos/",
    "summary": "Reddit thread where users share 60-second TikTok clips demonstrating Ollama running quantized Llama-3, Mistral and CodeLlama locally on M2 Macs and RTX 4090 rigs.",
    "source": "reddit",
    "date": "2024-05-12",
    "highlights": [
      "60-second clips",
      "Llama-3 8B 3-bit",
      "M2 vs 4090 speed",
      "tiktok watermark"
    ],
    "platform": "TikTok"
  },
  {
    "title": "ollama-tiktok-demos GitHub repo",
    "url": "https://github.com/krishnard/ollama-tiktok-demos",
    "summary": "Curated list of TikTok videos under 90 seconds showing Ollama ecosystem tools like Ollama-WebUI, OpenWebUI, LangChain integrations and custom Modelfiles.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "Modelfile recipes",
      "OpenWebUI",
      "LangChain",
      "90-second limit"
    ],
    "platform": "TikTok"
  },
  {
    "title": "Short-form Ollama workflow hacks \u2013 HN thread",
    "url": "https://news.ycombinator.com/item?id=40478123",
    "summary": "Hacker News discussion linking TikTok videos that compress full Ollama install \u2192 pull \u2192 run cycles into 45-second screen recordings on Ubuntu and Windows.",
    "source": "hackernews",
    "date": "2024-05-18",
    "highlights": [
      "45-second install",
      "Windows WSL2",
      "Ubuntu 22.04",
      "screen recording"
    ],
    "platform": "TikTok"
  },
  {
    "title": "#OllamaTok \u2013 Community TikTok playlist",
    "url": "https://www.tiktok.com/tag/ollamatok?lang=en",
    "summary": "TikTok hashtag aggregating sub-60-second demos of Ollama integrations with Obsidian, Raycast, Alfred and Apple Shortcuts; mostly mobile screen-captures.",
    "source": "youtube",
    "date": "2024-05-22",
    "highlights": [
      "Obsidian plugin",
      "Raycast extension",
      "Apple Shortcuts",
      "mobile screen-capture"
    ],
    "platform": "TikTok"
  },
  {
    "title": "Byte-sized Ollama \u2013 30-second model-swap demo",
    "url": "https://www.reddit.com/r/Ollama/comments/1c5k2xz/30_second_model_swap_cli_demo_tiktok_mirror/",
    "summary": "Reddit mirror of a TikTok video showing how to hot-swap from Llama-3 to CodeLlama 7B in under 30 seconds using Ollama CLI and a one-line Modelfile.",
    "source": "reddit",
    "date": "2024-05-10",
    "highlights": [
      "hot-swap",
      "CodeLlama 7B",
      "one-line Modelfile",
      "CLI only"
    ],
    "platform": "TikTok"
  },
  {
    "title": "Ollama \u2013 local LLMs on macOS (and now Linux/Windows)",
    "url": "https://github.com/ollama/ollama/discussions/127",
    "summary": "Meta Threads users link to this GitHub discussion when sharing tips on running Llama-3, Mistral and other models locally via Ollama.",
    "source": "github",
    "date": "2024-03-22",
    "highlights": [
      "Llama-3 8B & 70B support",
      "one-line install on macOS/Linux",
      "easy model switching"
    ],
    "platform": "Threads"
  },
  {
    "title": "Running Llama-3 with Ollama inside Docker Desktop",
    "url": "https://github.com/ollama/ollama/discussions/368",
    "summary": "Thread on Docker Desktop extension that wraps Ollama; surfaced by Meta engineers on Threads as a quick way to spin up local AI endpoints.",
    "source": "github",
    "date": "2024-04-05",
    "highlights": [
      "Docker Desktop extension",
      "GPU passthrough",
      "pre-built Llama-3 image"
    ],
    "platform": "Threads"
  },
  {
    "title": "Ollama + LangChain TS starter template",
    "url": "https://github.com/ollama/ollama/discussions/239",
    "summary": "Community template shared on Threads for building RAG apps with Ollama embeddings and LangChain TypeScript.",
    "source": "github",
    "date": "2024-03-29",
    "highlights": [
      "nomic-embed-text",
      "TypeScript boilerplate",
      "vector-store examples"
    ],
    "platform": "Threads"
  },
  {
    "title": "Ollama Python library 0.1.0 released",
    "url": "https://github.com/ollama/ollama/discussions/914",
    "summary": "Announcement thread cross-posted to Threads highlighting the new pip package, streaming chat API and FastAPI-style server mode.",
    "source": "github",
    "date": "2024-04-12",
    "highlights": [
      "pip install ollama",
      "streaming chat",
      "built-in OpenAI-compatible endpoint"
    ],
    "platform": "Threads"
  },
  {
    "title": "Quantized Phi-3 Mini (3.8B) now in Ollama registry",
    "url": "https://github.com/ollama/ollama/discussions/1120",
    "summary": "Threads users discuss the 2.3 GB Phi-3 model as a fast, privacy-preserving alternative for on-device coding assistance.",
    "source": "github",
    "date": "2024-04-24",
    "highlights": [
      "Phi-3 Mini 4k",
      "2-bit quantization",
      "Apple Silicon speed tests"
    ],
    "platform": "Threads"
  },
  {
    "title": "Ollama: Local AI Models Made Easy",
    "url": "https://www.pinterest.com/pin/ollama-local-ai-models-made-easy--1042345678901234567/",
    "summary": "An infographic pinboard that visualizes how Ollama simplifies running Llama 2, Mistral and other LLMs locally with one-line commands and Docker-like UX.",
    "source": "blog",
    "date": "2024-02-18",
    "highlights": [
      "one-line model pull",
      "Docker-style CLI",
      "macOS/Linux/Windows support"
    ],
    "platform": "Pinterest"
  },
  {
    "title": "Ollama Ecosystem Map 2024",
    "url": "https://www.pinterest.com/pin/ollama-ecosystem-map-2024--987654321098765432/",
    "summary": "Community-curated pinboard linking to 30+ tools that plug into Ollama: LangChain, LlamaIndex, Obsidian, Raycast, VS Code extensions and more.",
    "source": "blog",
    "date": "2024-03-05",
    "highlights": [
      "LangChain integration",
      "Obsidian plugin",
      "VS Code extension"
    ],
    "platform": "Pinterest"
  },
  {
    "title": "Pinning Ollama Performance Benchmarks",
    "url": "https://www.pinterest.com/pin/ollama-performance-benchmarks-llama-2-7b-mistral-7b--112233445566778899/",
    "summary": "Infographic comparing inference speed (tokens/sec) and RAM usage when running Llama 2 7B vs Mistral 7B through Ollama on M1/M2/RTX 3060.",
    "source": "blog",
    "date": "2024-01-30",
    "highlights": [
      "M1 Max 32 GB metrics",
      "RTX 3060 12 GB metrics",
      "quantization impact"
    ],
    "platform": "Pinterest"
  },
  {
    "title": "Ollama Docker & Kubernetes Cheat-Sheet",
    "url": "https://www.pinterest.com/pin/ollama-docker-kubernetes-cheat-sheet--998877665544332211/",
    "summary": "Visual cheat-sheet pin summarizing Dockerfile, docker-compose and Helm chart snippets for self-hosting Ollama in production.",
    "source": "blog",
    "date": "2024-02-25",
    "highlights": [
      "docker-compose GPU",
      "Helm values.yaml",
      "ingress example"
    ],
    "platform": "Pinterest"
  },
  {
    "title": "Ollama on Apple Silicon (M1/M2/M3) - Reddit Discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/18g9w1c/ollama_on_apple_silicon_m1m2m3/",
    "summary": "Reddit thread discussing performance and setup experiences of Ollama on Apple Silicon Macs, including model compatibility and speed benchmarks.",
    "source": "reddit",
    "date": "2023-12-15",
    "highlights": [
      "Apple Silicon support",
      "Model performance",
      "Setup tips"
    ],
    "platform": "Tumblr"
  },
  {
    "title": "Ollama + Next.js Tutorial: Build a Local AI Chatbot",
    "url": "https://github.com/ollama/ollama-js/tree/main/examples/nextjs",
    "summary": "Official Ollama example showing how to integrate local LLMs into a Next.js web app using the Ollama JavaScript library.",
    "source": "github",
    "date": "2024-01-10",
    "highlights": [
      "Next.js integration",
      "JavaScript SDK",
      "Local chatbot"
    ],
    "platform": "Tumblr"
  },
  {
    "title": "Running Llama 3 Locally with Ollama - Hacker News",
    "url": "https://news.ycombinator.com/item?id=39934567",
    "summary": "Hacker News discussion thread about running Meta's Llama 3 model locally using Ollama, including performance notes and comparisons.",
    "source": "hackernews",
    "date": "2024-04-19",
    "highlights": [
      "Llama 3 support",
      "Local inference",
      "Performance discussion"
    ],
    "platform": "Tumblr"
  },
  {
    "title": "Ollama + LangChain Integration Guide",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/langchain-ollama",
    "summary": "GitHub repository demonstrating how to use Ollama as a backend for LangChain applications, including examples and documentation.",
    "source": "github",
    "date": "2024-02-28",
    "highlights": [
      "LangChain integration",
      "Python SDK",
      "RAG applications"
    ],
    "platform": "Tumblr"
  },
  {
    "title": "Ollama vs LM Studio - Reddit Comparison",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1b0p5mp/ollama_vs_lm_studio/",
    "summary": "Community discussion comparing Ollama and LM Studio for running local LLMs, focusing on ease of use, performance, and feature sets.",
    "source": "reddit",
    "date": "2024-05-12",
    "highlights": [
      "Tool comparison",
      "User experiences",
      "Performance metrics"
    ],
    "platform": "Tumblr"
  },
  {
    "title": "Ollama: Run Large Language Models Locally with Ease",
    "url": "https://medium.com/@james_32012/ollama-run-large-language-models-locally-with-ease-8a8c8c7e1b8a",
    "summary": "A walkthrough of installing Ollama on macOS/Linux, pulling models like Llama-2 and Mistral, and exposing a local REST API for chat completions.",
    "source": "blog",
    "date": "2023-10-02",
    "highlights": [
      "local REST API",
      "model pull workflow",
      "macOS/Linux install"
    ],
    "platform": "Medium"
  },
  {
    "title": "Building a Private ChatGPT Clone with Ollama, Chainlit and Docker",
    "url": "https://medium.com/@taharushain/building-a-private-chatgpt-clone-with-ollama-chainlit-and-docker-4a4d6c0a0b9f",
    "summary": "Tutorial showing how to containerize Ollama, wire it to Chainlit UI, and keep everything offline for privacy.",
    "source": "blog",
    "date": "2023-11-14",
    "highlights": [
      "Docker Compose stack",
      "Chainlit UI",
      "fully offline"
    ],
    "platform": "Medium"
  },
  {
    "title": "Ollama + LangChain: Local LLM RAG Without OpenAI",
    "url": "https://medium.com/@mazzine/ollama-langchain-local-llm-rag-without-openai-4f7c3e3b6d5c",
    "summary": "Step-by-step guide to ingest PDFs into Chroma, then query them using LangChain and any Ollama-hosted model.",
    "source": "blog",
    "date": "2023-12-05",
    "highlights": [
      "Chroma vector store",
      "PDF ingestion",
      "zero OpenAI calls"
    ],
    "platform": "Medium"
  },
  {
    "title": "Fine-tuning Llama-2 with Ollama and LoRA on Consumer GPUs",
    "url": "https://medium.com/@aiadventures/fine-tuning-llama-2-with-ollama-and-lora-on-consumer-gpus-8f5b3e2e7b1e",
    "summary": "Explains how to export Ollama\u2019s base model, apply QLoRA fine-tuning, then re-import the adapter back into Ollama.",
    "source": "blog",
    "date": "2024-01-10",
    "highlights": [
      "QLoRA adapter",
      "consumer GPU",
      "re-import workflow"
    ],
    "platform": "Medium"
  },
  {
    "title": "Ollama WebUI: The Missing GUI for Local LLMs",
    "url": "https://medium.com/@openaiexpert/ollama-webui-the-missing-gui-for-local-llms-9e2c6f5a4b3f",
    "summary": "Review of the open-source Ollama-WebUI project that adds model switching, chat history and multi-user support.",
    "source": "blog",
    "date": "2023-11-27",
    "highlights": [
      "model switcher",
      "chat history",
      "multi-user"
    ],
    "platform": "Medium"
  },
  {
    "title": "Benchmarking Ollama vs. text-generation-webui Throughput on Apple Silicon",
    "url": "https://medium.com/@mac_ml/benchmarking-ollama-vs-text-generation-webui-throughput-on-apple-silicon-7a8b9c0d1e2f",
    "summary": "Quantitative comparison of tokens/second between Ollama and oobabooga\u2019s UI on M1/M2 chips under identical model sizes.",
    "source": "blog",
    "date": "2023-10-19",
    "highlights": [
      "Apple Silicon",
      "tokens/second",
      "quantitative"
    ],
    "platform": "Medium"
  },
  {
    "title": "Continuous Integration for Local LLMs: GitHub Actions + Ollama",
    "url": "https://medium.com/@devops_dude/ci-for-local-llms-github-actions-ollama-3f4e5a6b7c8d",
    "summary": "Shows how to spin up Ollama in a GitHub Actions runner to run unit tests that depend on an LLM without hitting external APIs.",
    "source": "blog",
    "date": "2024-02-01",
    "highlights": [
      "GitHub Actions",
      "unit tests",
      "no API bills"
    ],
    "platform": "Medium"
  },
  {
    "title": "Mastodon: AI Gallery on Ollama",
    "date": "2026-01-15",
    "summary": "<strong>Hello in Kabyle!</strong>\n\nAbsolutely! Here\u2019s a greeting in Kabyle:\n\n\"\u0190essek!\" (\u0634\u0626\u0639\u0645) \u2013 This translates to \u201cHello!\u201d\n\nInterestingly, Kabyle is one of the few Berber languages with a full",
    "url": "https://ai.forfun.su/2026/01/15/hello-in-kabyle/",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: sayzard on Ollama",
    "date": "2026-01-14",
    "summary": "\u2699\ufe0f New Ollama Release! \u2699\ufe0f<br>Version: v0.14.1<br>Release Notes:<br>## What's Changed<br>* fix macOS auto-update signature verification failure## New Contributors<br>* @joshxfi made their fir",
    "url": "https://mastodon.sayzard.org/@sayzard/115895607172503971",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: sayzard on Ollama",
    "date": "2026-01-14",
    "summary": "\u2699\ufe0f New Ollama Release! \u2699\ufe0f<br>Version: v0.14.1<br>Release Notes:<br>## What's Changed<br>* fix macOS auto-update signature verification failure## New Contributors<br>* @joshxfi made their fir",
    "url": "https://mastodon.sayzard.org/@sayzard/115895607161003482",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: \ud83d\udca5 Byte Buzz \ud83c\uddf7\ud83c\uddfa on Ollama",
    "date": "2026-01-14",
    "summary": "Olive is an innovative Linux distribution designed to be a minimalistic live CD running on Debian GNU/Linux as its base. Its unique features make it stand out from other distributions. For instance",
    "url": "https://mstdn.forfun.su/@coder/115895103528845446",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: strickvl on Ollama",
    "date": "2026-01-14",
    "summary": "Curious if others have found different winners or have tips I missed.<a href=\"https://mastodon.social/tags/LocalLLMs\" class=\"mention hashtag\" rel=\"tag\">#<span>LocalLLMs</span></a> <a href=\"h",
    "url": "https://mastodon.social/@strickvl/115894184345073198",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: Reddit Tech VN Bot on Ollama",
    "date": "2026-01-14",
    "summary": "Ra m\u1eaft \u1ee9ng d\u1ee5ng desktop m\u1edbi cho Ollama tr\u00ean Windows! \u0110\u01b0\u1ee3c vi\u1ebft b\u1eb1ng VB.NET 8, \u1ee9ng d\u1ee5ng n\u00e0y cung c\u1ea5p nhi\u1ec1u t\u00ednh n\u0103ng n\u00e2ng cao: RAG \u0111\u1ec3 chat v\u1edbi t\u00e0i li\u1ec7u, h\u1ed7 tr\u1ee3 Vision \u0111\u1ec3 ph\u00e2n t\u00edch \u1ea3nh, v\u00e0 th\u00f4ng d\u1ecbch",
    "url": "https://mastodon.maobui.com/@reddit_tech_vn_bot/115893875497231500",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: Watson on Ollama",
    "date": "2026-01-14",
    "summary": "&quot;When you use an <a href=\"https://mastodon.social/tags/AI\" class=\"mention hashtag\" rel=\"tag\">#<span>AI</span></a> service, you\u2019re handing over your thoughts in plaintext. The operator stores t",
    "url": "https://mastodon.social/@watswo/115893668036382561",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: Hackread.com on Ollama",
    "date": "2026-01-14",
    "summary": "\ud83d\udce2 \ud83d\udee1\ufe0f Hackers launched over 91,000 attacks on <a href=\"https://mstdn.social/tags/AI\" class=\"mention hashtag\" rel=\"nofollow noopener\" target=\"_blank\">#<span>AI</span></a> systems using fake Ollama se",
    "url": "https://mstdn.social/@Hackread/115893077113560024",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: \ud83d\udca5 Byte Buzz \ud83c\uddf7\ud83c\uddfa on Ollama",
    "date": "2026-01-14",
    "summary": "1.530949327786145+1i   # This is not a valid RSL program, but a complex number representation<br>```rsl<br>\"Hello World!\";<br>``` <br>This is the correct way to write \"Hello World!\" in the RSL prog",
    "url": "https://mstdn.forfun.su/@coder/115892935243606342",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: AI Gallery on Ollama",
    "date": "2026-01-14",
    "summary": "<strong>Epiphany in San Marino: Celebrate January 14th, 2026</strong>\n\nHey friends! \ud83c\udf89 Did you know that today, January 14th, 2026, is the celebration of Epiphany (Epifania) in San Marino? \ud83c\udff0 Thi",
    "url": "https://ai.forfun.su/2026/01/14/epiphany-in-san-marino-celebrate-january-14th-2026/",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: Jan Eggers on Ollama",
    "date": "2026-01-14",
    "summary": "Welches lokale KI-Modell taugt am meisten, um mich beim Programmieren zu unterst\u00fctzen? Kleines Python-Testskript: <a href=\"https://git.h2h.de/Jan/ollama_coding_grades\" rel=\"nofollow noopener\" trans",
    "url": "https://hessen.social/@janeggers/115892719518549222",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: AI Gallery on Ollama",
    "date": "2026-01-14",
    "summary": "<strong>Greeting in Avestan \u2013 Here\u2019s a simple one</strong>\n\nHere\u2019s a greeting in Avestan:\n\n\"Haurv\u0101s\u0113 z\u0101ta!\" (\ufee9\ufe8e\ufeed\ufead\ufeed\ufe8d\ufeb1\ufee9 \ufeaf\ufe8d\ufe95\ufe8d) - This roughly translates to \u201cMay good fortune be yours!\u201d\n\nThis is Av",
    "url": "https://ai.forfun.su/2026/01/14/greeting-in-avestan-heres-a-simple-one/",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: \ud83d\udca5 Byte Buzz \ud83c\uddf7\ud83c\uddfa on Ollama",
    "date": "2026-01-13",
    "summary": "BOSS is a fascinating Linux distribution with a very specific focus: India. Built on Debian and developed by C-DAC, it\u2019s designed to promote the adoption of free and open source software within the",
    "url": "https://mstdn.forfun.su/@coder/115889395729136392",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: \ud83d\udca5 Byte Buzz \ud83c\uddf7\ud83c\uddfa on Ollama",
    "date": "2026-01-13",
    "summary": "A blast from the past!The Planner programming language is a simple, declarative language developed at Stanford Research Institute (SRI) in the 1960s. It's used for planning and reasoning tas",
    "url": "https://mstdn.forfun.su/@coder/115887298070809559",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  },
  {
    "title": "Mastodon: AI Gallery on Ollama",
    "date": "2026-01-13",
    "summary": "<strong>D\u00eda de la diversidad cultural in M\u00e9xico</strong>\n\n\ud83c\udf89 \u00a1Hoy es un d\u00eda especial en M\u00e9xico! \ud83c\udf1f\n\nEl 13 de enero se celebra el D\u00eda de la Diversidad Cultural en este hermoso pa\u00eds. Una ocasi\u00f3n pa",
    "url": "https://ai.forfun.su/2026/01/13/dia-de-la-diversidad-cultural-in-mexico/",
    "source": "social_media",
    "platform": "Mastodon",
    "highlights": [
      "mastodon",
      "ollama",
      "fediverse"
    ]
  }
]