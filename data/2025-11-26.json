[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: lightweight, extensible framework for running Llama 2, Mistral, Gemma and other LLMs locally with a single command-line tool and Docker-style model pulls.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-contained binary",
      "model library",
      "OpenAI-compatible API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "First-party Python client for Ollama; chat, generate, embed, pull models with native async support.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "async/await",
      "pip install ollama",
      "embeddings",
      "streaming responses"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript SDK for Node & browsers; same API surface as the Python client.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "npm i ollama",
      "TypeScript types",
      "streaming",
      "browser compatible"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package exposing Ollama models as LLM, chat and embedding components.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install langchain-ollama",
      "chat models",
      "embeddings",
      "tool calling"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (chat, model management, RAG, multi-user, Dark/Light themes).",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Docker image",
      "file upload & RAG",
      "multi-model chat",
      "role-based access"
    ]
  },
  {
    "title": "ollama-litellm",
    "url": "https://github.com/BerriAI/litellm/tree/main/docs/my-website/docs/providers/ollama",
    "summary": "LiteLLM proxy lets you call Ollama models through OpenAI-compatible endpoints with unified interface.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "OpenAI drop-in",
      "cost tracking",
      "load balancing",
      "pip install litellm"
    ]
  },
  {
    "title": "chatbox-ai",
    "url": "https://github.com/Bin-Huang/chatbox",
    "summary": "Cross-platform desktop chat client for Ollama, OpenAI, Claude etc.; supports markdown, code highlight, prompts library.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Electron app",
      "local storage",
      "prompt templates",
      "multi-provider"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration providing OllamaGenerator and OllamaChatGenerator nodes for pipelines.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "pipeline node",
      "generator & chat",
      "pip install haystack-ai[ollama]"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama-ai/ollama-copilot",
    "summary": "Experiment to run a GitHub-Copilot-style code-completion service backed by Ollama models.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "VS Code extension",
      "FIM completions",
      "local GPU",
      "custom model"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama-ai/ollama-rag",
    "summary": "Minimal RAG template using Ollama embeddings + Chroma vector store + Streamlit UI.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Chroma DB",
      "Streamlit",
      "pdf loader",
      "docker-compose"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community Helm chart to deploy Ollama on Kubernetes with GPU support and model auto-pull.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Helm chart",
      "GPU nodeSelector",
      "init model list",
      "Ingress"
    ]
  },
  {
    "title": "ollama-nestjs",
    "url": "https://github.com/fortra/ollama-nestjs",
    "summary": "NestJS module wrapping the Ollama JS client for dependency-injected LLM services.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "npm i ollama-nestjs",
      "Injectable service",
      "async providers",
      "example app"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sammcj/ollama-cli",
    "summary": "Enhanced interactive CLI for Ollama with conversation history, syntax highlighting and prompt snippets.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "readline",
      "history file",
      "themes",
      "snippets"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/ollama-ai/ollama-cookbook",
    "summary": "Community recipes: LangChain agents, function-calling, embeddings, fine-tune, Docker, GPU tweaks.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "recipes",
      "function calling",
      "fine-tune guide",
      "GPU tuning"
    ]
  },
  {
    "title": "ollama-gui",
    "url": "https://github.com/stellar-amen/ollama-gui",
    "summary": "Lightweight Tauri-based desktop GUI for chatting with Ollama models (Rust backend, React frontend).",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Tauri",
      "cross-platform",
      "small binary",
      "themes"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama-ai/ollama-discord",
    "summary": "Discord bot that streams Ollama model responses into channels with slash commands and moderation.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "slash commands",
      "streaming",
      "moderation",
      "Docker image"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/ollama-ai/ollama-slack",
    "summary": "Slack bot using Bolt JS to run Ollama models in threads with role-based access and usage quotas.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Bolt JS",
      "threading",
      "role ACL",
      "quotas"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/shykes/daggerverse/tree/main/ollama",
    "summary": "Dagger module that spins up Ollama service in CI pipelines for testing LLM-powered features.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Dagger cue",
      "CI caching",
      "GPU runners",
      "tests"
    ]
  },
  {
    "title": "ollama-fastapi",
    "url": "https://github.com/1rgs/ollama-fastapi",
    "summary": "FastAPI micro-template exposing Ollama chat/completions endpoints with Pydantic models and Docker.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "FastAPI",
      "Pydantic",
      "Docker",
      "OpenAPI"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama-ai/ollama-obsidian",
    "summary": "Obsidian plugin to run local LLM prompts on selected notes and insert generated text.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "Obsidian plugin",
      "local LLM",
      "prompt templates",
      "community plugin"
    ]
  }
]