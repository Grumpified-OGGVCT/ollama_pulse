[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo \u2013 lightweight, extensible framework for running Llama 2, Mistral, Gemma and other LLMs locally via a simple CLI and REST API.",
    "source": "github",
    "date": "2024-06-12",
    "highlights": [
      "self-contained binary",
      "model library",
      "OpenAI-compatible API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama; chat, embed, pull and manage models from Node or the browser.",
    "source": "github",
    "date": "2024-06-10",
    "highlights": [
      "npm install ollama",
      "Promise-based",
      "full TypeScript defs",
      "ESM & CJS bundles"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama \u2013 synchronous and async clients, streaming chat, embeddings, model management.",
    "source": "github",
    "date": "2024-06-11",
    "highlights": [
      "pip install ollama",
      "asyncio support",
      "Pydantic models",
      "Jupyter friendly"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/llms/ollama",
    "summary": "LangChain integration that wraps Ollama endpoints as LLM and ChatModel components for chains, agents and RAG.",
    "source": "github",
    "date": "2024-06-08",
    "highlights": [
      "pip install langchain-community",
      "streaming",
      "callback support",
      "native tool calling"
    ]
  },
  {
    "title": "ollama-webui (formerly ollama-ui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama \u2013 chat history, multi-user, model manager, OpenAI-compat endpoint, markdown & code highlighting.",
    "source": "github",
    "date": "2024-06-12",
    "highlights": [
      "Docker image",
      "dark/light themes",
      "PWA",
      "role-based auth",
      "RAG via uploads"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/ollama/ollama-cli",
    "summary": "Community-built interactive CLI that wraps the Ollama REST API with readline, syntax highlighting and conversation persistence.",
    "source": "github",
    "date": "2024-05-28",
    "highlights": [
      "npm global install",
      "REPL mode",
      "chat sessions saved as JSON",
      "custom system prompts"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "VS Code extension that brings local Ollama models into GitHub Copilot-style inline suggestions and chat sidebar.",
    "source": "github",
    "date": "2024-06-07",
    "highlights": [
      "inline completions",
      "chat panel",
      "configurable model per task",
      "no cloud required"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Minimal retrieval-augmented-generation example using Ollama embeddings + Chroma vector DB for local document Q&A.",
    "source": "github",
    "date": "2024-06-09",
    "highlights": [
      "Chroma integration",
      "streaming answers",
      "PDF ingestion",
      "all-local pipeline"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama with fluent API, model listing, chat completions and progress callbacks.",
    "source": "github",
    "date": "2024-06-05",
    "highlights": [
      "Maven Central",
      "Kotlin coroutines",
      "Spring Boot starter",
      "reactive streams"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration providing OllamaGenerator and OllamaEmbedder nodes for production-grade RAG pipelines.",
    "source": "github",
    "date": "2024-06-03",
    "highlights": [
      "pip install haystack-ai[ollama]",
      "batch embed",
      "pipeline YAML config",
      "telemetry opt-out"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Official Helm chart to deploy Ollama on Kubernetes with GPU support, PVC model cache and horizontal pod autoscaler.",
    "source": "github",
    "date": "2024-06-10",
    "highlights": [
      "nvidia-device-plugin",
      "configmaps for models",
      "ingress",
      "prometheus metrics"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/NixOS/nixpkgs/tree/master/pkgs/tools/misc/ollama",
    "summary": "Nix flake that builds Ollama from source with CUDA and ROCm variants for reproducible local deployments.",
    "source": "github",
    "date": "2024-06-11",
    "highlights": [
      "nix run",
      "binary cache",
      "rocmSupport",
      "cgO3 optimizations"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/pkgs/container/ollama",
    "summary": "Official multi-arch Docker image (AMD64/ARM64) with automatic GPU runtime detection and volume-mounted model cache.",
    "source": "github",
    "date": "2024-06-12",
    "highlights": [
      "docker pull ollama/ollama",
      "rootless mode",
      "systemd notify",
      "healthcheck endpoint"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/ollama-csharp",
    "summary": ".NET 8 community SDK for Ollama providing async chat, embeddings and model management with nullable ref type support.",
    "source": "github",
    "date": "2024-06-06",
    "highlights": [
      "NuGet OllamaSharp",
      "IAsyncEnumerable streaming",
      "DI-friendly",
      "Blazor sample"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Async Rust client using reqwest and serde for typed chat, pull and delete operations against local Ollama server.",
    "source": "github",
    "date": "2024-06-04",
    "highlights": [
      "crates.io/ollama-rs",
      "tokio runtime",
      "examples folder",
      "no unsafe code"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/sbplat/ollama-discord-bot",
    "summary": "Self-hosted Discord bot that brings local Ollama models into any server with slash commands, thread support and rate limits.",
    "source": "github",
    "date": "2024-06-02",
    "highlights": [
      "slash /ask",
      "conversation threads",
      "per-user quotas",
      "Docker compose"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/lindehoff/ollama-slack",
    "summary": "Slack Bolt app that exposes Ollama as a Slack bot with DM support, app mentions and ephemeral responses.",
    "source": "github",
    "date": "2024-06-01",
    "highlights": [
      "Socket Mode",
      "event subscriptions",
      "interactive modals",
      "gzip compression"
    ]
  },
  {
    "title": "ollama-elixir",
    "url": "https://github.com/benjaminwolkchen/ollama-elixir",
    "summary": "Elixir OTP client for Ollama with GenServer-based connection pooling and telemetry events for observability.",
    "source": "github",
    "date": "2024-05-30",
    "highlights": [
      "Hex.pm package",
      "Nx tensor support",
      "LiveView demo",
      "supervision tree"
    ]
  },
  {
    "title": "ollama-django",
    "url": "https://github.com/mnbf9rca/ollama-django",
    "summary": "Reusable Django app providing REST endpoints and admin UI to manage Ollama models and chat sessions with user quotas.",
    "source": "github",
    "date": "2024-05-29",
    "highlights": [
      "DRF serializers",
      "Swagger UI",
      "celery tasks",
      "redis cache"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hinterdupfinger/ollama-obsidian",
    "summary": "Obsidian plugin that lets you query local Ollama models from within notes via command palette and templater snippets.",
    "source": "github",
    "date": "2024-06-08",
    "highlights": [
      "command: Ollama: Ask",
      "template variables",
      "streaming into editor",
      "offline"
    ]
  }
]