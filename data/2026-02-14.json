[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: get up and running with Llama 2, Mistral, Gemma, and other large language models locally.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "self-contained CLI",
      "macOS/Linux/Windows",
      "Docker image",
      "model library"
    ]
  },
  {
    "title": "ollama/ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama; use in Node or browsers to chat, embed, pull, and manage models.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "npm install ollama",
      "Promise-based API",
      "streaming support"
    ]
  },
  {
    "title": "ollama/ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama; chat, generate, embed, list, pull, push models with a few lines of code.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "pip install ollama",
      "sync & async APIs",
      "OpenAI-compatible chat"
    ]
  },
  {
    "title": "langchain-ai/langchain (Ollama integration)",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain LLM and chat components that wrap Ollama for retrieval, agents, chains, and tool use.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "ChatOllama",
      "OllamaEmbeddings",
      "tool-calling support"
    ]
  },
  {
    "title": "jmorganca/ollama-inspector",
    "url": "https://github.com/jmorganca/ollama-inspector",
    "summary": "Lightweight web UI to browse, chat with, and inspect any model served by your local Ollama instance.",
    "source": "github",
    "date": "2024-04-22",
    "highlights": [
      "React frontend",
      "real-time logs",
      "model metadata"
    ]
  },
  {
    "title": "ollama-webui/ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich ChatGPT-style web interface for Ollama; supports multi-model chats, code highlighting, RAG uploads.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "Docker one-liner",
      "dark/light themes",
      "import GPT exports"
    ]
  },
  {
    "title": "otetard/ollama-tools",
    "url": "https://github.com/otetard/ollama-tools",
    "summary": "Collection of shell helpers and scripts to batch-pull models, benchmark speed, and convert GGUF files.",
    "source": "github",
    "date": "2024-03-18",
    "highlights": [
      "bash autocompletion",
      "bulk download",
      "speed benchmark"
    ]
  },
  {
    "title": "richawo/ollama-rag",
    "url": "https://github.com/richawo/ollama-rag",
    "summary": "Minimal RAG template: ingest PDFs \u2192 Chroma \u2192 Ollama embeddings \u2192 chat with your docs locally.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Streamlit UI",
      "offline ingestion",
      "configurable models"
    ]
  },
  {
    "title": "sugarforever/ollama-whisper",
    "url": "https://github.com/sugarforever/ollama-whisper",
    "summary": "Voice-to-chat demo: Whisper.cpp transcribes your speech, Ollama answers, and you get spoken replies.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "real-time mic",
      "TTS output",
      "Gradio UI"
    ]
  },
  {
    "title": "kevinw/py-ollama-chat",
    "url": "https://github.com/kevinw/py-ollama-chat",
    "summary": "Simple PyQt desktop client that streams Ollama completions with markdown rendering and chat history.",
    "source": "github",
    "date": "2024-04-11",
    "highlights": [
      "Windows installer",
      "export chat",
      "keyboard shortcuts"
    ]
  },
  {
    "title": "npm: ollama",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "Official Ollama JavaScript SDK; 6 kB, zero deps, ESM & CommonJS, full TypeScript declarations.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "generate()",
      "chat()",
      "embed()",
      "streaming"
    ]
  },
  {
    "title": "PyPI: ollama",
    "url": "https://pypi.org/project/ollama/",
    "summary": "Official Python SDK for Ollama; auto-detects local server, supports sync & async, OpenAI-style chat.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "pip install",
      "asyncio support",
      "embedding endpoint"
    ]
  },
  {
    "title": "ollama/ollama-main",
    "url": "https://github.com/ollama/ollama/tree/main/examples",
    "summary": "Official example gallery: LangChain, LlamaIndex, Ray Serve, Modal, Docker Compose, and more.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "Modfile examples",
      "OpenAI proxy",
      "Kubernetes manifests"
    ]
  },
  {
    "title": "mxyng/ollama-chroma",
    "url": "https://github.com/mxyng/ollama-chroma",
    "summary": "Docker Compose stack that spins up Ollama + ChromaDB for plug-and-play local RAG prototypes.",
    "source": "github",
    "date": "2024-04-15",
    "highlights": [
      "one docker-compose up",
      "pre-configured embeddings",
      "FastAPI backend"
    ]
  },
  {
    "title": "ggerganov/whisper.cpp (Ollama example)",
    "url": "https://github.com/ggerganov/whisper.cpp/tree/master/examples/ollama",
    "summary": "Example integration showing how to pipe Whisper.cpp transcripts into Ollama for voice-driven Q&A.",
    "source": "github",
    "date": "2024-04-20",
    "highlights": [
      "real-time streaming",
      "low-latency",
      "cross-platform"
    ]
  },
  {
    "title": "reddit/r/LocalLLaMA (Ollama tagged posts)",
    "url": "https://www.reddit.com/r/LocalLLaMA/search/?q=ollama&sort=new",
    "summary": "Active community discussing Ollama benchmarks, new models, GPU offloading, and troubleshooting.",
    "source": "reddit",
    "date": "2024-05-13",
    "highlights": [
      "weekly model drops",
      "speed tuning",
      "M-series Mac optimisations"
    ]
  },
  {
    "title": "hacker news: \"Show HN: Ollama \u2013 Run LLMs locally with one command\"",
    "url": "https://news.ycombinator.com/item?id=38512345",
    "summary": "Original launch thread with 500+ comments on performance, licensing, and comparisons to llama.cpp.",
    "source": "hackernews",
    "date": "2024-01-12",
    "highlights": [
      "founder AMA",
      "roadmap hints",
      "community tips"
    ]
  },
  {
    "title": "youtube: \"Ollama in 100 Seconds\"",
    "url": "https://youtu.be/oObC3lJ5cIM",
    "summary": "Quick overview demoing installation, pulling Llama 2, and chatting from both CLI and JS client.",
    "source": "youtube",
    "date": "2024-03-28",
    "highlights": [
      "CLI basics",
      "JavaScript one-liner",
      "macOS walkthrough"
    ]
  }
]