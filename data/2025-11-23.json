[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and Python library for downloading, running and chatting with local LLaMA-2, Mistral, Gemma, CodeLlama and other GGUF models via a REST API.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "self-contained binary",
      "REST API",
      "model library",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama/ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama API\u2014chat, generate, pull, push and list models from Node or the browser.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "TypeScript",
      "Promise-based",
      "browser+Node",
      "tree-shakeable"
    ]
  },
  {
    "title": "ollama/ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama; chat, stream, embeddings, pull/push models, built-in async support.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "asyncio",
      "streaming",
      "embeddings",
      "PyPI package"
    ]
  },
  {
    "title": "jmorganca/ollama",
    "url": "https://pypi.org/project/ollama/",
    "summary": "PyPI landing page for the official ollama-python package (mirror of GitHub repo).",
    "source": "pypi",
    "date": "2024-05-10",
    "highlights": [
      "pip install ollama",
      "official",
      "async"
    ]
  },
  {
    "title": "ollama/ollama-npm",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "npm registry page for the official ollama-js SDK.",
    "source": "npm",
    "date": "2024-05-11",
    "highlights": [
      "npm install ollama",
      "TypeScript defs",
      "browser support"
    ]
  },
  {
    "title": "langchain-ai/langchain (Ollama integration)",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain LLM and chat components that wrap Ollama\u2019s local models for RAG, agents and chains.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "LangChain",
      "RAG",
      "streaming",
      "chat model"
    ]
  },
  {
    "title": "langchain-ai/langchainjs (Ollama)",
    "url": "https://js.langchain.com/docs/integrations/llms/ollama",
    "summary": "JavaScript/TypeScript LangChain integration for Ollama\u2014chat, generate, embeddings.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "JS LangChain",
      "TypeScript",
      "embeddings"
    ]
  },
  {
    "title": "embedchain/embedchain (Ollama embedder & LLM)",
    "url": "https://github.com/embedchain/embedchain",
    "summary": "Open-source RAG framework with native Ollama support for embeddings and chat.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "RAG",
      "embeddings",
      "config yaml",
      "stream"
    ]
  },
  {
    "title": "continuedev/continue (VS Code extension)",
    "url": "https://github.com/continuedev/continue",
    "summary": "VS Code/JetBrains plugin that adds local AI coding assistant via Ollama models.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "VS Code",
      "JetBrains",
      "inline autocomplete",
      "chat sidebar"
    ]
  },
  {
    "title": "jina-ai/llama-cpp-python (Ollama compatible)",
    "url": "https://github.com/jina-ai/llama-cpp-python",
    "summary": "High-performance Python bindings for llama.cpp with OpenAI-compatible server that can proxy to Ollama.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "OpenAI API",
      "llama.cpp",
      "fast"
    ]
  },
  {
    "title": "ollama-webui/ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (ChatGPT-like interface) with multi-user, model manager, code highlighting.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "React",
      "self-hosted",
      "dark mode",
      "multi-chat"
    ]
  },
  {
    "title": "otetard/ollama-cli",
    "url": "https://github.com/otetard/ollama-cli",
    "summary": "Lightweight interactive TUI for chatting with Ollama models in the terminal.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "TUI",
      "rust",
      "markdown",
      "keyboard shortcuts"
    ]
  },
  {
    "title": "richawo/ollama-docker",
    "url": "https://github.com/richawo/ollama-docker",
    "summary": "Ready-to-run Docker Compose stack for Ollama + Open WebUI with GPU support.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "docker-compose",
      "CUDA",
      "AMD ROCm",
      "one-liner"
    ]
  },
  {
    "title": "r2d2rigo/ollama-winui",
    "url": "https://github.com/r2d2rigo/ollama-winui",
    "summary": "Native Windows Fluent UI client for Ollama built in .NET MAUI.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Windows",
      ".NET MAUI",
      "MSIX",
      "dark theme"
    ]
  },
  {
    "title": "ollama4j/ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Java/Kotlin client for Ollama with Spring Boot starters and Android compatibility.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Java",
      "Kotlin",
      "Spring Boot",
      "Android"
    ]
  },
  {
    "title": "michaelpoluektov/ollama-r (R wrapper)",
    "url": "https://github.com/michaelpoluektov/ollama-r",
    "summary": "R client for Ollama providing tidyverse-friendly functions for chat and embeddings.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "R",
      "tidyverse",
      "CRAN-ready",
      "stream"
    ]
  },
  {
    "title": "ollama/ollama-hub",
    "url": "https://github.com/ollama/ollama-hub",
    "summary": "Community registry for sharing custom Modelfile templates and quantized GGUF models.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "Modelfile",
      "community",
      "pull-request workflow",
      "tags"
    ]
  },
  {
    "title": "reddit/r/ollama",
    "url": "https://www.reddit.com/r/ollama/",
    "summary": "Subreddit for Ollama users to share models, troubleshooting and integrations.",
    "source": "reddit",
    "date": "2024-05-15",
    "highlights": [
      "community",
      "models",
      "help",
      "showcase"
    ]
  },
  {
    "title": "hacker news: Show HN: Ollama \u2013 Run LLMs locally with one command",
    "url": "https://news.ycombinator.com/item?id=38512345",
    "summary": "Launch discussion covering Ollama v0.1 with benchmarks and feature requests.",
    "source": "hackernews",
    "date": "2024-05-01",
    "highlights": [
      "performance",
      "feedback",
      "roadmap"
    ]
  },
  {
    "title": "blog.ollama.ai \u2013 Introducing Ollama Tool Support",
    "url": "https://blog.ollama.ai/posts/tool-support",
    "summary": "Official post announcing function-calling and tool-use capabilities in Ollama v0.2.",
    "source": "blog",
    "date": "2024-05-14",
    "highlights": [
      "function calling",
      "tools",
      "JSON mode",
      "examples"
    ]
  }
]