[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and library for running Llama 2, Mistral, Gemma, and other large language models locally with GPU/CPU acceleration and a built-in model registry.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "self-contained binaries",
      "Modelfile system",
      "REST & Go APIs",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama server; chat, generate, embed, pull, push, list, delete models via npm.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "Promise-based",
      "browser & Node",
      "streaming support",
      "zero deps"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client on PyPI; identical API surface to JS client plus sync/async generators and native embedding helpers.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "asyncio support",
      "pip install ollama",
      "embeddings helper",
      "type hints"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "First-party LangChain integration; exposes Ollama models as LLM, chat, and embedding components with callback streaming.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip install langchain-ollama",
      "streaming tokens",
      "native embed",
      "tool calling"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Full-featured open-source ChatGPT-style web UI for Ollama; supports multi-user, RAG, code highlighting, and model management.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "Docker image",
      "file upload/RAG",
      "dark/light themes",
      "role-based auth"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Unofficial Java/Kotlin client that wraps the Ollama REST API with synchronous, asynchronous, and reactive (RxJava) helpers.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "Maven Central",
      "Spring Boot starter",
      "Kotlin coroutines",
      "model management"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sugarforever/ollama-cli",
    "summary": "Interactive CLI REPL written in Python that adds conversation memory, syntax highlighting, and markdown rendering on top of Ollama.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "pip install ollama-cli",
      "conversation history",
      "vim mode",
      "themes"
    ]
  },
  {
    "title": "ollama-copilot.vim",
    "url": "https://github.com/github/copilot.vim/discussions/1943",
    "summary": "Neovim plugin that bridges Ollama models into GitHub Copilot-style inline completions and chat pane.",
    "source": "github",
    "date": "2024-05-17",
    "highlights": [
      "inline suggestions",
      "chat pane",
      "fim templates",
      "lazy.nvim ready"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/andrewyng/ollama-rag",
    "summary": "Minimal reference implementation of retrieval-augmented generation using Ollama embeddings + Chroma vector DB.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "Chroma integration",
      "PDF ingestion",
      "streamlit UI",
      "Docker compose"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/ollama/ollama-cookbook",
    "summary": "Community-driven recipes: fine-tune with LoRA, build Discord bots, expose OpenAI-compatible endpoints, etc.",
    "source": "github",
    "date": "2024-05-16",
    "highlights": [
      "LoRA fine-tuning",
      "OpenAI proxy",
      "Discord bot",
      "function calling"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU node-selector and horizontal pod autoscaling.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "GPU scheduling",
      "PVC model cache",
      "ingress",
      "prometheus metrics"
    ]
  },
  {
    "title": "ollama-napi-rs",
    "url": "https://github.com/napi-rs/ollama-napi",
    "summary": "High-performance Node.js native addon written in Rust that exposes Ollama APIs with zero-copy streaming.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "napi-rs",
      "zero-copy",
      "TypeScript types",
      "prebuilt binaries"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration providing OllamaGenerator and OllamaEmbedder nodes for building LLM pipelines.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Haystack 2.x",
      "pipeline YAML",
      "embeddings",
      "batch generate"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Community Rust crate that wraps the Ollama REST API with async/await and Tokio, published on crates.io.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "async tokio",
      "crates.io",
      "serde",
      "examples"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/ollama-csharp",
    "summary": ".NET SDK for Ollama with strongly-typed models, dependency injection helpers, and Blazor sample.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "NuGet package",
      "DI extensions",
      "Blazor chat",
      "streaming"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/shykes/daggerverse/tree/main/ollama",
    "summary": "Dagger module that spins up Ollama as a containerized service for CI pipelines and reproducible dev environments.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Dagger CUE",
      "CI caching",
      "GPU passthrough",
      "devShell"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hinterdupinger/obsidian-ollama",
    "summary": "Obsidian plugin that adds Ollama-powered summarization, continuation, and Q&A commands inside your vault.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "command palette",
      "template variables",
      "local models",
      "community plugin"
    ]
  },
  {
    "title": "ollama-ray",
    "url": "https://github.com/ray-project/ray/tree/master/python/ray/llm/examples/ollama",
    "summary": "Ray Serve example showing how to scale Ollama inference workers across a GPU cluster with autoscaling.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Ray Serve",
      "GPU autoscaling",
      "model sharding",
      "batching"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/mikegajda/ollama-discord-bot",
    "summary": "Self-hostable Discord bot that brings local LLM chat to any server using slash commands and thread memory.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "slash commands",
      "thread memory",
      "Docker",
      "configurable models"
    ]
  },
  {
    "title": "ollama-spring-boot-starter",
    "url": "https://github.com/sanmibuh/ollama-spring-boot-starter",
    "summary": "Spring Boot auto-configuration that exposes Ollama as injectable services with WebFlux streaming endpoints.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Spring Boot 3",
      "WebFlux",
      "actuator metrics",
      "starter"
    ]
  }
]