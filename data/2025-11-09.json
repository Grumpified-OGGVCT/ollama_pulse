[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Ollama CLI and server that lets you pull, run and manage large language models locally.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "CLI",
      "REST API",
      "Model hub",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "langchain-ai/langchain",
    "url": "https://github.com/langchain-ai/langchain",
    "summary": "LangChain\u2019s Ollama integration enables building chains & agents that call local Ollama models via Python/JS.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "Python/JS SDK",
      "Chains",
      "Agents",
      "Streaming"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client library for Ollama; chat, generate embeddings, list/pull models programmatically.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "PyPI package",
      "Embeddings",
      "Async support",
      "Streaming"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node & browsers; same API surface as the Python client.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "npm package",
      "TypeScript",
      "Streaming",
      "Browser support"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (ChatGPT-like interface) with model switching, chat history, multi-user.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "Docker image",
      "Dark/light theme",
      "Import/export chats",
      "Admin panel"
    ]
  },
  {
    "title": "ollama-webui-lite",
    "url": "https://github.com/ollama-webui/ollama-webui-lite",
    "summary": "Lightweight Svelte-based web UI for Ollama under 1 MB, runs entirely in browser.",
    "source": "github",
    "date": "2024-05-17",
    "highlights": [
      "No backend",
      "PWA",
      "Tiny bundle",
      "Mobile friendly"
    ]
  },
  {
    "title": "ollama-gui",
    "url": "https://github.com/jmorganca/ollama-gui",
    "summary": "Minimal Electron desktop GUI for Ollama with tray icon and native notifications.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Electron",
      "System tray",
      "Auto-updater",
      "macOS/Win/Linux"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/salty-flower/ollama-cli",
    "summary": "Interactive TUI for Ollama written in Rust; fuzzy-find models, syntax-highlighted chat.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "Rust TUI",
      "Fuzzy search",
      "Themes",
      "Keybindings"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "Turn any Ollama model into a GitHub Copilot-style code-completion service for VS Code.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "VS Code extension",
      "Inline completions",
      "FIM templates",
      "Multi-language"
    ]
  },
  {
    "title": "ollama-vscode",
    "url": "https://github.com/lfom/ollama-vscode",
    "summary": "VS Code extension that adds a chat sidebar powered by any local Ollama model.",
    "source": "github",
    "date": "2024-05-16",
    "highlights": [
      "Chat sidebar",
      "Slash commands",
      "Code selection",
      "Configurable prompts"
    ]
  },
  {
    "title": "ollama-jetbrains",
    "url": "https://github.com/ollama-jetbrains/ollama-jetbrains",
    "summary": "JetBrains IDE plugin providing inline AI assistance and chat tool-window via Ollama.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "IntelliJ/PyCharm",
      "Inline prompts",
      "Chat window",
      "Token usage stats"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama-obsidian/ollama-obsidian",
    "summary": "Obsidian community plugin to run local LLM prompts inside notes and get inline completions.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "Obsidian plugin",
      "Templater integration",
      "Custom commands",
      "Offline"
    ]
  },
  {
    "title": "ollama-logseq",
    "url": "https://github.com/ollama-logseq/ollama-logseq",
    "summary": "LogSeq plugin for querying Ollama models inside blocks and summarizing pages.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "LogSeq",
      "Block queries",
      "Page summarization",
      "Slash commands"
    ]
  },
  {
    "title": "ollama-raycast",
    "url": "https://github.com/ollama-raycast/ollama-raycast",
    "summary": "Raycast extension to chat with any Ollama model from the macOS launcher.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "Raycast",
      "macOS",
      "Quick chat",
      "Custom hotkeys",
      "History"
    ]
  },
  {
    "title": "ollama-alfred",
    "url": "https://github.com/ollama-alfred/ollama-alfred",
    "summary": "Alfred workflow for macOS that lets you prompt Ollama models and paste results anywhere.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Alfred workflow",
      "Snippet expansion",
      "Arg/vars",
      "No mouse"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama-haystack/ollama-haystack",
    "summary": "Haystack integration package on PyPI to use Ollama models for RAG pipelines and agents.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "PyPI",
      "RAG",
      "Agents",
      "DocumentStore",
      "Embedding support"
    ]
  },
  {
    "title": "ollama-llamaindex",
    "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/llms/llama-index-llms-ollama",
    "summary": "LlamaIndex LLM and embedding integrations for Ollama; pip install llama-index-llms-ollama.",
    "source": "github",
    "date": "2024-05-17",
    "highlights": [
      "pip package",
      "Chat/Stream",
      "Embeddings",
      "RAG examples"
    ]
  },
  {
    "title": "ollama-fastapi",
    "url": "https://github.com/ollama-fastapi/ollama-fastapi",
    "summary": "FastAPI wrapper that exposes Ollama endpoints with extra auth, rate-limiting and OpenAI-compatible routes.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "OpenAI-compat",
      "JWT auth",
      "Rate limits",
      "Docker"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official Docker image and docker-compose examples for running Ollama server plus GPU support.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "Official image",
      "CUDA",
      "ROCm",
      "Compose examples"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama-helm/ollama-helm",
    "summary": "Helm chart to deploy Ollama on Kubernetes with autoscaling and PVC model cache.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "Helm chart",
      "K8s",
      "Autoscale",
      "PVC cache",
      "GPU nodes"
    ]
  }
]