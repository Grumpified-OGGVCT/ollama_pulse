[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Ollama repo: a lightweight, extensible framework for running Llama 2, Code Llama, Mistral, Gemma & other LLMs locally on macOS, Linux, and Windows with one command.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "self-contained binary",
      "model hot-reload",
      "OpenAI-compatible API",
      "GPU/CPU fallback"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python library for Ollama; chat, generate, embed, pull, list, delete models via simple async/sync client.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip install ollama",
      "async/await support",
      "streaming responses",
      "type hints"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama; works in Node & browsers (via proxy) to chat, generate, pull models.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "npm i ollama",
      "Promise-based",
      "streaming support",
      "ESM/CJS bundles"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://pypi.org/project/langchain-ollama/",
    "summary": "PyPI package integrating Ollama models into LangChain as LLM & chat components with callbacks & tool calling.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install langchain-ollama",
      "tool usage",
      "streaming",
      "native embeddings"
    ]
  },
  {
    "title": "ollama-webui (open-webui)",
    "url": "https://github.com/open-webui/open-webui",
    "summary": "Feature-rich, self-hosted web UI for Ollama with chat history, RAG, user roles, and OpenAI-compatible endpoint proxy.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "docker run",
      "file upload & RAG",
      "multi-user",
      "workspace folders"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama exposing sync/async APIs, streaming, and model management; Maven Central available.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Java 8+",
      "Kotlin DSL",
      "reactive streams",
      "Spring Boot starter"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sugarforever/ollama-cli",
    "summary": "Interactive terminal UI (Rust) for browsing, chatting, and managing Ollama models with keyboard shortcuts.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "TUI",
      "fuzzy search",
      "chat history",
      "cross-platform binary"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "VS Code extension that turns any Ollama model into inline GitHub Copilot-like assistant with autocomplete & chat.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "inline suggestions",
      "chat sidebar",
      "custom model pick",
      "no API key"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Minimal RAG example using Ollama embeddings + llama-index to chat with local PDFs entirely offline.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "llama-index",
      "local embeddings",
      "PDF loader",
      "Gradio UI"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Helm chart for deploying Ollama on Kubernetes with GPU node selector, PVC, and optional web-ui sidecar.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "GPU support",
      "autoscaling",
      "ingress",
      "Helm repo"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official multi-arch Docker images (cuda/rocm/cpu) for Ollama with one-liner run commands and compose samples.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "official images",
      "CUDA 12",
      "ROCm 6",
      "compose examples"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-ollama",
    "summary": "Haystack integration letting you use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "pip install haystack-ollama",
      "pipeline YAML",
      "streaming",
      "embeddings"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/ollama-csharp",
    "summary": ".NET client for Ollama with async chat, vision, and model management; NuGet package available.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "NET 6+",
      "vision support",
      "streaming",
      "dependency injection"
    ]
  },
  {
    "title": "ollama-rs",
    "url": "https://github.com/pepperoni21/ollama-rs",
    "summary": "Rust crate providing strongly-typed async client for Ollama with tokio & streaming support; crates.io published.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "async/await",
      "serde",
      "streaming",
      "examples"
    ]
  },
  {
    "title": "ollama-vscode",
    "url": "https://github.com/lf-/ollama-vscode",
    "summary": "Lightweight VS Code extension to chat with any Ollama model inside the editor sidebar or terminal panel.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "sidebar chat",
      "slash commands",
      "configurable model",
      "no telemetry"
    ]
  },
  {
    "title": "ollama-camunda",
    "url": "https://github.com/camunda-community-hub/ollama-camunda",
    "summary": "Community plugin embedding Ollama workers into Camunda 8 workflows for local LLM tasks (classification, summarization).",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "Camunda 8",
      "job workers",
      "BPMN",
      "no cloud LLM fees"
    ]
  },
  {
    "title": "ollama-rust-api",
    "url": "https://crates.io/crates/ollama-api",
    "summary": "Minimal Rust crate exposing typed REST calls to Ollama; published on crates.io with examples.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "zero deps",
      "tokio optional",
      "examples",
      "MIT"
    ]
  },
  {
    "title": "ollama-go",
    "url": "https://github.com/jmorganca/ollama-go",
    "summary": "Community-maintained Go client for Ollama with generate/chat, streaming, and model admin methods.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "go get",
      "context support",
      "streaming",
      "tests"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/solygambas/ollama-discord",
    "summary": "Self-hosted Discord bot that lets servers chat with local Ollama models using slash commands and threads.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "slash commands",
      "thread isolation",
      "multi-model",
      "Docker"
    ]
  },
  {
    "title": "ollama-huggingface",
    "url": "https://huggingface.co/spaces/ollama/ollama",
    "summary": "Hugging Face Space demo running Ollama in-browser via WebGPU; showcases pulling and chatting with models.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "WebGPU",
      "in-browser",
      "no install",
      "open space"
    ]
  }
]