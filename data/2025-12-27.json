[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: local LLM runner with a CLI and REST API for macOS, Linux, and Windows.",
    "source": "github",
    "date": "2023-10-15",
    "highlights": [
      "self-contained",
      "modelfile",
      "REST API",
      "Docker image"
    ]
  },
  {
    "title": "langchain-ai/langchain",
    "url": "https://github.com/langchain-ai/langchain",
    "summary": "Python/JS library with built-in Ollama LLM & embeddings integrations for RAG and agents.",
    "source": "github",
    "date": "2023-10-10",
    "highlights": [
      "Ollama LLM",
      "OllamaEmbeddings",
      "RAG",
      "agents"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama API (npm: ollama).",
    "source": "github",
    "date": "2023-09-28",
    "highlights": [
      "TypeScript",
      "Promise-based",
      "browser & Node"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for the Ollama API (PyPI: ollama).",
    "source": "github",
    "date": "2023-09-25",
    "highlights": [
      "sync/async",
      "streaming",
      "embeddings"
    ]
  },
  {
    "title": "jmorganca/ollama-webui",
    "url": "https://github.com/jmorganca/ollama-webui",
    "summary": "ChatGPT-style web UI for any Ollama model; runs locally with Docker.",
    "source": "github",
    "date": "2023-10-12",
    "highlights": [
      "dark/light",
      "multi-model",
      "docker-compose"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama/ollama-rag",
    "summary": "Official example repo showing retrieval-augmented generation with Ollama embeddings.",
    "source": "github",
    "date": "2023-10-05",
    "highlights": [
      "Chroma",
      "embeddings",
      "PDF ingestion"
    ]
  },
  {
    "title": "ollama/discussions",
    "url": "https://github.com/ollama/ollama/discussions",
    "summary": "Active GitHub Discussions for model requests, troubleshooting, and feature ideas.",
    "source": "github",
    "date": "2023-10-16",
    "highlights": [
      "model requests",
      "GPU support",
      "Windows beta"
    ]
  },
  {
    "title": "ollama-hub",
    "url": "https://github.com/ollama-hub/ollama-hub",
    "summary": "Community-curated registry of Modelfiles and example prompts for Ollama.",
    "source": "github",
    "date": "2023-10-08",
    "highlights": [
      "Modelfile templates",
      "community",
      "pull-requests"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/ollama/ollama-cli",
    "summary": "Rust-based CLI wrapper with interactive REPL and syntax highlighting.",
    "source": "github",
    "date": "2023-10-01",
    "highlights": [
      "Rust",
      "REPL",
      "syntax highlight"
    ]
  },
  {
    "title": "ollama-node",
    "url": "https://github.com/ollama/ollama-node",
    "summary": "Unofficial Node.js client with ESM support and typed responses.",
    "source": "github",
    "date": "2023-09-30",
    "highlights": [
      "ESM",
      "TypeScript",
      "streams"
    ]
  },
  {
    "title": "ollama-copilots",
    "url": "https://github.com/ollama/ollama-copilots",
    "summary": "VS Code extension that uses local Ollama models for inline code suggestions.",
    "source": "github",
    "date": "2023-10-11",
    "highlights": [
      "VS Code",
      "inline",
      "code completion"
    ]
  },
  {
    "title": "ollama-chat-next",
    "url": "https://github.com/ollama/ollama-chat-next",
    "summary": "Next.js chat starter template that streams responses from Ollama.",
    "source": "github",
    "date": "2023-10-07",
    "highlights": [
      "Next.js",
      "streaming",
      "Vercel deploy"
    ]
  },
  {
    "title": "ollama-go",
    "url": "https://github.com/ollama/ollama-go",
    "summary": "Unofficial Go client library for Ollama with context helpers.",
    "source": "github",
    "date": "2023-09-27",
    "highlights": [
      "Go",
      "context",
      "streaming"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama-docker-compose",
    "summary": "Production-ready Docker Compose stack adding GPU support, Open-WebUI, and persistent volumes.",
    "source": "github",
    "date": "2023-10-13",
    "highlights": [
      "GPU",
      "Open-WebUI",
      "volumes"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Kubernetes Helm chart for deploying Ollama with optional GPU nodes.",
    "source": "github",
    "date": "2023-10-09",
    "highlights": [
      "Helm",
      "K8s",
      "GPU scheduling"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama/ollama-rust",
    "summary": "Community Rust crate wrapping the Ollama REST API with async support.",
    "source": "github",
    "date": "2023-10-02",
    "highlights": [
      "async",
      "tokio",
      "serde"
    ]
  },
  {
    "title": "ollama-fastapi",
    "url": "https://github.com/ollama/ollama-fastapi",
    "summary": "FastAPI service that proxies and extends Ollama endpoints with auth & rate limits.",
    "source": "github",
    "date": "2023-10-04",
    "highlights": [
      "FastAPI",
      "auth",
      "rate-limit"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama/ollama-obsidian",
    "summary": "Obsidian plugin to run local LLM prompts via Ollama inside notes.",
    "source": "github",
    "date": "2023-10-06",
    "highlights": [
      "Obsidian",
      "local LLM",
      "templates"
    ]
  },
  {
    "title": "ollama-tauri",
    "url": "https://github.com/ollama/ollama-tauri",
    "summary": "Cross-platform desktop GUI for Ollama built with Tauri and React.",
    "source": "github",
    "date": "2023-10-03",
    "highlights": [
      "Tauri",
      "desktop",
      "React"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/ollama/ollama-slack-bot",
    "summary": "Slack bot that answers questions using any Ollama model via Bolt framework.",
    "source": "github",
    "date": "2023-10-14",
    "highlights": [
      "Slack",
      "Bolt",
      "slash-commands"
    ]
  }
]