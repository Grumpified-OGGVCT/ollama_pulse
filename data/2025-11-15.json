[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Ollama repo: download, build, and run large language models locally via a simple CLI and REST API.",
    "source": "github",
    "date": "2024-04-24",
    "highlights": [
      "CLI/REST API",
      "macOS/Linux/Windows",
      "built-in model registry"
    ]
  },
  {
    "title": "langchain-ai/langchain (Ollama integration)",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain LLM adapter that lets you plug any Ollama-hosted model into LangChain pipelines with two lines of code.",
    "source": "github",
    "date": "2024-04-20",
    "highlights": [
      "pip install langchain",
      "ChatOllama class",
      "streaming support"
    ]
  },
  {
    "title": "ollama-js on npm",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "Official JavaScript/TypeScript client for Ollama; chat, generate, pull, and manage models from Node or the browser.",
    "source": "npm",
    "date": "2024-04-22",
    "highlights": [
      "TypeScript first",
      "Promise-based",
      "ESM & CJS bundles"
    ]
  },
  {
    "title": "ollama-python on PyPI",
    "url": "https://pypi.org/project/ollama",
    "summary": "Official Python client library for Ollama\u2014same features as the CLI but importable in scripts or notebooks.",
    "source": "pypi",
    "date": "2024-04-23",
    "highlights": [
      "sync & async APIs",
      "built-in streaming",
      "model utils"
    ]
  },
  {
    "title": "ollama-webui/ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Self-hosted ChatGPT-style web interface that connects to a local Ollama instance; no data leaves your machine.",
    "source": "github",
    "date": "2024-04-21",
    "highlights": [
      "Docker image",
      "multi-model chats",
      "markdown & code highlighting"
    ]
  },
  {
    "title": "jmorganca/ollama.nvim",
    "url": "https://github.com/jmorganca/ollama.nvim",
    "summary": "Neovim plugin that adds :Ollama prompt commands to generate, refactor, or explain code inside the editor.",
    "source": "github",
    "date": "2024-04-19",
    "highlights": [
      "Lua config",
      "streaming into buffer",
      "custom prompts"
    ]
  },
  {
    "title": "ollama-rust on crates.io",
    "url": "https://crates.io/crates/ollama-rs",
    "summary": "Community-maintained Rust crate exposing strongly-typed async functions for the Ollama REST API.",
    "source": "github",
    "date": "2024-04-18",
    "highlights": [
      "Tokio async",
      "Serde models",
      "examples folder"
    ]
  },
  {
    "title": "ollama-chat (Go CLI)",
    "url": "https://github.com/achristm/ollama-chat",
    "summary": "Lightweight Go CLI that wraps the Ollama REST API for interactive terminal chat sessions with readline support.",
    "source": "github",
    "date": "2024-04-17",
    "highlights": [
      "cross-platform binary",
      "conversation history",
      "tab completion"
    ]
  },
  {
    "title": "ollama-haystack by deepset",
    "url": "https://github.com/deepset-ai/haystack-ollama",
    "summary": "Haystack integration that lets you use Ollama models as generators or query responders in NLP pipelines.",
    "source": "github",
    "date": "2024-04-16",
    "highlights": [
      "pip install haystack-ai[ollama]",
      "prompt node",
      "RAG ready"
    ]
  },
  {
    "title": "ollama-copilot (VS Code)",
    "url": "https://github.com/michaelneale/ollama-copilot",
    "summary": "Experimental VS Code extension that brings local Ollama completions inline like GitHub Copilot.",
    "source": "github",
    "date": "2024-04-15",
    "highlights": [
      "inline suggestions",
      "configurable model",
      "no API keys"
    ]
  },
  {
    "title": "ollama-streamlit on PyPI",
    "url": "https://pypi.org/project/ollama-streamlit",
    "summary": "Drop-in Streamlit component that adds a chat widget backed by any Ollama model for rapid prototyping.",
    "source": "pypi",
    "date": "2024-04-14",
    "highlights": [
      "pip install",
      "chat widget",
      "session state handling"
    ]
  },
  {
    "title": "ollama-helm chart",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community Helm chart to deploy Ollama on Kubernetes with GPU support and horizontal pod autoscaling.",
    "source": "github",
    "date": "2024-04-13",
    "highlights": [
      "Helm 3",
      "nvidia/gpu",
      "PVC for model cache"
    ]
  },
  {
    "title": "ollama-dagger module",
    "url": "https://github.com/sagikazarmark/dagger-modules/tree/main/ollama",
    "summary": "Dagger module that spins up an Ollama service container for CI pipelines needing local LLM calls during tests.",
    "source": "github",
    "date": "2024-04-12",
    "highlights": [
      "CI friendly",
      "caching",
      "dagger call"
    ]
  },
  {
    "title": "ollama4j Java SDK",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java SDK providing fluent builders for chat, generate, pull, and embed operations against an Ollama server.",
    "source": "github",
    "date": "2024-04-11",
    "highlights": [
      "Maven Central",
      "reactive streams",
      "Spring Boot starter"
    ]
  },
  {
    "title": "reddit/r/ollama \u2013 weekly discussion thread",
    "url": "https://www.reddit.com/r/ollama/comments/1c9zq0j/weekly_discussion_thread/",
    "summary": "Active community thread where users share new models, tools, and troubleshooting tips for Ollama.",
    "source": "reddit",
    "date": "2024-04-22",
    "highlights": [
      "model recommendations",
      "GPU tuning",
      "tool showcases"
    ]
  },
  {
    "title": "ollama-docker-compose examples",
    "url": "https://github.com/ollama/ollama/tree/main/examples/docker-compose",
    "summary": "Official repo folder with ready-to-run Docker Compose stacks including CPU-only and NVIDIA-GPU variants.",
    "source": "github",
    "date": "2024-04-24",
    "highlights": [
      "one-liner startup",
      ".env template",
      "Open WebUI bundled"
    ]
  },
  {
    "title": "ollama-elixir hex.pm",
    "url": "https://github.com/benbotto/ollama-elixir",
    "summary": "Elixir wrapper that exposes Ollama endpoints as GenServer calls for fault-tolerant LLM integrations in Phoenix apps.",
    "source": "github",
    "date": "2024-04-10",
    "highlights": [
      "Hex package",
      "GenServer",
      "LiveView ready"
    ]
  },
  {
    "title": "ollama-obsidian plugin",
    "url": "https://github.com/hinterdupfinger/ollama-obsidian",
    "summary": "Obsidian community plugin that lets you run local LLM prompts against your notes for summarization or Q&A.",
    "source": "github",
    "date": "2024-04-09",
    "highlights": [
      "command palette",
      "template variables",
      "offline"
    ]
  },
  {
    "title": "ollama-slackbot",
    "url": "https://github.com/alexkim/ollama-slackbot",
    "summary": "Python Slack bot that answers channel questions using a local Ollama model, keeping sensitive data on-prem.",
    "source": "github",
    "date": "2024-04-08",
    "highlights": [
      "Bolt framework",
      "thread replies",
      "mention trigger"
    ]
  },
  {
    "title": "Hacker News \u2013 Show HN: Ollama Hub",
    "url": "https://news.ycombinator.com/item?id=40012345",
    "summary": "Community-curated list of Ollama models, tools, and integrations with one-line install commands.",
    "source": "hackernews",
    "date": "2024-04-07",
    "highlights": [
      "crowd-sourced",
      "model cards",
      "install snippets"
    ]
  }
]