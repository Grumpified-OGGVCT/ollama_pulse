[
  {
    "title": "How to connect to MySQL database with SQLAlchemy when running the application and MySQL running as a Docker container?",
    "date": "2025-12-13T20:28:30",
    "summary": "Stack Overflow question with 0 answers, 33 views",
    "url": "https://stackoverflow.com/questions/79846734/how-to-connect-to-mysql-database-with-sqlalchemy-when-running-the-application-an",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 33",
      "answers: 0",
      "score: 0",
      "tags: mysql, docker, sqlalchemy"
    ]
  },
  {
    "title": "How to integrate Gemma 3 with Visual Studio Code?",
    "date": "2026-01-09T22:37:57",
    "summary": "Stack Overflow question with 0 answers, 67 views",
    "url": "https://stackoverflow.com/questions/79864691/how-to-integrate-gemma-3-with-visual-studio-code",
    "source": "stackoverflow",
    "turbo_score": -1.8,
    "highlights": [
      "views: 67",
      "answers: 0",
      "score: -7",
      "tags: visual-studio-code, vscode-extensions, ollama"
    ]
  },
  {
    "title": "Import \"llama_index.llms.ollama\" could not be resolved",
    "date": "2025-08-13T15:07:32",
    "summary": "Stack Overflow question with 1 answers, 183 views",
    "url": "https://stackoverflow.com/questions/79734455/import-llama-index-llms-ollama-could-not-be-resolved",
    "source": "stackoverflow",
    "turbo_score": 1.0,
    "highlights": [
      "views: 183",
      "answers: 1",
      "score: 2",
      "tags: python, python-venv, llama"
    ]
  },
  {
    "title": "json.loads() fails with JSONDecodeError: Extra data when parsing LLM output (Ollama router)",
    "date": "2026-01-08T09:12:29",
    "summary": "Stack Overflow question with 2 answers, 90 views",
    "url": "https://stackoverflow.com/questions/79863122/json-loads-fails-with-jsondecodeerror-extra-data-when-parsing-llm-output-oll",
    "source": "stackoverflow",
    "turbo_score": 1.0,
    "highlights": [
      "views: 90",
      "answers: 2",
      "score: 0",
      "tags: python, json, ollama"
    ]
  },
  {
    "title": "Improve RAGFlow RAG Search and chunk embeddings",
    "date": "2026-01-06T05:21:36",
    "summary": "Stack Overflow question with 0 answers, 24 views",
    "url": "https://stackoverflow.com/questions/79861296/improve-ragflow-rag-search-and-chunk-embeddings",
    "source": "stackoverflow",
    "turbo_score": -0.06,
    "highlights": [
      "views: 24",
      "answers: 0",
      "score: -1",
      "tags: python, large-language-model, ollama"
    ]
  },
  {
    "title": "Problems using local OLLAMA during embedding to ChromaDB",
    "date": "2026-01-04T11:00:59",
    "summary": "Stack Overflow question with 0 answers, 98 views",
    "url": "https://stackoverflow.com/questions/79860107/problems-using-local-ollama-during-embedding-to-chromadb",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 98",
      "answers: 0",
      "score: 0",
      "tags: python, large-language-model, embedding"
    ]
  },
  {
    "title": "How do you disable the \"Potentially dangerous command\" warning in the continue extension for vscode?",
    "date": "2025-11-26T21:34:05",
    "summary": "Stack Overflow question with 1 answers, 105 views",
    "url": "https://stackoverflow.com/questions/79831170/how-do-you-disable-the-potentially-dangerous-command-warning-in-the-continue-e",
    "source": "stackoverflow",
    "turbo_score": 0.7,
    "highlights": [
      "views: 105",
      "answers: 1",
      "score: 0",
      "tags: visual-studio-code, artificial-intelligence, vscode-extensions"
    ]
  },
  {
    "title": "Making an Ollama Model read dict/json data properly",
    "date": "2025-12-04T15:50:14",
    "summary": "Stack Overflow question with 1 answers, 85 views",
    "url": "https://stackoverflow.com/questions/79838108/making-an-ollama-model-read-dict-json-data-properly",
    "source": "stackoverflow",
    "turbo_score": 0.7,
    "highlights": [
      "views: 85",
      "answers: 1",
      "score: 0",
      "tags: python, json, ollama"
    ]
  },
  {
    "title": "Incorrect output when running opencode with an ollama provider",
    "date": "2025-12-19T12:30:37",
    "summary": "Stack Overflow question with 0 answers, 90 views",
    "url": "https://stackoverflow.com/questions/79851100/incorrect-output-when-running-opencode-with-an-ollama-provider",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 90",
      "answers: 0",
      "score: 0",
      "tags: ollama"
    ]
  },
  {
    "title": "Is there a limit to file size upload on Ollama Web UI",
    "date": "2024-03-07T05:41:13",
    "summary": "Stack Overflow question with 1 answers, 2003 views",
    "url": "https://stackoverflow.com/questions/78118990/is-there-a-limit-to-file-size-upload-on-ollama-web-ui",
    "source": "stackoverflow",
    "turbo_score": 0.4,
    "highlights": [
      "views: 2003",
      "answers: 1",
      "score: -1",
      "tags: ollama"
    ]
  },
  {
    "title": "How to fix EOF Server Ollama Error when embedding",
    "date": "2025-12-04T06:59:52",
    "summary": "Stack Overflow question with 0 answers, 422 views",
    "url": "https://stackoverflow.com/questions/79837611/how-to-fix-eof-server-ollama-error-when-embedding",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 422",
      "answers: 0",
      "score: 0",
      "tags: laravel, artificial-intelligence, embedding"
    ]
  },
  {
    "title": "llama_index Ollama misloading model issue",
    "date": "2025-12-05T02:26:43",
    "summary": "Stack Overflow question with 1 answers, 125 views",
    "url": "https://stackoverflow.com/questions/79838505/llama-index-ollama-misloading-model-issue",
    "source": "stackoverflow",
    "turbo_score": 1.0,
    "highlights": [
      "views: 125",
      "answers: 1",
      "score: 1",
      "tags: artificial-intelligence, large-language-model, llama-index"
    ]
  },
  {
    "title": "How to run a local Open Source LLM in llama-index in a restricted environment?",
    "date": "2024-05-13T10:41:03",
    "summary": "Stack Overflow question with 1 answers, 1585 views",
    "url": "https://stackoverflow.com/questions/78471692/how-to-run-a-local-open-source-llm-in-llama-index-in-a-restricted-environment",
    "source": "stackoverflow",
    "turbo_score": 0.7,
    "highlights": [
      "views: 1585",
      "answers: 1",
      "score: 0",
      "tags: large-language-model, huggingface, llama-index"
    ]
  },
  {
    "title": "llama3.2 Installation Error: exiting with status 0xc0000135",
    "date": "2024-10-10T00:15:55",
    "summary": "Stack Overflow question with 2 answers, 999 views",
    "url": "https://stackoverflow.com/questions/79072393/llama3-2-installation-error-exiting-with-status-0xc0000135",
    "source": "stackoverflow",
    "turbo_score": 1.0,
    "highlights": [
      "views: 999",
      "answers: 2",
      "score: 0",
      "tags: ollama, llama3"
    ]
  },
  {
    "title": "Ollama: Inconsistent Responses and `done:false` with mistral-small3.1:24b",
    "date": "2025-11-25T11:10:05",
    "summary": "Stack Overflow question with 0 answers, 45 views",
    "url": "https://stackoverflow.com/questions/79829612/ollama-inconsistent-responses-and-donefalse-with-mistral-small3-124b",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 45",
      "answers: 0",
      "score: 0",
      "tags: ollama, mistral-ai"
    ]
  },
  {
    "title": "Is there a way to stream ollama chat into a Marimo ui?",
    "date": "2025-11-23T20:25:04",
    "summary": "Stack Overflow question with 0 answers, 44 views",
    "url": "https://stackoverflow.com/questions/79828094/is-there-a-way-to-stream-ollama-chat-into-a-marimo-ui",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 44",
      "answers: 0",
      "score: 0",
      "tags: python, chatbot, ollama"
    ]
  },
  {
    "title": "How is the output from prompt responses formated in the tools?",
    "date": "2025-11-18T19:05:50",
    "summary": "Stack Overflow question with 0 answers, 25 views",
    "url": "https://stackoverflow.com/questions/79823740/how-is-the-output-from-prompt-responses-formated-in-the-tools",
    "source": "stackoverflow",
    "turbo_score": 0.25,
    "highlights": [
      "views: 25",
      "answers: 0",
      "score: 0",
      "tags: large-language-model"
    ]
  },
  {
    "title": "AI Ollama console application, pass an image to the LLM using KernelFunction",
    "date": "2025-11-12T16:55:00",
    "summary": "Stack Overflow question with 0 answers, 91 views",
    "url": "https://stackoverflow.com/questions/79818028/ai-ollama-console-application-pass-an-image-to-the-llm-using-kernelfunction",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 91",
      "answers: 0",
      "score: 0",
      "tags: c#, .net-9.0, ollama"
    ]
  },
  {
    "title": "Which LLMs can I run locally on RTX 1080 8GB with 48GB RAM?",
    "date": "2025-11-10T15:46:14",
    "summary": "Stack Overflow question with 0 answers, 139 views",
    "url": "https://stackoverflow.com/questions/79815816/which-llms-can-i-run-locally-on-rtx-1080-8gb-with-48gb-ram",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 139",
      "answers: 0",
      "score: 0",
      "tags: gpu, large-language-model, ollama"
    ]
  },
  {
    "title": "Qwen 2.5 3B VLM Index error at the line trainer.train()",
    "date": "2025-11-06T14:30:18",
    "summary": "Stack Overflow question with 0 answers, 143 views",
    "url": "https://stackoverflow.com/questions/79811390/qwen-2-5-3b-vlm-index-error-at-the-line-trainer-train",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 143",
      "answers: 0",
      "score: 0",
      "tags: python, large-language-model, index-error"
    ]
  },
  {
    "title": "I want to know where to locate the file I upload though the ragflow system, how to find it in the windows system",
    "date": "2025-03-19T01:22:03",
    "summary": "Stack Overflow question with 1 answers, 174 views",
    "url": "https://stackoverflow.com/questions/79518917/i-want-to-know-where-to-locate-the-file-i-upload-though-the-ragflow-system-how",
    "source": "stackoverflow",
    "turbo_score": -0.5,
    "highlights": [
      "views: 174",
      "answers: 1",
      "score: -4",
      "tags: rag"
    ]
  },
  {
    "title": "AgentWorkflow doesn't call functions when using Ollama",
    "date": "2025-10-21T08:46:28",
    "summary": "Stack Overflow question with 0 answers, 49 views",
    "url": "https://stackoverflow.com/questions/79795621/agentworkflow-doesnt-call-functions-when-using-ollama",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 49",
      "answers: 0",
      "score: 0",
      "tags: python, typescript, artificial-intelligence"
    ]
  },
  {
    "title": "How do I pass an image to DSPy for analysis?",
    "date": "2025-05-28T11:28:21",
    "summary": "Stack Overflow question with 1 answers, 833 views",
    "url": "https://stackoverflow.com/questions/79642103/how-do-i-pass-an-image-to-dspy-for-analysis",
    "source": "stackoverflow",
    "turbo_score": 1.0,
    "highlights": [
      "views: 833",
      "answers: 1",
      "score: 1",
      "tags: ollama, dspy"
    ]
  },
  {
    "title": "Model not found Scrapegraph-ai",
    "date": "2024-08-12T09:32:01",
    "summary": "Stack Overflow question with 2 answers, 612 views",
    "url": "https://stackoverflow.com/questions/78860941/model-not-found-scrapegraph-ai",
    "source": "stackoverflow",
    "turbo_score": 1.0,
    "highlights": [
      "views: 612",
      "answers: 2",
      "score: 0",
      "tags: python, web-scraping, artificial-intelligence"
    ]
  },
  {
    "title": "How to stop Ollama model streaming",
    "date": "2024-07-12T23:11:33",
    "summary": "Stack Overflow question with 1 answers, 2302 views",
    "url": "https://stackoverflow.com/questions/78742490/how-to-stop-ollama-model-streaming",
    "source": "stackoverflow",
    "turbo_score": 1.0,
    "highlights": [
      "views: 2302",
      "answers: 1",
      "score: 1",
      "tags: python, websocket, fastapi"
    ]
  },
  {
    "title": "How to stream LLM responses in a Shiny app instead of waiting for full output?",
    "date": "2025-09-17T15:41:54",
    "summary": "Stack Overflow question with 1 answers, 313 views",
    "url": "https://stackoverflow.com/questions/79767548/how-to-stream-llm-responses-in-a-shiny-app-instead-of-waiting-for-full-output",
    "source": "stackoverflow",
    "turbo_score": 1.0,
    "highlights": [
      "views: 313",
      "answers: 1",
      "score: 5",
      "tags: r, shiny, large-language-model"
    ]
  },
  {
    "title": "Llama Stack Agent not invoking MCP server in Docker setup despite tool group being registered",
    "date": "2025-09-24T09:10:13",
    "summary": "Stack Overflow question with 0 answers, 76 views",
    "url": "https://stackoverflow.com/questions/79773500/llama-stack-agent-not-invoking-mcp-server-in-docker-setup-despite-tool-group-bei",
    "source": "stackoverflow",
    "turbo_score": 0.6,
    "highlights": [
      "views: 76",
      "answers: 0",
      "score: 1",
      "tags: amazon-web-services, docker, docker-compose"
    ]
  },
  {
    "title": "Running Ollama on local computer and prompting from jupyter notebook - does the model recall prior prompts like if it was the same chat?",
    "date": "2025-09-23T23:35:57",
    "summary": "Stack Overflow question with 0 answers, 60 views",
    "url": "https://stackoverflow.com/questions/79773153/running-ollama-on-local-computer-and-prompting-from-jupyter-notebook-does-the",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 60",
      "answers: 0",
      "score: 0",
      "tags: large-language-model, llama, ollama"
    ]
  },
  {
    "title": "langgraph with ollama not responding with a response",
    "date": "2025-09-23T14:01:30",
    "summary": "Stack Overflow question with 0 answers, 104 views",
    "url": "https://stackoverflow.com/questions/79772697/langgraph-with-ollama-not-responding-with-a-response",
    "source": "stackoverflow",
    "turbo_score": 0.3,
    "highlights": [
      "views: 104",
      "answers: 0",
      "score: 0",
      "tags: python, langchain, ollama"
    ]
  },
  {
    "title": "Pycharm: \"Python Interpreter exited with non-zero exit code -1\" when connecting to an existing Docker Compose Service",
    "date": "2025-09-16T11:25:45",
    "summary": "Stack Overflow question with 1 answers, 232 views",
    "url": "https://stackoverflow.com/questions/79766168/pycharm-python-interpreter-exited-with-non-zero-exit-code-1-when-connecting",
    "source": "stackoverflow",
    "turbo_score": 1.0,
    "highlights": [
      "views: 232",
      "answers: 1",
      "score: 3",
      "tags: python, docker, pycharm"
    ]
  }
]