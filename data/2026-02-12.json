[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and server for running large language models locally with minimal setup; supports Llama 2, Mistral, Gemma, etc.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-contained binary",
      "macOS/Linux/Windows",
      "Docker image",
      "OpenAI-compatible API"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client library for the Ollama server; chat, generate, embed, pull, list models.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install ollama",
      "sync/async APIs",
      "streaming support",
      "BSD-3 license"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node & browsers; same capabilities as Python client.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "npm i ollama",
      "ESM & CommonJS",
      "type definitions",
      "streaming support"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package exposing Ollama models as LLM, chat, and embedding components.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pip install langchain-ollama",
      "unified interface",
      "retrieval chains",
      "tool calling"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (renamed to Open WebUI); chat history, model management, RAG, multi-user.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Docker one-liner",
      "markdown/math rendering",
      "document upload",
      "role-based access"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama; synchronous & async APIs, fluent builder style.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Maven Central",
      "Kotlin coroutines",
      "Spring Boot starter",
      "Apache-2"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/gbaptista/ollama-rb",
    "summary": "Ruby gem wrapping the Ollama REST API; generate, chat, pull, list, copy, delete models.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "gem install ollama",
      "streaming blocks",
      "ActiveSupport-style",
      "MIT license"
    ]
  },
  {
    "title": "ollama-cli (mattw/ollama-cli)",
    "url": "https://github.com/mattw/ollama-cli",
    "summary": "Interactive REPL and command wrapper around ollama with shell completion, syntax highlighting.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Go binary",
      "readline prompts",
      "fuzzy model search",
      "config profiles"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Experimental GitHub Copilot proxy that routes completions through a local Ollama model.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "Neovim plugin",
      "VS Code extension",
      "self-hosted copilot",
      "MIT license"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Helm chart to deploy Ollama server and models on Kubernetes with autoscaling and PVC support.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "custom model lists",
      "GPU node selector",
      "ingress ready",
      "Apache-2"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official container images (CPU & CUDA) with docker-compose examples for persistent storage.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "scratch-based",
      "multi-arch",
      "CUDA 11/12 tags",
      "rootless mode"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration providing OllamaGenerator and OllamaChatGenerator nodes for pipelines.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "pip install ollama-haystack",
      "YAML pipelines",
      "RAG examples",
      "Apache-2"
    ]
  },
  {
    "title": "ollama-curl",
    "url": "https://github.com/technovangelist/ollama-curl",
    "summary": "Collection of copy-paste curl snippets for every Ollama API endpoint with JSON examples.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "generate/chat/embed",
      "streaming vs non-stream",
      "image input",
      "MIT license"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/Maximilian-Winter/ollama-discord",
    "summary": "Discord bot that brings local LLM chat to servers using slash commands and thread support.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "Python bot",
      "thread isolation",
      "model hot-swap",
      "Docker ready"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/royjhan/ollama-slack",
    "summary": "Slack Bolt app integrating Ollama models as a conversational bot with DM and channel modes.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "Socket Mode",
      "app mentions",
      "modelfile switch",
      "env var config"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/shykes/ollama-dagger",
    "summary": "Dagger module that caches and serves Ollama models inside CI pipelines for reproducible LLM tests.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "CUE config",
      "GitHub Actions example",
      "model pinning",
      "Apache-2"
    ]
  },
  {
    "title": "ollama-nim",
    "url": "https://github.com/ire4ever1190/ollama-nim",
    "summary": "Nim client for Ollama with async HTTP, jsony marshalling, and nimble package distribution.",
    "source": "github",
    "date": "2024-04-24",
    "highlights": [
      "nimble install ollama",
      "asyncdispatch",
      "strict types",
      "MIT license"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Rust crate providing strongly-typed async client for the Ollama REST API with tokio.",
    "source": "github",
    "date": "2024-04-23",
    "highlights": [
      "cargo add ollama-rs",
      "tokio streams",
      "serde models",
      "Apache-2"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/ollama-csharp",
    "summary": ".NET client library for Ollama targeting .NET 6+ with async enumerable streaming.",
    "source": "github",
    "date": "2024-04-22",
    "highlights": [
      "NuGet Ollama",
      "System.Text.Json",
      "ASP.NET sample",
      "MIT license"
    ]
  },
  {
    "title": "ollama-zig",
    "url": "https://github.com/leroycep/ollama-zig",
    "summary": "Zig client implementation for Ollama using std.http with allocator-aware JSON parsing.",
    "source": "github",
    "date": "2024-04-21",
    "highlights": [
      "zig fetch",
      "allocator pattern",
      "streaming JSON",
      "MIT license"
    ]
  }
]