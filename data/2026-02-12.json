[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: self-contained LLM runner for macOS/Linux with a REST/CLI API and a growing model library.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "CLI + REST API",
      "built-in model hub",
      "macOS/Linux binaries"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK that wraps the Ollama REST API for easy local LLM inference.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install ollama",
      "async/sync clients",
      "chat & embed endpoints"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript SDK for Node and browsers to talk to Ollama.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "npm i ollama",
      "Promise-based",
      "TypeScript types included"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/langchain-ollama",
    "summary": "LangChain integration package letting you plug Ollama models into chains, agents, RAG, etc.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pip install langchain-ollama",
      "ChatOllama & OllamaEmbeddings",
      "RAG ready"
    ]
  },
  {
    "title": "ollama-webui (ollama-web/Ollama-WebUI)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI (React/Flask) that talks to a local Ollama instance; supports multi-model chats, code highlighting, RAG uploads.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Docker image available",
      "file upload & RAG",
      "dark/light themes"
    ]
  },
  {
    "title": "ollama-cli (maccoy/ollama-cli)",
    "url": "https://github.com/maccoy/ollama-cli",
    "summary": "Rust-based TUI client for Ollama with conversation history, syntax highlighting and keyboard-centric workflow.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "cross-platform binary",
      "conversation save/load",
      "themes"
    ]
  },
  {
    "title": "ollama-copilot (cmiller01/ollama-copilot)",
    "url": "https://github.com/cmiller01/ollama-copilot",
    "summary": "GitHub Copilot-style VS-Code extension that uses your own Ollama models for inline suggestions.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "VS-Code ext",
      "inline completions",
      "configurable model"
    ]
  },
  {
    "title": "ollama-helm (otwld/ollama-helm)",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community-maintained Helm chart to deploy Ollama (with optional GPU support) on Kubernetes.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Helm chart",
      "GPU nodeSelector",
      "persistent model cache"
    ]
  },
  {
    "title": "ollama-rag (ggerganov/ollama-rag)",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Minimal RAG example that spins up Ollama + Chroma + LangChain to chat over your documents locally.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "docker-compose stack",
      "Chroma vector DB",
      "ingest CLI"
    ]
  },
  {
    "title": "ollama-nvim (nomnivore/ollama-nvim)",
    "url": "https://github.com/nomnivore/ollama-nvim",
    "summary": "Neovim plugin to generate, refactor or explain code using any model served by Ollama.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "Lua config",
      "selection \u2192 prompt",
      "streaming answers"
    ]
  },
  {
    "title": "ollama-dagger (dagger/daggerverse/tree/main/ollama)",
    "url": "https://github.com/dagger/daggerverse/tree/main/ollama",
    "summary": "Dagger module that lets CI pipelines pull models and run inference via Ollama containers.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Dagger cue module",
      "cache-friendly",
      "multi-arch"
    ]
  },
  {
    "title": "ollama-streamlit (sauljabin/ollama-streamlit)",
    "url": "https://github.com/sauljabin/ollama-streamlit",
    "summary": "Clean Streamlit chat interface that connects to local Ollama, supports multiple concurrent models and markdown rendering.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "pip install streamlit-ollama",
      "multi-model sidebar",
      "session memory"
    ]
  },
  {
    "title": "ollama-haystack (deepset-ai/haystack-integrations/tree/main/integrations/ollama)",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration providing OllamaGenerator and OllamaEmbedder nodes for building production pipelines.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "pip install haystack-ai[ollama]",
      "pipeline YAML support",
      "batching"
    ]
  },
  {
    "title": "ollama-embeddings (mxbai/ollama-embeddings)",
    "url": "https://github.com/mxbai/ollama-embeddings",
    "summary": "Lightweight Python package focused solely on generating embeddings through Ollama\u2019s embed endpoint with numpy helpers.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "pip install ollama-embeddings",
      "numpy arrays out",
      "batch encode"
    ]
  },
  {
    "title": "ollama-curl (jmorganca/ollama-curl)",
    "url": "https://github.com/jmorganca/ollama-curl",
    "summary": "Collection of copy-paste curl snippets showing every Ollama REST endpoint in one place.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "curl examples",
      "generate/chat/embed",
      "JSON streaming"
    ]
  },
  {
    "title": "ollama-discord (rikkriuk/ollama-discord)",
    "url": "https://github.com/rikkriuk/ollama-discord",
    "summary": "Self-hostable Discord bot that answers questions using your local Ollama models, supports slash commands and per-guild config.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "slash commands",
      "guild-scoped models",
      "Dockerfile"
    ]
  },
  {
    "title": "ollama-qt (foldleft/ollama-qt)",
    "url": "https://github.com/foldleft/ollama-qt",
    "summary": "Cross-platform desktop GUI built with Qt6 and Python that wraps Ollama for point-and-click model management and chatting.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "Qt6 UI",
      "model pull GUI",
      "chat history"
    ]
  },
  {
    "title": "ollama-slack (knksmith/ollama-slack)",
    "url": "https://github.com/knksmith/ollama-slack",
    "summary": "Slack Bolt app that listens in channels/DM and responds via Ollama, includes rate-limiting and admin allow-lists.",
    "source": "github",
    "date": "2024-04-24",
    "highlights": [
      "Bolt JS",
      "rate-limit",
      "allow-list"
    ]
  },
  {
    "title": "ollama-obsidian (joethei/ollama-obsidian)",
    "url": "https://github.com/joethei/obsidian-ollama",
    "summary": "Obsidian plugin to summarize notes, brainstorm ideas or generate text using any Ollama model from within your vault.",
    "source": "github",
    "date": "2024-04-23",
    "highlights": [
      "command palette",
      "template prompts",
      "custom model"
    ]
  },
  {
    "title": "ollama-models (library/ollama)",
    "url": "https://github.com/ollama/ollama/tree/main/library",
    "summary": "Official curated model library (Modelfiles) for Llama 3, Phi-3, Mistral, Gemma, etc., ready to pull via \u2018ollama run\u2019.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "Modelfile templates",
      "quantized sizes",
      "community submissions"
    ]
  }
]