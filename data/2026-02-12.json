[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and server for running large language models locally (Llama 2, Mistral, Gemma, etc.) with a simple pull/run workflow.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-contained binary",
      "Docker image",
      "OpenAI-compatible API",
      "model library"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama server; chat, generate, embed, list, pull, delete models.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "npm install ollama",
      "Promise-based",
      "browser & Node",
      "TypeScript defs"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client; synchronous & async APIs for chat, generate, embeddings, model management.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install ollama",
      "async/await",
      "streaming responses",
      "embeddings"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package exposing Ollama models as LLM, chat, and embedding components.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pip install langchain-ollama",
      "LLM/Chat/Embeddings",
      "streaming",
      "callback support"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (chat, model manager, multi-user, OpenAI-compat endpoint).",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "dark/light themes",
      "code highlighting",
      "multi-model chat",
      "Docker ready"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Java/Kotlin client library for Ollama; synchronous & async APIs, POJO model mapping.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Maven Central",
      "Kotlin coroutines",
      "streaming",
      "embeddings"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Official Ruby gem for Ollama; chat, generate, pull, delete, list, copy, show models.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "gem install ollama",
      "streaming blocks",
      "Ruby 3.x",
      "RDoc"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/ollama/ollama-cli",
    "summary": "Community Rust CLI that wraps the Ollama REST API with prompts history and REPL.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Rust binary",
      "REPL",
      "history file",
      "cross-platform"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "VS Code extension bringing local LLM code completion via Ollama (inline & panel).",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "inline suggestions",
      "configurable model",
      "status bar toggle"
    ]
  },
  {
    "title": "ollama-chatbot-ui",
    "url": "https://github.com/ollama/ollama-chatbot-ui",
    "summary": "Next.js chatbot template that talks to Ollama; Markdown, code highlight, streaming.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Next.js 14",
      "Tailwind",
      "Vercel deploy",
      ".env model choice"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Helm chart to deploy Ollama server on Kubernetes with GPU & PVC support.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "GPU nodeSelector",
      "persistent storage",
      "ingress",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official Docker image (CPU & GPU) and compose examples for Ollama server.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "official image",
      "cuda/rocm tags",
      "docker-compose.yml",
      "rootless"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration by deepset allowing Ollama models as generators & embedders.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "pip install ollama-haystack",
      "Generator & Embedder",
      "Haystack 2.x"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/ollama/ollama-csharp",
    "summary": "Community C#/.NET client for Ollama with async streaming and strongly-typed responses.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "NuGet package",
      ".NET 6+",
      "IAsyncEnumerable",
      "System.Text.Json"
    ]
  },
  {
    "title": "ollama-spring-boot-starter",
    "url": "https://github.com/ollama-spring/ollama-spring-boot-starter",
    "summary": "Spring Boot starter auto-configuring Ollama REST client beans for Java apps.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Spring Boot 3",
      "autoconfiguration",
      "@OllamaClient",
      "application.yml"
    ]
  },
  {
    "title": "ollama-n8n-node",
    "url": "https://github.com/n8n-io/n8n/tree/master/packages/nodes-base/nodes/Ollama",
    "summary": "n8n community node to call Ollama chat & generate workflows without code.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "n8n node",
      "no-code",
      "workflow automation",
      "credentials"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama-obsidian/ollama-obsidian",
    "summary": "Obsidian plugin for local AI assistance using Ollama models inside notes.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Obsidian plugin",
      "command palette",
      "template prompts",
      "streaming"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama/ollama-rust",
    "summary": "Unofficial Rust crate providing typed async bindings to the Ollama API.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "crates.io",
      "tokio",
      "serde",
      "streaming"
    ]
  },
  {
    "title": "ollama-go",
    "url": "https://github.com/ollama/ollama-go",
    "summary": "Community Go client library for Ollama with context cancellation and streaming.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "go get",
      "context.Context",
      "io.Reader streaming",
      "structs"
    ]
  },
  {
    "title": "ollama-django",
    "url": "https://github.com/ollama-django/ollama-django",
    "summary": "Reusable Django app adding Ollama chat & embed endpoints with admin UI.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Django app",
      "admin integration",
      "REST endpoints",
      "async views"
    ]
  }
]