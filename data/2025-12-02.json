[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: get up and running with Llama 3, Mistral, Gemma and other large language models locally.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "self-hosted LLM runtime",
      "Docker/CLI installers",
      "model library pull/push"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama API (npm: ollama).",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Promise-based client",
      "chat & generate endpoints",
      "streaming support"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama (PyPI: ollama).",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "sync/async APIs",
      "embeddings endpoint",
      "built-in model management helpers"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package (PyPI: langchain-ollama) to use Ollama models as LLMs or embedders.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "chat & embeddings interfaces",
      "streaming",
      "callback support"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (Docker image ollama-webui) with chat, model manager, RAG.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "responsive UI",
      "multi-user sessions",
      "document upload & RAG"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/ollama/ollama-cli",
    "summary": "Community-enhanced interactive CLI wrapper around ollama with REPL, history, syntax highlighting.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "readline REPL",
      "conversation history",
      "themeable output"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "VS Code extension that turns any Ollama model into a GitHub Copilot-like pair programmer.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "inline completions",
      "custom model pick",
      "status-bar toggle"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Lightweight RAG template using Ollama embeddings + ChromaDB for question-answering over documents.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "PDF ingestion",
      "Chroma persistence",
      "Gradio demo UI"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-ollama",
    "summary": "Haystack integration (PyPI: haystack-ollama) exposing Ollama models as Generators & Embedders.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Haystack 2.x nodes",
      "streaming answers",
      "model warm-up"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama-discord/ollama-discord",
    "summary": "Self-hostable Discord bot that brings Ollama chat to any server with slash commands.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "slash commands",
      "per-guild model config",
      "conversation threads"
    ]
  },
  {
    "title": "ollama-n8n-node",
    "url": "https://github.com/n8n-io/n8n/tree/master/packages/nodes-base/nodes/Ollama",
    "summary": "Official n8n node (npm: n8n-nodes-ollama) to invoke Ollama models inside workflows.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "no-code workflows",
      "prompt templating",
      "credentials support"
    ]
  },
  {
    "title": "ollama-fastapi",
    "url": "https://github.com/ollama-fastapi/ollama-fastapi",
    "summary": "FastAPI service that wraps Ollama with OpenAI-compatible /v1/chat/completions & embeddings endpoints.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "drop-in OpenAI replacement",
      "/v1/models listing",
      "streaming JSON"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama-helm/ollama-helm",
    "summary": "Community Helm chart for deploying Ollama on Kubernetes with GPU & PVC support.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "GPU node-selector",
      "model init containers",
      "HPA ready"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama-rust/ollama-rust",
    "summary": "Async Rust client crate (crates.io: ollama) for chatting and pulling models from Ollama.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "tokio-based",
      "serde models",
      "streaming via futures"
    ]
  },
  {
    "title": "ollama-django",
    "url": "https://github.com/ollama-django/ollama-django",
    "summary": "Reusable Django app (PyPI: django-ollama) adding Ollama chat & embeddings to admin & views.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "admin chat widget",
      "model forms",
      "async views"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/ollama-slack/ollama-slack",
    "summary": "Slack Bolt app that lets teams chat with any Ollama model through mentions or DMs.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "mention trigger",
      "threaded replies",
      "model switching"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama-obsidian/ollama-obsidian",
    "summary": "Obsidian plugin that offers AI autocomplete, summarization & Q&A powered by local Ollama models.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "editor autocomplete",
      "command palette",
      "local privacy"
    ]
  },
  {
    "title": "ollama-rustic",
    "url": "https://github.com/mdrokz/ollama-rustic",
    "summary": "Lightweight TUI dashboard in Rust for monitoring Ollama models, GPU usage & logs.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "real-time GPU stats",
      "model switcher",
      "log tail"
    ]
  },
  {
    "title": "ollama-nextchat",
    "url": "https://github.com/ollama-nextchat/ollama-nextchat",
    "summary": "Next.js chat interface styled like ChatGPT that connects to any Ollama backend.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "Next.js 14",
      "streaming responses",
      "theme & i18n"
    ]
  },
  {
    "title": "ollama-node",
    "url": "https://github.com/ollama/ollama-node",
    "summary": "Official community Node.js client (npm: ollama-node) for Ollama with ESM/CJS support.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "TypeScript defs",
      "stream utils",
      "model listing"
    ]
  }
]