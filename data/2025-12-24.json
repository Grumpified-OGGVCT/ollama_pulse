[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: get up and running with Llama 3, Mistral, Gemma and other large language models locally.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-contained LLM runner",
      "macOS/Linux/Windows binaries",
      "Docker images",
      "REST & CLI APIs"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama; chat, embed, pull and manage models with a few lines of code.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip install ollama",
      "sync & async APIs",
      "built-in embedding helpers"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node and browsers; same feature parity as the Python SDK.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "npm install ollama",
      "TypeScript definitions",
      "streaming chat support"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package that wraps Ollama as an LLM, chat and embeddings provider.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "pip install langchain-ollama",
      "drop-in replacement for OpenAI",
      "streaming & batch support"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (chat, model management, RAG, multi-user) \u2013 runs in Docker.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Markdown & code highlighting",
      "document upload/RAG",
      "OpenAI-compatible API proxy"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama; offers synchronous & reactive APIs and Spring Boot starters.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Maven Central",
      "Spring Boot auto-config",
      "Kotlin coroutines support"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Community-maintained Ruby gem for chatting and managing models via Ollama.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "gem install ollama",
      "interactive console example",
      "Rails helpers"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sugarforever/ollama-cli",
    "summary": "Conversational TUI for Ollama written in Go; supports chat history and markdown rendering.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "go install",
      "vim-style keybinds",
      "configurable model aliases"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "VS Code extension that pipes GitHub Copilot prompts to a local Ollama model for offline completions.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "inline suggestions",
      "configurable model per language",
      "privacy first"
    ]
  },
  {
    "title": "ollama-chem",
    "url": "https://github.com/ncfrey/ollama-chem",
    "summary": "Chemistry-aware fine-tune of Llama 3 hosted on Ollama for IUPAC naming and property prediction.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "custom Modelfile",
      "PubChem embeddings",
      "CLI demo"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU autoscaling.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "GPU node-selector",
      "PVC model cache",
      "Prometheus metrics"
    ]
  },
  {
    "title": "ollama-nim",
    "url": "https://github.com/ire4ever1190/ollama-nim",
    "summary": "Nim language wrapper for Ollama\u2019s REST API; includes macro-based async DSL.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Nimble package",
      "zero-copy streaming",
      "compile-time route checks"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/sagikazarmark/daggerverse/tree/main/ollama",
    "summary": "Dagger module that spins up Ollama as a CI service for testing LLM-powered features in pipelines.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "cached model layers",
      "GitHub Actions example",
      "multi-arch images"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ivanfioravivi/ollama-rag",
    "summary": "Minimal RAG template using Ollama embeddings + ChromaDB + Streamlit to chat with PDFs offline.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Docker Compose one-liner",
      "GPU/CPU toggle",
      "chunk-size slider UI"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/mxyng/ollama-discord",
    "summary": "Discord bot that answers questions using any Ollama model; supports threads and slash commands.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Docker image",
      "role-based model access",
      "conversation memory"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/arnavkankani/ollama-slack",
    "summary": "Slack Bolt app that brings Ollama models into channels with configurable system prompts.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Socket-mode support",
      "emoji reaction triggers",
      "usage analytics"
    ]
  },
  {
    "title": "ollama-mistral-ft",
    "url": "https://github.com/michaelthomasletts/ollama-mistral-ft",
    "summary": "Tutorial repo showing how to quantize and serve custom Mistral-7B fine-tunes via Ollama.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "GGUF conversion",
      "Modelfile walkthrough",
      "benchmark numbers"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hoooolock/ollama-obsidian",
    "summary": "Obsidian plugin that adds an LLM sidebar powered by any Ollama model for note summarization.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "offline vault search",
      "template prompts",
      "hot-key invocation"
    ]
  },
  {
    "title": "ollama-on-termux",
    "url": "https://github.com/almighty-007/ollama-on-termux",
    "summary": "Step-by-step guide and scripts to compile and run Ollama on Android via Termux (no root).",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "proot-distro helper",
      "CPU-only quantized models",
      "4 GB RAM minimum"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Community Rust crate providing strongly-typed async bindings to Ollama\u2019s REST API.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "crates.io release",
      "tokio & reqwest",
      "serde schemas"
    ]
  }
]