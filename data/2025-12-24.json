[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo \u2013 CLI and Go library to pull, run, and manage quantized LLMs locally with a built-in model registry and OpenAI-compatible REST API.",
    "source": "github",
    "date": "2024-03-15",
    "highlights": [
      "self-hosted LLM runner",
      "Modelfile system",
      "OpenAI drop-in API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama; chat, embed, pull, and manage models with a few lines of code.",
    "source": "github",
    "date": "2024-03-12",
    "highlights": [
      "pip install ollama",
      "sync & async APIs",
      "embedding support"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript SDK for Node & browsers; chat, stream, and list local models.",
    "source": "github",
    "date": "2024-03-10",
    "highlights": [
      "npm install ollama",
      "streaming chat",
      "TypeScript defs"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain adapter that adds Ollama models as first-class LLM & embedding providers.",
    "source": "github",
    "date": "2024-03-08",
    "highlights": [
      "pip install langchain-ollama",
      "chat & embeddings",
      "chain & agent ready"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama\u2014chat folders, multi-user, code highlighting, RAG, & OpenAI-compatible proxy.",
    "source": "github",
    "date": "2024-03-14",
    "highlights": [
      "Docker one-liner",
      "RAG uploads",
      "multi-model chats",
      "dark/light themes"
    ]
  },
  {
    "title": "ollama-cli-copilot",
    "url": "https://github.com/sugarforever/ollama-cli-copilot",
    "summary": "Terminal copilot that uses Ollama models to suggest shell commands on demand.",
    "source": "github",
    "date": "2024-03-05",
    "highlights": [
      "bash/zsh integration",
      "explain generated commands",
      "configurable model"
    ]
  },
  {
    "title": "ollama-copilot.vim",
    "url": "https://github.com/github/copilot.vim/compare/main...ollama-copilot",
    "summary": "Experimental fork replacing GitHub Copilot backend with a local Ollama model inside Vim/Neovim.",
    "source": "github",
    "date": "2024-02-28",
    "highlights": [
      "pure-local completions",
      "zero-config install",
      "Neovim lua"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/pegasus-lynx/ollama-rag",
    "summary": "Minimal RAG template\u2014chunk documents, embed with Ollama, and chat over your data.",
    "source": "github",
    "date": "2024-03-09",
    "highlights": [
      "Chroma vector store",
      "streamlit UI",
      "PDF ingestion"
    ]
  },
  {
    "title": "ollama-agents",
    "url": "https://github.com/andrewyng/ollama-agents",
    "summary": "Lightweight multi-agent orchestration layer on top of Ollama for tool-calling workflows.",
    "source": "github",
    "date": "2024-03-07",
    "highlights": [
      "ReAct pattern",
      "local tools",
      "JSON mode"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart to deploy Ollama on Kubernetes with GPU autoscaling.",
    "source": "github",
    "date": "2024-03-11",
    "highlights": [
      "GPU node-selector",
      "PVC model cache",
      "HPA support"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-ollama",
    "summary": "Haystack integration package letting pipelines use Ollama models for generation & embeddings.",
    "source": "github",
    "date": "2024-03-06",
    "highlights": [
      "pip install haystack-ollama",
      "pipeline nodes",
      "embedding retriever"
    ]
  },
  {
    "title": "ollama-camunda-bridge",
    "url": "https://github.com/camunda-community-hub/ollama-camunda",
    "summary": "Camunda 8 connector that delegates service tasks to Ollama models for human-like decisions.",
    "source": "github",
    "date": "2024-02-25",
    "highlights": [
      "BPMN task worker",
      "zero-code setup",
      "JSON output mapping"
    ]
  },
  {
    "title": "ollama-n8n-node",
    "url": "https://github.com/n8n-io/n8n/tree/master/packages/nodes-base/nodes/Ollama",
    "summary": "n8n community node to call Ollama chat & completion endpoints inside no-code workflows.",
    "source": "github",
    "date": "2024-03-03",
    "highlights": [
      "n8n node",
      "workflow triggers",
      "credential management"
    ]
  },
  {
    "title": "ollama-grafana-datasource",
    "url": "https://github.com/yesoreyeram/grafana-ollama-datasource",
    "summary": "Grafana datasource plugin to visualize Ollama model metrics & prompt/response latency.",
    "source": "github",
    "date": "2024-02-20",
    "highlights": [
      "Grafana panel",
      "live latency",
      "token throughput"
    ]
  },
  {
    "title": "ollama-pulumi-provider",
    "url": "https://github.com/pulumi/registry/tree/master/themes/default/content/registry/packages/ollama",
    "summary": "Pulumi provider to provision Ollama instances on any cloud with GPU support as infrastructure-as-code.",
    "source": "github",
    "date": "2024-03-01",
    "highlights": [
      "TypeScript/Python SDK",
      "GPU VM auto-provision",
      "model cache setup"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama-rs/ollama-rs",
    "summary": "Community-built Rust crate exposing async/sync APIs for chat, embeddings, and model management.",
    "source": "github",
    "date": "2024-03-13",
    "highlights": [
      "cargo add ollama-rs",
      "tokio runtime",
      "strong typing"
    ]
  },
  {
    "title": "ollama-django",
    "url": "https://github.com/andreyfedoseev/django-ollama",
    "summary": "Reusable Django app adding Ollama-backed chat & completion REST endpoints with admin UI.",
    "source": "github",
    "date": "2024-03-04",
    "highlights": [
      "Django REST",
      "admin model manager",
      "streaming HTTP"
    ]
  },
  {
    "title": "ollama-elixir",
    "url": "https://github.com/iamjarvo/ollama_ex",
    "summary": "Elixir wrapper that brings Ollama chat & embeddings into Phoenix/LiveView apps.",
    "source": "github",
    "date": "2024-02-27",
    "highlights": [
      "Hex.pm package",
      "LiveView components",
      "GenServer client"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official Docker Compose stacks including GPU runtime, CPU-only, and bundled WebUI variants.",
    "source": "github",
    "date": "2024-03-15",
    "highlights": [
      "nvidia-docker",
      "CPU fallback",
      "WebUI service"
    ]
  },
  {
    "title": "ollama-reddit-discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1bcy23a/ollama_v010_released_with_tools_calling_support/",
    "summary": "Community thread covering the new tool-calling feature in Ollama v0.10 and example use-cases.",
    "source": "reddit",
    "date": "2024-03-14",
    "highlights": [
      "tool calling",
      "function JSON",
      "community examples"
    ]
  }
]