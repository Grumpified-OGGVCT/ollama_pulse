[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: self-host LLMs (Llama 2, Mistral, Gemma, \u2026) behind a simple REST/CLI API.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "built-in model library",
      "one-line install",
      "OpenAI-compatible endpoints",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama; works in Node, Bun, Deno & browsers.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Promise-based",
      "streaming support",
      "npm i ollama"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama; sync & async, generator streaming, PyPI package.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip install ollama",
      "asyncio support",
      "type hints"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain adapter to use Ollama models for chat, embeddings, tool-calling & RAG.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install langchain-ollama",
      "native tool use",
      "streaming",
      "embeddings"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Full-featured ChatGPT-style web UI for Ollama (channels, RAG, admin panel).",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Docker ready",
      "file upload & RAG",
      "multi-user",
      "OpenAI-compatible API proxy"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/ollama/ollama-cli",
    "summary": "Rich interactive CLI wrapper around ollama with chat history & markdown rendering.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "REPL mode",
      "syntax highlighting",
      "conversation save/load"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "VS Code extension that turns Ollama models into inline code copilots.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "inline completions",
      "custom model pick",
      "status-bar toggle"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/helm-charts",
    "summary": "Community-maintained Helm chart to deploy Ollama on Kubernetes with GPU support.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "GPU node-selector",
      "persistent storage",
      "HPA ready"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama/ollama-rag",
    "summary": "Minimal RAG template using Ollama embeddings + Chroma + Streamlit.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "PDF ingestion",
      "Chroma vector store",
      "Streamlit UI"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Java/Kotlin client for Ollama with Spring Boot starters and reactive support.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Maven Central",
      "Spring Boot auto-config",
      "Project Loom friendly"
    ]
  },
  {
    "title": "ollama-nvidia-docker",
    "url": "https://github.com/ollama/ollama-nvidia-docker",
    "summary": "One-command Docker Compose for Ollama with NVIDIA runtime & GPU memory tuning.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "nvidia-docker",
      "runtime args",
      "CUDA env vars"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama/obsidian-ollama",
    "summary": "Obsidian plugin that lets you query local Ollama models from within notes.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "command palette",
      "template variables",
      "offline"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration to use Ollama models as generators & embedders in pipelines.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "pipeline nodes",
      "batch embedding",
      "pip install farm-haystack[ollama]"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama-discord/ollama-bot",
    "summary": "Self-hostable Discord bot that answers via Ollama with slash commands & context.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "slash commands",
      "conversation threads",
      "role-based access"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/ollama-slack/ollama-slack",
    "summary": "Slack Bolt app bringing Ollama completions to channels/DMs with rate limits.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Bolt JS",
      "rate-limit middleware",
      "app manifest"
    ]
  },
  {
    "title": "ollama-macos-menu",
    "url": "https://github.com/ollama/macos-menu-bar",
    "summary": "Swift menu-bar app to start/stop Ollama service and switch models from macOS toolbar.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "SwiftUI",
      "launchctl integration",
      "model dropdown"
    ]
  },
  {
    "title": "ollama-express",
    "url": "https://github.com/ollama-express/ollama-express",
    "summary": "Express.js starter exposing Ollama endpoints with JWT auth & rate limiting.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "JWT middleware",
      "swagger docs",
      "Dockerfile"
    ]
  },
  {
    "title": "ollama-terraform",
    "url": "https://github.com/ollama-terraform/aws-ollama",
    "summary": "Terraform module to spin up GPU-enabled EC2 instances with Ollama pre-installed.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "g5.xlarge spot",
      "cloud-init Ollama",
      "ebs persistence"
    ]
  }
]