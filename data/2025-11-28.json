[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: lightweight, extensible framework for running LLMs locally with a simple CLI and REST API.",
    "source": "github",
    "date": "2023-07-15",
    "highlights": [
      "self-contained binaries",
      "model library",
      "Docker images",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama API, published on npm as 'ollama'.",
    "source": "github",
    "date": "2023-10-02",
    "highlights": [
      "Promise-based",
      "ESM/CommonJS",
      "full TypeScript defs",
      "chat & embed endpoints"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama, available on PyPI; supports sync/async, streaming, embeddings.",
    "source": "github",
    "date": "2023-09-20",
    "highlights": [
      "asyncio support",
      "Pydantic models",
      "embeddings API",
      "pull/generate/chat"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package (PyPI: langchain-ollama) exposing Ollama models as LLM & embeddings.",
    "source": "github",
    "date": "2023-11-05",
    "highlights": [
      "LangChain LLM interface",
      "embeddings wrapper",
      "streaming",
      "callback support"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI for Ollama (Docker-ready) with multi-model chats, code highlighting, RAG.",
    "source": "github",
    "date": "2023-12-01",
    "highlights": [
      "Responsive UI",
      "file uploads",
      "RAG via LangChain",
      "admin settings",
      "Dark mode"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama; offers fluent builder API and Android compatibility (Maven Central).",
    "source": "github",
    "date": "2023-10-18",
    "highlights": [
      "Java 8+",
      "Android support",
      "async streaming",
      "model management"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/saltyorg/ollama-cli",
    "summary": "Community Rust CLI that wraps Ollama REST endpoints with ergonomic commands and shell completions.",
    "source": "github",
    "date": "2023-11-12",
    "highlights": [
      "Rust binary",
      "shell completions",
      "config profiles",
      "JSON output"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Experimental GitHub Copilot-style VS-Code extension that uses local Ollama models for inline suggestions.",
    "source": "github",
    "date": "2023-12-05",
    "highlights": [
      "VS-Code extension",
      "inline completions",
      "configurable model",
      "privacy focused"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Community-maintained Ruby gem wrapping the Ollama API with ActiveModel-style interfaces.",
    "source": "github",
    "date": "2023-10-30",
    "highlights": [
      "Ruby gem",
      "streaming support",
      "Rails friendly",
      "model utils"
    ]
  },
  {
    "title": "ollama-laravel",
    "url": "https://github.com/llama-assistant/ollama-laravel",
    "summary": "Laravel package providing a service provider and facades for chatting with Ollama models.",
    "source": "github",
    "date": "2023-11-20",
    "highlights": [
      "Laravel facade",
      "queue jobs",
      "streaming responses",
      "configurable host"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official Docker image and compose stacks (CPU/GPU) for running Ollama server containerized.",
    "source": "github",
    "date": "2023-09-01",
    "highlights": [
      "Official image",
      "CUDA support",
      "AMD ROCm",
      "rootless mode"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community Helm chart for deploying Ollama on Kubernetes with GPU node-selector and autoscaling.",
    "source": "github",
    "date": "2023-11-25",
    "highlights": [
      "Helm chart",
      "GPU scheduling",
      "PVC for models",
      "HPA support"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/ollama",
    "summary": "Haystack 2.0 integration offering OllamaGenerator and OllamaChatGenerator components.",
    "source": "github",
    "date": "2023-12-10",
    "highlights": [
      "Haystack nodes",
      "chat & generate",
      "streaming",
      "pipeline ready"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/ollama-csharp",
    "summary": "Community .NET SDK for Ollama supporting .NET 6+ with async enumerable streaming.",
    "source": "github",
    "date": "2023-10-14",
    "highlights": [
      ".NET 6+",
      "IAsyncEnumerable",
      "model management",
      "strong naming"
    ]
  },
  {
    "title": "ollama-cord",
    "url": "https://github.com/ollama/ollama-cord",
    "summary": "Discord bot boilerplate that plugs Ollama models into slash commands with conversation memory.",
    "source": "github",
    "date": "2023-11-08",
    "highlights": [
      "Discord.js",
      "slash commands",
      "conversation memory",
      "Dockerfile"
    ]
  },
  {
    "title": "ollama-streamlit",
    "url": "https://github.com/rain-1/ollama-streamlit",
    "summary": "Minimal Streamlit chat app that talks to a local Ollama instance with session memory.",
    "source": "github",
    "date": "2023-10-22",
    "highlights": [
      "Streamlit UI",
      "session state",
      "markdown rendering",
      "one-file app"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Reference RAG pipeline using Ollama embeddings + llama.cpp for retrieval and generation.",
    "source": "github",
    "date": "2023-11-30",
    "highlights": [
      "ChromaDB",
      "sentence transformers",
      "streaming answers",
      "PDF ingestion"
    ]
  },
  {
    "title": "ollama-nestjs",
    "url": "https://github.com/peerfich/ollama-nestjs",
    "summary": "NestJS module that injects Ollama client as a provider with RxJS streaming support.",
    "source": "github",
    "date": "2023-12-03",
    "highlights": [
      "NestJS provider",
      "RxJS observables",
      "swagger docs",
      "config module"
    ]
  }
]