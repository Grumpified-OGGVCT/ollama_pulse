[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and server for running large language models locally (Llama 2, Mistral, Gemma, etc.) with a simple pull/run workflow.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-contained binary",
      "Docker image",
      "REST & OpenAI-compatible API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "First-party Python client library for the Ollama server; chat, generate, embed, pull, list models.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "sync & async APIs",
      "streaming support",
      "PyPI: ollama"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node & browsers; same operations as Python client.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "npm: ollama",
      "TypeScript types included",
      "streaming fetch"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package providing LLM, embedder and retriever wrappers around Ollama.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip: langchain-ollama",
      "chat & embed",
      "native tool-calling"
    ]
  },
  {
    "title": "llama-index-integrations-llms-ollama",
    "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/llms/llama-index-llms-ollama",
    "summary": "LlamaIndex LLM and embedding drivers for Ollama; plug local models into RAG pipelines.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "pip: llama-index-llms-ollama",
      "streaming",
      "custom prompt helper"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI (React/Flask) for Ollama servers; multi-model chats, code highlighting, RAG uploads.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Docker image",
      "OpenAI-compatible endpoint",
      "PDF/CSV ingestion"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sigma67/ollama-cli",
    "summary": "Interactive terminal UI (Python) for chatting with Ollama models; history, markdown, syntax highlight.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "pip: ollama-cli",
      "keybindings like nano",
      "themes"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama4coding/ollama-copilot",
    "summary": "Visual Studio Code extension that uses Ollama models for inline code completion and chat sidebar.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "local inference",
      "configurable model per language",
      "FIM templates"
    ]
  },
  {
    "title": "ollama-ray",
    "url": "https://github.com/ray-project/ray/tree/master/python/ray/llm/examples/ollama",
    "summary": "Ray Serve example deploying Ollama for scalable local LLM serving across GPUs/nodes.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "multi-GPU",
      "autoscaling",
      "Ray dashboard metrics"
    ]
  },
  {
    "title": "ollama-tools",
    "url": "https://github.com/jmorganca/ollama-tools",
    "summary": "Experimental repo demonstrating function-calling/tool-use with Ollama models via constrained JSON output.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "pydantic schemas",
      "automatic retries",
      "examples: weather, shell"
    ]
  },
  {
    "title": "ollama-chatbot",
    "url": "https://github.com/ollama4coding/ollama-chatbot",
    "summary": "Streamlit chatbot template that chains Ollama + LangChain + memory for quick POCs.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "session memory",
      "Dockerfile",
      "one-click deploy to HuggingFace Spaces"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama4coding/ollama-rag",
    "summary": "Minimal RAG starter using Ollama embeddings + Chroma vector store + LlamaIndex.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "ingest folder of docs",
      "Gradio UI",
      "switch models via env var"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community-maintained Helm chart for running Ollama in Kubernetes with GPU support.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "NodeSelector for GPU",
      "persistent volume for models",
      "autoscaler"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official container images (CPU & CUDA) and docker-compose examples for Ollama server.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "rootless image",
      "CUDA 12",
      "compose with Open WebUI"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/NixOS/nixpkgs/tree/nixos-unstable/pkgs/tools/misc/ollama",
    "summary": "Nix package derivation for Ollama enabling reproducible installs on Linux/macOS.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "CUDA & ROCm variants",
      "systemd service",
      "cacheable binary"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/ollama4coding/ollama-discord-bot",
    "summary": "Discord.py bot that answers questions using local Ollama models; supports slash commands and threads.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "rate limiting",
      "per-guild model config",
      "streaming replies"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama4coding/ollama-obsidian",
    "summary": "Obsidian plugin for AI-assisted writing using Ollama; templates, summarize selection, custom prompts.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "local LLM",
      "community template repo",
      "hotkey support"
    ]
  },
  {
    "title": "ollama4coding on GitHub",
    "url": "https://github.com/ollama4coding",
    "summary": "Organization collecting IDE plugins, bots and templates all built on Ollama.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "VS Code, JetBrains, Discord, Slack integrations"
    ]
  },
  {
    "title": "r/ollama",
    "url": "https://www.reddit.com/r/ollama/",
    "summary": "Active subreddit for sharing Ollama tips, new models, benchmarks and troubleshooting.",
    "source": "reddit",
    "date": "2024-05-10",
    "highlights": [
      "17k members",
      "weekly model threads",
      "hardware performance"
    ]
  },
  {
    "title": "Ollama on Hacker News",
    "url": "https://news.ycombinator.com/item?id=38820150",
    "summary": "Launch discussion covering Ollama v0.1.0 with benchmarks versus llama.cpp and Docker workflow.",
    "source": "hackernews",
    "date": "2024-01-12",
    "highlights": [
      "300+ comments",
      "M1 vs RTX 4090 metrics",
      "open-source MIT license"
    ]
  }
]