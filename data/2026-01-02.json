[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: self-hosted LLM runner with a simple CLI and REST API that lets you pull, run, and chat with 70+ open-source models (Llama 3, Mistral, Gemma, etc.) on macOS, Linux, and Windows.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "docker image",
      "REST API",
      "model library",
      "GPU/CPU inference",
      "open-source"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "First-party Python client and async wrapper for the Ollama API; install via pip install ollama.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "async support",
      "chat & generate",
      "embeddings endpoint",
      "PyPI package"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node & browsers; on npm as ollama.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "TypeScript types",
      "streaming responses",
      "browser & node",
      "npm package"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package (langchain-ollama on PyPI) that adds Ollama models as LLM and embeddings providers.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "chat models",
      "embeddings",
      "tool calling",
      "streaming",
      "PyPI"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich ChatGPT-style web UI for any Ollama model; runs in Docker with one command.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "docker",
      "markdown & code highlighting",
      "multi-user",
      "model manager",
      "open-source"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client SDK for Ollama; available on Maven Central.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "Java/Kotlin",
      "reactive streams",
      "Maven Central",
      "chat & generate"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/saltyorg/ollama-cli",
    "summary": "Community-built interactive TUI (text UI) that wraps ollama run with history, syntax highlighting, and prompt templates.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "TUI",
      "prompt templates",
      "conversation history",
      "open-source"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/fostermaier/ollama-copilot",
    "summary": "Visual Studio Code extension that adds local AI autocomplete powered by any Ollama model.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "VS Code",
      "inline suggestions",
      "configurable model",
      "MIT license"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration (haystack-ai[ollama]) letting you use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Haystack",
      "pipelines",
      "embeddings",
      "generator",
      "PyPI"
    ]
  },
  {
    "title": "ollama-codex",
    "url": "https://github.com/ollama-webui/ollama-codex",
    "summary": "Open-source GitHub Copilot alternative: VS Code extension that streams Ollama code completions.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "VS Code",
      "code completions",
      "streaming",
      "open-source"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU support.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Helm",
      "Kubernetes",
      "GPU nodes",
      "persistent storage"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Official Ruby gem for Ollama; install with gem install ollama.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Ruby gem",
      "streaming",
      "chat & generate",
      "official"
    ]
  },
  {
    "title": "ollama-compose",
    "url": "https://github.com/techno-rocks/ollama-compose",
    "summary": "Collection of ready-made docker-compose stacks pairing Ollama with Open-WebUI, LibreChat, n8n, Flowise, etc.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "docker-compose",
      "stacks",
      "Open-WebUI",
      "n8n",
      "Flowise"
    ]
  },
  {
    "title": "ollama-n8n-node",
    "url": "https://github.com/n8n-io/n8n/tree/master/packages/nodes-base/nodes/Ollama",
    "summary": "Built-in n8n node (since 1.39) that exposes Ollama chat & generate actions inside no-code workflows.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "n8n",
      "no-code",
      "workflows",
      "built-in node"
    ]
  },
  {
    "title": "ollama-go",
    "url": "https://github.com/ollama/ollama-go",
    "summary": "Official Go client for Ollama; go get github.com/ollama/ollama/api.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Go",
      "streaming",
      "chat & generate",
      "official"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rs",
    "summary": "Community Rust crate (ollama-rs on crates.io) providing strongly-typed async bindings to the Ollama API.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Rust",
      "async",
      "crates.io",
      "type-safe"
    ]
  },
  {
    "title": "ollama-docker-gpu",
    "url": "https://github.com/ollama/ollama/pkgs/container/ollama",
    "summary": "Official container image with CUDA and ROCm variants; docker run -d --gpus all ghcr.io/ollama/ollama.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Docker",
      "CUDA",
      "ROCm",
      "official image"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Minimal RAG example using Ollama embeddings + llama.cpp for retrieval; shows chunking & vector search.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "RAG",
      "embeddings",
      "llama.cpp",
      "example"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/alexander-turner/ollama-slack-bot",
    "summary": "Python Slack bot that answers questions using any local Ollama model; deploys to Fly.io or Docker.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Slack",
      "bot",
      "Fly.io",
      "Docker"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/jakobdylanc/llmcord",
    "summary": "Discord.py bot that streams Ollama (or OpenAI) replies in Discord threads; configurable roles & models.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Discord",
      "streaming",
      "threads",
      "configurable"
    ]
  }
]