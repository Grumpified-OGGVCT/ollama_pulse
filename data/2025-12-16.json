[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo \u2013 lightweight, extensible framework to run Llama 2, Mistral, Gemma and other LLMs locally with a single CLI or REST API.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "self-contained binary",
      "built-in model registry",
      "OpenAI-compatible API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "First-party Python client library for chatting, embedding and pulling models from a local Ollama instance.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip install ollama",
      "sync & async APIs",
      "streaming support",
      "type hints"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript SDK for Node and browsers to interact with the Ollama REST API.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "npm i ollama",
      "Promise & stream interfaces",
      "ESM + CJS bundles",
      "zero deps"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/langchain-ollama",
    "summary": "LangChain integration package exposing Ollama models as standard LangChain LLMs and embedders.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "pip install langchain-ollama",
      "chat & embed endpoints",
      "callback support",
      "batching"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI (predecessor to Open WebUI) that talks to a local Ollama server; supports folders, sharing, dark mode.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "Docker ready",
      "multi-user",
      "markdown & code highlighting",
      "modelfile editor"
    ]
  },
  {
    "title": "open-webui",
    "url": "https://github.com/open-webui/open-webui",
    "summary": "Actively maintained successor of ollama-webui; offers RAG, plugin store, OAuth, admin panel and offline-first PWA.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "RAG via embeddings",
      "plugin ecosystem",
      "LDAP/OAuth",
      "mobile PWA",
      "admin console"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/technomancer/ollama-cli",
    "summary": "Rust-based interactive CLI that wraps Ollama with readline history, syntax highlighting and conversation management.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "rust binary",
      "readline",
      "conversation save/load",
      "themes"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/hughcrt/ollama-copilot",
    "summary": "Turn any Ollama model into a GitHub Copilot replacement by proxying VS-Code\u2019s copilot requests to a local endpoint.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "VS-Code plugin",
      "self-hosted",
      "no MS account",
      "inline completions"
    ]
  },
  {
    "title": "ollama-ray",
    "url": "https://github.com/ray-project/ray/tree/master/python/ray/llm/examples/ollama",
    "summary": "Ray example showing how to scale Ollama inference across a cluster using Ray Serve for batching and autoscaling.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Ray Serve",
      "autoscaling",
      "batch inference",
      "GPU scheduling"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-ollama",
    "summary": "Haystack integration providing OllamaGenerator and OllamaEmbedder nodes for building local RAG pipelines.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "pip install haystack-ollama",
      "RAG pipelines",
      "embeddings",
      "HuggingFace datasets"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/JVM client library for Ollama with Kotlin extensions, Spring Boot starter and reactive adapters.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Maven Central",
      "Spring Boot starter",
      "Kotlin coroutines",
      "reactive"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Official Ruby gem for integrating local Ollama models into Rails or Sinatra apps with a simple DSL.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "gem install ollama",
      "Rails ready",
      "streaming",
      "ActiveRecord-like syntax"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official container images (CPU & CUDA) plus docker-compose examples for running Ollama in production.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "official images",
      "CUDA",
      "docker-compose",
      "rootless"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community Helm chart for deploying Ollama on Kubernetes with autoscaling, PVC and GPU node-selector support.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Helm chart",
      "GPU nodes",
      "HPA",
      "PVC",
      "ingress"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/ollama/ollama-csharp",
    "summary": ".NET SDK and ASP.NET Core integration package for chatting with Ollama models via strongly-typed APIs.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "NuGet",
      "ASP.NET Core",
      "strong typing",
      "streaming"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ivanfioravivi/ollama-rag",
    "summary": "Minimal Python template that chains Ollama embeddings, Chroma vector DB and LangChain for offline RAG.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "ChromaDB",
      "offline",
      "template repo",
      "gradio UI"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/mxyng/ollama-discord",
    "summary": "Discord bot that lets server members chat with local Ollama models through slash commands and threads.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "slash commands",
      "threads",
      "multi-model",
      "Docker"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/NixOS/nixpkgs/pull/303123",
    "summary": "Merged Nix flake that packages Ollama and common models for reproducible deployments on NixOS.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Nix flake",
      "reproducible",
      "NixOS service",
      "pre-built models"
    ]
  },
  {
    "title": "ollama-vscode",
    "url": "https://github.com/lf-/ollama-vscode",
    "summary": "Lightweight VS Code extension that adds \u2018Ask Ollama\u2019 command palette and editor inline completion provider.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "VSIX",
      "inline completions",
      "command palette",
      "configurable prompt"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Community Rust crate providing async/tokio bindings and strong types for the Ollama HTTP API.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "crates.io",
      "tokio",
      "async",
      "strong types"
    ]
  }
]