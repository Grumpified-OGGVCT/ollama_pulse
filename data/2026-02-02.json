[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama CLI and server for running large language models locally on macOS, Linux, and Windows; supports Llama 2, Mistral, Gemma, Phi-3, etc.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "self-contained",
      "model library",
      "REST API",
      "model quantization"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama; install via pip install ollama, chat & embed APIs, streaming support.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip installable",
      "async/sync",
      "embeddings",
      "streaming"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node & browsers; npm install ollama, identical API surface to Python SDK.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "npm installable",
      "TypeScript",
      "browser support",
      "streaming"
    ]
  },
  {
    "title": "langchain-community/llms/ollama",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain integration allowing any Ollama model as an LLM or chat backend with memory, tools, agents, RAG pipelines.",
    "source": "blog",
    "date": "2024-04-30",
    "highlights": [
      "LangChain",
      "RAG",
      "agents",
      "memory"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Full-featured open-source web UI (ChatGPT-style) for Ollama; Docker one-liner, multi-user, model manager, code highlighting.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "Docker",
      "ChatGPT UI",
      "multi-user",
      "code highlight"
    ]
  },
  {
    "title": "ollama4j \u2013 Java Client for Ollama",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Community Java/Kotlin SDK wrapping Ollama REST endpoints; Maven Central artifact, fluent builder API.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Maven Central",
      "Java/Kotlin",
      "fluent API",
      "async"
    ]
  },
  {
    "title": "ollama-rb \u2013 Ruby gem",
    "url": "https://github.com/nileshtrivedi/ollama-rb",
    "summary": "Lightweight Ruby client for Ollama; gem install ollama, supports chat, generate, embeddings, streaming.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Ruby gem",
      "streaming",
      "embeddings"
    ]
  },
  {
    "title": "ollama-cli (Go)",
    "url": "https://github.com/sammcj/ollama-cli",
    "summary": "Community Go CLI with fuzzy model picker, conversation history, prompt templates, and TUI mode.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "Go TUI",
      "fuzzy finder",
      "prompt templates",
      "history"
    ]
  },
  {
    "title": "ollama-copilot.nvim",
    "url": "https://github.com/CopilotC-Nvim/CopilotC-Nvim",
    "summary": "Neovim plugin that turns any Ollama model into a Copilot-style code-completion assistant; inline suggestions.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Neovim",
      "inline completions",
      "Copilot-like"
    ]
  },
  {
    "title": "ollama-helm-chart",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama server & models on Kubernetes with GPU support.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Helm",
      "Kubernetes",
      "GPU",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama-webui/ollama-docker-compose",
    "summary": "Quick-start Docker Compose stack bundling Ollama server, WebUI, and optional GPU runtime.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "docker-compose",
      "GPU",
      "WebUI",
      "one-liner"
    ]
  },
  {
    "title": "ollama-qt-client",
    "url": "https://github.com/JackLarson/ollama-qt",
    "summary": "Cross-platform desktop chat client built with Qt6 and C++; supports multiple concurrent chats, markdown rendering.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Qt6",
      "desktop",
      "multi-chat",
      "markdown"
    ]
  },
  {
    "title": "ollama-streamlit-chat",
    "url": "https://github.com/SauravPanchal/ollama-streamlit",
    "summary": "Minimal Streamlit chat app that proxies to Ollama; pip install streamlit-ollama, customizable system prompt.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Streamlit",
      "pip",
      "custom prompt",
      "session state"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama-haystack/ollama-haystack",
    "summary": "Haystack integration by deepset; use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "Haystack",
      "RAG",
      "embeddings",
      "pipelines"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/OllamaSharp",
    "summary": ".NET client library for Ollama; NuGet package with async streaming, chat & embeddings APIs.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "NuGet",
      ".NET",
      "C#",
      "async streaming"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rs",
    "summary": "Community Rust crate providing strongly-typed async client for Ollama; crates.io publish, tokio based.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "Rust",
      "crates.io",
      "tokio",
      "typed API"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hintergrund/ollama-obsidian",
    "summary": "Obsidian plugin to run local LLMs via Ollama for summarizing notes, brainstorming, and Q&A inside vault.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "Obsidian",
      "vault",
      "summarize",
      "local LLM"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/ollama-bot/ollama-discord",
    "summary": "Self-hostable Discord bot that brings any Ollama model into channels with slash commands and thread support.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "Discord",
      "slash commands",
      "threads",
      "self-hosted"
    ]
  },
  {
    "title": "ollama-terraform-provider",
    "url": "https://github.com/ollama-terraform/ollama-terraform",
    "summary": "Experimental Terraform provider to declaratively pull and manage Ollama models as infrastructure.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "Terraform",
      "infrastructure",
      "model management"
    ]
  },
  {
    "title": "ollama-models-hub",
    "url": "https://github.com/ollama-models/ollama-models",
    "summary": "Community-curated registry of custom Ollama Modelfiles for fine-tuned, quantized, or domain-specific models.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "Modelfile",
      "fine-tuned",
      "community",
      "registry"
    ]
  }
]