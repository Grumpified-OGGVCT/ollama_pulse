[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repository providing the CLI and server to pull, run, and manage large language models locally.",
    "source": "github",
    "date": "2023-06-26",
    "highlights": [
      "self-hosted LLM runtime",
      "Docker & macOS/Linux binaries",
      "GGUF/GGML model support"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client library for the Ollama API, usable in Node.js or browsers.",
    "source": "github",
    "date": "2023-09-05",
    "highlights": [
      "npm install ollama",
      "Promise-based API",
      "chat & generate helpers"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama; pip install ollama gives synchronous and async access to the local server.",
    "source": "github",
    "date": "2023-08-14",
    "highlights": [
      "sync & asyncio clients",
      "streaming support",
      "embeddings endpoint"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/ollama",
    "summary": "LangChain integration package letting you use Ollama models as LLM or chat components in LangChain apps.",
    "source": "github",
    "date": "2023-10-12",
    "highlights": [
      "pip install langchain-ollama",
      "standard LangChain interface",
      "streaming & callbacks"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (renamed to Open WebUI) that offers ChatGPT-like experience for local models.",
    "source": "github",
    "date": "2023-10-01",
    "highlights": [
      "Docker image",
      "model management",
      "RAG & plugin support"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client library for Ollama providing fluent builders and async wrappers.",
    "source": "github",
    "date": "2023-11-03",
    "highlights": [
      "Maven Central",
      "Kotlin coroutines",
      "streaming responses"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/nileshtrivedi/ollama-rb",
    "summary": "Community-maintained Ruby gem for interacting with the Ollama HTTP API.",
    "source": "github",
    "date": "2023-09-18",
    "highlights": [
      "gem install ollama-rb",
      "Faraday-based",
      "chat & embed helpers"
    ]
  },
  {
    "title": "ollama-cli (dylanspyer/ollama-cli)",
    "url": "https://github.com/dylanspyer/ollama-cli",
    "summary": "Interactive Node.js CLI that wraps ollama-js for chatting, system prompts, and conversation history.",
    "source": "github",
    "date": "2023-11-20",
    "highlights": [
      "npm i -g ollama-cli",
      "REPL mode",
      "persistent chat history"
    ]
  },
  {
    "title": "ollama-copilot (cmiller01/ollama-copilot)",
    "url": "https://github.com/cmiller01/ollama-copilot",
    "summary": "Experimental GitHub Copilot proxy that routes completions to a local Ollama model instead of OpenAI.",
    "source": "github",
    "date": "2023-10-30",
    "highlights": [
      "Copilot plugin compatible",
      "self-hosted proxy",
      "configurable model"
    ]
  },
  {
    "title": "ollama-chat (sugarforever/ollama-chat)",
    "url": "https://github.com/sugarforever/ollama-chat",
    "summary": "Minimal Next.js chat interface that uses the ollama-js client for streaming conversations.",
    "source": "github",
    "date": "2023-11-15",
    "highlights": [
      "Next.js frontend",
      "server-sent events",
      "Docker compose setup"
    ]
  },
  {
    "title": "ollama-haystack (jannik4/ollama-haystack)",
    "url": "https://github.com/jannik4/ollama-haystack",
    "summary": "Haystack integration by the community enabling Ollama models as generators or rankers in Haystack pipelines.",
    "source": "github",
    "date": "2023-12-02",
    "highlights": [
      "pip install ollama-haystack",
      "Generator & Ranker nodes",
      "streaming support"
    ]
  },
  {
    "title": "ollama-csharp (awaescher/ollama-csharp)",
    "url": "https://github.com/awaescher/ollama-csharp",
    "summary": ".NET client for Ollama with strongly-typed models and async streaming, distributed via NuGet.",
    "source": "github",
    "date": "2023-11-27",
    "highlights": [
      "NuGet OllamaSharp",
      "async IAsyncEnumerable",
      "chat & embeddings"
    ]
  },
  {
    "title": "ollama-ts (langchain-ai/ollama-ts)",
    "url": "https://github.com/langchain-ai/ollama-ts",
    "summary": "Lightweight TypeScript wrapper around Ollama\u2019s REST API used inside LangChain but usable standalone.",
    "source": "github",
    "date": "2023-10-05",
    "highlights": [
      "zero deps",
      "ESM only",
      "streaming utilities"
    ]
  },
  {
    "title": "ollama-helm (thedatabaseme/ollama-helm)",
    "url": "https://github.com/thedatabaseme/ollama-helm",
    "summary": "Official Helm chart to deploy Ollama server and models on Kubernetes clusters.",
    "source": "github",
    "date": "2023-11-10",
    "highlights": [
      "Helm repo",
      "GPU node selector",
      "model pre-load config"
    ]
  },
  {
    "title": "ollama-docker (ai-dock/ollama-docker)",
    "url": "https://github.com/ai-dock/ollama-docker",
    "summary": "Pre-built CUDA-enabled Docker images for Ollama with optional WebUI, targeting workstations & servers.",
    "source": "github",
    "date": "2023-10-22",
    "highlights": [
      "CUDA base",
      "docker-compose stacks",
      "runtime model download"
    ]
  },
  {
    "title": "ollama-rag (zachary62/ollama-rag)",
    "url": "https://github.com/zachary62/ollama-rag",
    "summary": "Minimal retrieval-augmented generation demo using Ollama embeddings + Chroma vector store.",
    "source": "github",
    "date": "2023-11-28",
    "highlights": [
      "Chroma integration",
      "PDF ingestion",
      "streamed answers"
    ]
  },
  {
    "title": "ollama-discord (sockheadrps/ollama-discord)",
    "url": "https://github.com/sockheadrps/ollama-discord",
    "summary": "Discord bot that streams Ollama model replies into guild channels via slash commands.",
    "source": "github",
    "date": "2023-12-05",
    "highlights": [
      "slash commands",
      "conversation threads",
      "role-based access"
    ]
  },
  {
    "title": "ollama-slack (mneedham/ollama-slack)",
    "url": "https://github.com/mneedham/ollama-slack",
    "summary": "Slack bot using Bolt JS to query local Ollama models and post streaming responses in threads.",
    "source": "github",
    "date": "2023-11-08",
    "highlights": [
      "Bolt framework",
      "thread replies",
      "mention trigger"
    ]
  },
  {
    "title": "ollama-nest (jmcdo29/ollama-nest)",
    "url": "https://github.com/jmcdo29/ollama-nest",
    "summary": "NestJS module wrapping ollama-js for dependency-injected LLM services in Node backends.",
    "source": "github",
    "date": "2023-10-18",
    "highlights": [
      "Injectable service",
      "async providers",
      "configurable host"
    ]
  },
  {
    "title": "ollama-fastapi (krrishdholakia/ollama-fastapi)",
    "url": "https://github.com/krrishdholakia/ollama-fastapi",
    "summary": "FastAPI proxy that adds OpenAI-compatible endpoints on top of Ollama for easier client migration.",
    "source": "github",
    "date": "2023-12-01",
    "highlights": [
      "/v1/chat/completions",
      "drop-in replacement",
      "streaming JSON"
    ]
  }
]