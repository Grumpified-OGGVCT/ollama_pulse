[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama CLI and server that lets you run Llama 2, Mistral, Gemma, and other LLMs locally with a single command.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "self-contained binary",
      "model library",
      "OpenAI-compatible API"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama, providing simple async/sync clients to chat with local models.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip install ollama",
      "async/await support",
      "streaming responses"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript SDK for Ollama, usable in Node.js and browsers via a lightweight client.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "npm install ollama",
      "TypeScript types",
      "browser compatible"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain integration that wraps Ollama models as standard LangChain LLMs and chat models.",
    "source": "blog",
    "date": "2024-04-22",
    "highlights": [
      "pip install langchain-ollama",
      "chain support",
      "callback handlers"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (renamed to Open WebUI) offering ChatGPT-like interface and admin controls.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "docker-compose setup",
      "multi-user",
      "RAG plugins"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client library for Ollama with fluent API, streaming, and custom timeout support.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Maven Central",
      "Kotlin coroutines",
      "custom options"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Community-maintained Ruby gem for interacting with the Ollama REST API.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "gem install ollama",
      "Faraday backend",
      "streaming chat"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/salty-flower/ollama-cli",
    "summary": "Cross-platform TUI (terminal UI) client for Ollama written in Rust, offering interactive chats and model management.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Rust binary",
      "keyboard shortcuts",
      "conversation history"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Visual Studio Code extension that brings local Ollama models into GitHub Copilot-like inline suggestions.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "VS Code marketplace",
      "inline completions",
      "configurable model"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration by deepset allowing Ollama models to be used as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "pip install ollama-haystack",
      "pipeline nodes",
      "embedding support"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/ollama/ollama-cookbook",
    "summary": "Community recipes and notebooks showing how to combine Ollama with LangChain, LlamaIndex, RAG, and fine-tuning.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "Jupyter notebooks",
      "RAG examples",
      "fine-tuning scripts"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama-docker",
    "summary": "Official Docker image and compose stacks for running Ollama server and WebUI containers with GPU support.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "CUDA image",
      "docker-compose.yml",
      "ROCm support"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Helm chart to deploy Ollama on Kubernetes with optional WebUI, PVC, and GPU node-selector configuration.",
    "source": "github",
    "date": "2024-04-24",
    "highlights": [
      "Helm repo",
      "GPU nodes",
      "ingress config"
    ]
  },
  {
    "title": "ollama-chat",
    "url": "https://github.com/jmorganca/ollama-chat",
    "summary": "Minimal React chat app example using the ollama-js SDK and Vite, demonstrating streaming responses.",
    "source": "github",
    "date": "2024-04-20",
    "highlights": [
      "React hooks",
      "Vite dev",
      "streaming UI"
    ]
  },
  {
    "title": "ollama-gui",
    "url": "https://github.com/ollama/ollama-gui",
    "summary": "Lightweight Flutter desktop app for chatting with Ollama models on Windows, macOS, and Linux.",
    "source": "github",
    "date": "2024-04-18",
    "highlights": [
      "Flutter cross-platform",
      "local storage",
      "theme switch"
    ]
  },
  {
    "title": "ollama.nvim",
    "url": "https://github.com/nanoteer/ollama.nvim",
    "summary": "Neovim plugin that integrates Ollama models for code explanation, generation, and inline chat.",
    "source": "github",
    "date": "2024-04-15",
    "highlights": [
      "Lua config",
      "telescope picker",
      "inline prompt"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hintergrund/ollama-obsidian",
    "summary": "Obsidian community plugin to run local LLMs via Ollama for summarizing notes and brainstorming.",
    "source": "github",
    "date": "2024-04-14",
    "highlights": [
      "Obsidian plugin",
      "command palette",
      "template variables"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/valpackett/ollama-slack-bot",
    "summary": "Slack Bolt app that adds a /ollama slash command to query local models directly from Slack channels.",
    "source": "github",
    "date": "2024-04-12",
    "highlights": [
      "Bolt framework",
      "slash command",
      "thread replies"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama/ollama-discord",
    "summary": "Discord.py bot bringing Ollama models into Discord with conversation memory and per-server settings.",
    "source": "github",
    "date": "2024-04-10",
    "highlights": [
      "discord.py",
      "conversation memory",
      "admin commands"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama/ollama-rag",
    "summary": "Minimal RAG starter using Ollama embeddings + Chroma vector store and Streamlit frontend for PDF Q&A.",
    "source": "github",
    "date": "2024-04-08",
    "highlights": [
      "Streamlit UI",
      "Chroma DB",
      "PDF ingestion"
    ]
  }
]