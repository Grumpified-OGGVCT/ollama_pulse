[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Ollama repo: CLI, REST API, and Python/JS SDKs for downloading and running Llama-3, Mistral, Gemma, etc. locally.",
    "source": "github",
    "date": "2024-05-23",
    "highlights": [
      "built-in model library",
      "GPU/CPU inference",
      "Modelfile DSL",
      "Docker image"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama; chat, embed, pull, push, and delete models with a few lines of code.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "sync & async APIs",
      "streaming support",
      "PyPI package"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node & browsers; same features as the Python SDK.",
    "source": "github",
    "date": "2024-05-22",
    "highlights": [
      "npm package",
      "TypeScript defs",
      "streaming chat"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration that wraps Ollama for local LLM chains, agents, retrieval, and tool use.",
    "source": "github",
    "date": "2024-05-21",
    "highlights": [
      "pip install langchain-ollama",
      "ChatOllama",
      "OllamaEmbeddings"
    ]
  },
  {
    "title": "ollama-webui (open-webui)",
    "url": "https://github.com/open-webui/open-webui",
    "summary": "Self-hosted ChatGPT-style web UI that plugs into Ollama; supports RAG, multi-user, and plugins.",
    "source": "github",
    "date": "2024-05-23",
    "highlights": [
      "Docker image",
      "file upload & RAG",
      "admin panel"
    ]
  },
  {
    "title": "ollama4j \u2013 Java client",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Lightweight Java/Kotlin SDK for Ollama; chat, completions, embeddings, and model management.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "Maven Central",
      "async & blocking APIs",
      "Android ready"
    ]
  },
  {
    "title": "ollama-rag \u2013 CLI RAG tool",
    "url": "https://github.com/promptable/ollama-rag",
    "summary": "Plug-and-play CLI that ingests PDFs/txts into Chroma and answers questions with Ollama.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "ingest command",
      "Chroma vector store",
      "local only"
    ]
  },
  {
    "title": "ollama-copilot \u2013 VS-Code extension",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "Bring local Ollama models into VS Code as inline Copilot suggestions.",
    "source": "github",
    "date": "2024-05-17",
    "highlights": [
      "inline completions",
      "configurable model",
      "status-bar toggle"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU autoscaling.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "GPU node-pool",
      "PVC model cache",
      "ingress"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official Docker & docker-compose snippets for CPU and CUDA containers.",
    "source": "github",
    "date": "2024-05-22",
    "highlights": [
      "CUDA runtime",
      "AMD ROCm",
      "rootless image"
    ]
  },
  {
    "title": "ollama-chatbot-ui",
    "url": "https://github.com/ivanfioravanti/ollama-chatbot-ui",
    "summary": "Minimal Next.js chat UI that streams Ollama completions; deploy to Vercel in one click.",
    "source": "github",
    "date": "2024-05-16",
    "highlights": [
      "Next.js 14",
      "Tailwind",
      "Vercel template"
    ]
  },
  {
    "title": "ollama-cli \u2013 Rust CLI",
    "url": "https://github.com/ollama-rs/ollama-cli",
    "summary": "Fast Rust CLI for interactive chat and batch inference with Ollama.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "cross-platform binary",
      "syntax highlighting",
      "REPL"
    ]
  },
  {
    "title": "ollama-rs \u2013 Rust SDK",
    "url": "https://github.com/ollama-rs/ollama-rs",
    "summary": "Async Rust crate exposing full Ollama REST API; used by ollama-cli.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "crates.io",
      "tokio/async",
      "strong types"
    ]
  },
  {
    "title": "ollama-nest \u2013 NestJS module",
    "url": "https://github.com/andycandrea/ollama-nest",
    "summary": "Injectable NestJS service for chatting and embedding with Ollama models.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "npm package",
      "configurable providers",
      "RxJS streams"
    ]
  },
  {
    "title": "ollama-ex \u2013 Elixir client",
    "url": "https://github.com/danielspofford/ollama-ex",
    "summary": "Lightweight Elixir wrapper around Ollama REST API with GenServer helpers.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "Hex.pm",
      "LiveView ready",
      "streaming"
    ]
  },
  {
    "title": "ollama-csharp \u2013 .NET client",
    "url": "https://github.com/ollama-csharp/ollama-csharp",
    "summary": "Community-maintained C# SDK for Ollama; supports chat, completions, and embeddings.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "NuGet package",
      "async/await",
      "strong naming"
    ]
  },
  {
    "title": "ollama-django-chat",
    "url": "https://github.com/ahmedbesbes/ollama-django-chat",
    "summary": "Django + Channels project that streams Ollama responses over WebSockets.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Docker setup",
      "Celery queues",
      "Bootstrap UI"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/ollama",
    "summary": "Haystack 2.0 integration for using Ollama generators and embedders in pipelines.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install haystack-ai[ollama]",
      "pipeline YAML",
      "RAG templates"
    ]
  },
  {
    "title": "ollama-gpt-prompt-engineering",
    "url": "https://github.com/svpino/ollama-gpt-prompt-engineering",
    "summary": "Collection of Jupyter notebooks showing advanced prompt techniques with local Ollama models.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "few-shot",
      "CoT",
      "ReAct",
      "function calling"
    ]
  },
  {
    "title": "r/ollama \u2013 community subreddit",
    "url": "https://reddit.com/r/ollama",
    "summary": "Active Reddit community sharing model tips, benchmarks, and integrations.",
    "source": "reddit",
    "date": "2024-05-23",
    "highlights": [
      "model recommendations",
      "performance tuning",
      "show-and-tell"
    ]
  }
]