[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: get up and running with Llama 3, Mistral, Gemma and other large language models locally.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "self-hosted LLM runtime",
      "Docker-like CLI",
      "bundles model weights"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama; chat, generate, embed with any model served by Ollama.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip install ollama",
      "sync & async APIs",
      "embedding support"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama; works in Node, Deno, Bun and browsers.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "npm i ollama",
      "TypeScript types",
      "streaming responses"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://pypi.org/project/langchain-ollama/",
    "summary": "PyPI package integrating Ollama models as LangChain LLM & chat components.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip install langchain-ollama",
      "drop-in LLM",
      "chat & embed"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (chat, admin, model management) installable via Docker.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "dark/light theme",
      "multi-user",
      "Docker one-liner"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sugarforever/ollama-cli",
    "summary": "Community-built interactive CLI wrapper with history, syntax highlighting and prompt templates.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "interactive REPL",
      "prompt templates",
      "chat history"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "VS Code extension that turns any Ollama model into a GitHub Copilot replacement.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "inline completions",
      "configurable model",
      "open-source"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Helm chart to deploy Ollama on Kubernetes with GPU autoscaling support.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "GPU nodes",
      "autoscaling",
      "persistent storage"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin SDK for Ollama (Maven Central) with fluent API and reactive wrappers.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Maven Central",
      "reactive",
      "Kotlin extensions"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Official Ruby gem for Ollama; simple interface for chat, generate and pull commands.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "gem install ollama",
      "Ruby DSL",
      "streaming"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration letting you use Ollama models as generators or embedders in RAG pipelines.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "RAG ready",
      "embeddings",
      "Haystack nodes"
    ]
  },
  {
    "title": "ollama-gui",
    "url": "https://github.com/jakobhoeg/ollama-gui",
    "summary": "Lightweight Tauri desktop app for chatting with Ollama models (Windows/Mac/Linux).",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Tauri/Rust",
      "cross-platform",
      "small binary"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama-docker-compose",
    "summary": "Official Docker Compose stacks with CPU and GPU variants plus Open-WebUI sidecar.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "one-command spin-up",
      "GPU support",
      "bundled WebUI"
    ]
  },
  {
    "title": "ollama-nvidia-chat",
    "url": "https://github.com/NVIDIA/ollama-nvidia-chat",
    "summary": "NVIDIA sample showing Ollama + TensorRT-LLM for 2-3\u00d7 faster inference on RTX GPUs.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "TensorRT backend",
      "RTX optimized",
      "benchmarks included"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/ollama-slack-bot/ollama-slack-bot",
    "summary": "Slack bot server that exposes Ollama models as slash commands or DM chat.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Slack slash cmd",
      "DM support",
      "Docker image"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama-discord/ollama-discord",
    "summary": "Discord bot using Ollama for on-server chat, image captioning and moderation.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "Discord.py",
      "image caption",
      "moderation"
    ]
  },
  {
    "title": "ollama-mistral-finetune",
    "url": "https://github.com/ollama/ollama-mistral-finetune",
    "summary": "Tutorial repo showing how to fine-tune Mistral-7B and load the result into Ollama.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "LoRA fine-tune",
      "GGUF export",
      "step-by-step"
    ]
  },
  {
    "title": "ollama-go",
    "url": "https://github.com/ollama/ollama-go",
    "summary": "Official Go client library for Ollama; idiomatic Go API with streaming support.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "go get github.com/ollama/ollama-go",
      "streaming",
      "context"
    ]
  },
  {
    "title": "ollama-distroless",
    "url": "https://github.com/ollama/ollama-distroless",
    "summary": "Minimal distroless Docker image for Ollama reducing attack surface and image size.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "distroless",
      "security hardened",
      "tiny image"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama/ollama-rust",
    "summary": "Community Rust crate providing async bindings to the Ollama REST API.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "async/tokio",
      "crates.io",
      "type-safe"
    ]
  }
]