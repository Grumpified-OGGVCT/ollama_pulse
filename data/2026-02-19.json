[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and Go library for downloading, building and running large language models locally.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "CLI",
      "Docker",
      "REST API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "langchain-ai/langchain",
    "url": "https://github.com/langchain-ai/langchain",
    "summary": "Python/JS framework with first-class Ollama integration for building LLM applications.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "Python",
      "JavaScript",
      "RAG",
      "Agents"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://pypi.org/project/ollama/",
    "summary": "Official Python client for Ollama; chat, generate, embed with any model.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "PyPI",
      "embeddings",
      "streaming",
      "async"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "Official Node.js/TypeScript client for Ollama; same API surface as Python.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "npm",
      "TypeScript",
      "ESM",
      "streaming"
    ]
  },
  {
    "title": "jmorganca/ollama-helm",
    "url": "https://github.com/jmorganca/ollama-helm",
    "summary": "Helm chart to deploy Ollama on Kubernetes with GPU support.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Kubernetes",
      "Helm",
      "GPU",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich ChatGPT-style web interface for Ollama models.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "React",
      "Docker",
      "multi-model",
      "code-highlight"
    ]
  },
  {
    "title": "ollama-gui",
    "url": "https://github.com/ollama-gui/ollama-gui",
    "summary": "Lightweight Tauri desktop GUI for Ollama on Windows/Mac/Linux.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Tauri",
      "Rust",
      "desktop",
      "cross-platform"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Java/Kotlin client library for Ollama with Spring Boot starters.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Java",
      "Kotlin",
      "Spring Boot",
      "Maven Central"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama-rb/ollama-rb",
    "summary": "Ruby gem for interacting with Ollama\u2019s REST API.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Ruby",
      "REST",
      "streaming",
      "gem"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/ollama-cli/ollama-cli",
    "summary": "Rust-based interactive CLI with syntax highlighting and history.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Rust",
      "REPL",
      "syntax-highlight",
      "history"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama-cop/ollama-copilot",
    "summary": "VS Code extension that uses Ollama models for inline code suggestions.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "VS Code",
      "inline",
      "code-completion",
      "local"
    ]
  },
  {
    "title": "ollama-sd-webui",
    "url": "https://github.com/ollama-sd/ollama-sd-webui",
    "summary": "AUTOMATIC1111 extension to drive Ollama LLM prompts for image generation.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "Stable Diffusion",
      "prompting",
      "AUTOMATIC1111",
      "plugin"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama-discord/ollama-discord",
    "summary": "Self-hostable Discord bot that chats using any Ollama model.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Discord",
      "bot",
      "slash-commands",
      "Docker"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/ollama-slack/ollama-slack",
    "summary": "Slack Bolt app to bring Ollama completions into channels or DMs.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Slack",
      "Bolt",
      "socket-mode",
      "Docker"
    ]
  },
  {
    "title": "ollama-streamlit",
    "url": "https://github.com/ollama-streamlit/ollama-streamlit",
    "summary": "Streamlit chat component template powered by Ollama.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "Streamlit",
      "component",
      "chat",
      "template"
    ]
  },
  {
    "title": "ollama-django",
    "url": "https://github.com/ollama-django/ollama-django",
    "summary": "Django app + REST endpoints for serving Ollama models in web projects.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "Django",
      "REST",
      "channels",
      "async"
    ]
  },
  {
    "title": "ollama-fastapi",
    "url": "https://github.com/ollama-fastapi/ollama-fastapi",
    "summary": "FastAPI wrapper that adds OpenAI-compatible endpoints on top of Ollama.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "FastAPI",
      "OpenAI",
      "drop-in",
      "compatibility"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama-obsidian/ollama-obsidian",
    "summary": "Obsidian plugin to run local LLM completions inside your notes.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "Obsidian",
      "plugin",
      "local",
      "privacy"
    ]
  },
  {
    "title": "ollama-ray",
    "url": "https://github.com/ollama-ray/ollama-ray",
    "summary": "Ray Serve deployment recipes for scaling Ollama models across GPUs.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "Ray",
      "Serve",
      "scaling",
      "GPU"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama-haystack/ollama-haystack",
    "summary": "Haystack integration letting you use Ollama models in RAG pipelines.",
    "source": "github",
    "date": "2024-04-24",
    "highlights": [
      "Haystack",
      "RAG",
      "pipeline",
      "retrieval"
    ]
  }
]