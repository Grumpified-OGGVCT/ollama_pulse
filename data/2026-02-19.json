[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Ollama repo: self-contained CLI and server that lets you pull, run, and manage LLMs locally (Llama 3, Mistral, Gemma, \u2026) with a single command.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pull & run models locally",
      "macOS/Linux/Windows",
      "built-in Modelfile DSL",
      "OpenAI-compatible API"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama; works in Node, Deno, Bun and browsers to chat, stream, embed, pull, and delete models.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "npm install ollama",
      "Promise & async-iterator streaming",
      "full TypeScript types"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama; pip install ollama gives you chat, generate, embed, pull, list, delete, and streaming support.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "sync & async APIs",
      "built-in embedding helpers",
      "100 % OpenAI-compatible wrapper"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "First-party LangChain integration that exposes Ollama models as LLM, Chat, and Embeddings interfaces with native streaming and tool calling.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip install langchain-ollama",
      "tool use & JSON mode",
      "zero-config local RAG"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama comparable to ChatGPT: chat history, multi-user, code highlighting, RAG uploads, admin panel, dark mode.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "docker-compose one-liner",
      "upload PDF/txt for RAG",
      "OpenAI-compatible endpoint proxy"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Java/Kotlin client for Ollama with fluent builder API, streaming, embeddings, and pull/generate utilities; Maven Central artifact.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Java 8+",
      "Kotlin coroutine extensions",
      "Spring Boot starters"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Official Ruby gem for Ollama: chat, generate, pull, list, delete, and embeddings with native streaming and Fiber support.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "gem install ollama",
      "100 % OpenAI drop-in",
      "Rails console friendly"
    ]
  },
  {
    "title": "ollama-cli-csharp",
    "url": "https://github.com/ollama/ollama-csharp",
    "summary": "Official C# / .NET SDK for Ollama; NuGet package with async streaming, chat, embeddings, and tool calling for .NET 6+.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "dotnet add package Ollama",
      "ASP.NET Core sample",
      "OpenAI-compatible typed client"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/ollama/ollama-cookbook",
    "summary": "Community recipes for Ollama: Vision models, function calling, RAG with Chroma, LlamaIndex, private GPT, Docker Compose stacks, GPU tweaks.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "copy-paste examples",
      "Modelfile snippets",
      "GPU/CPU performance tuning"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Helm chart to deploy Ollama server on Kubernetes with GPU autoscaling, PVC model cache, and optional web-ui sidecar.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "helm install ollama",
      "NVIDIA device plugin",
      "HPA & PVC out-of-box"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official multi-arch Docker images (CPU & GPU) and compose snippets for running Ollama server and web-ui in containers.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "ollama/ollama:latest",
      "CUDA & ROCm tags",
      "one-line compose with web-ui"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration that registers Ollama models as Generators and Embedders for production-grade RAG pipelines.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pip install haystack-ai[ollama]",
      "YAML pipeline support",
      "batch embedding"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama/ollama-rust",
    "summary": "Community Rust crate wrapping Ollama\u2019s REST API with Tokio streaming, strong typing, and embed helpers; published on crates.io.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "cargo add ollama-rs",
      "async/await",
      "examples for Axum & Tauri"
    ]
  },
  {
    "title": "ollama-go",
    "url": "https://github.com/ollama/ollama-go",
    "summary": "Official Go client for Ollama; go get github.com/ollama/ollama/api gives you chat, generate, pull, and embeddings with context cancellation.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "idiomatic Go",
      "streaming via io.Reader",
      "tiny CLI example included"
    ]
  },
  {
    "title": "ollama-npm",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "npm registry page for the official ollama JavaScript package; 0 dependencies, ESM & CJS exports, 100 % TypeScript.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      ">150 k weekly downloads",
      "zero deps",
      "browser & server"
    ]
  },
  {
    "title": "ollama-pypi",
    "url": "https://pypi.org/project/ollama/",
    "summary": "PyPI landing for the official ollama Python package; wheels for 3.8+, 0 runtime deps, async & sync APIs.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      ">200 k monthly downloads",
      "pure Python",
      "built-in httpx"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/ollama-ecosystem/ollama-discord-bot",
    "summary": "Self-hostable Discord bot that brings Ollama models into any server with slash commands, thread support, and per-user rate limits.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Docker image",
      "threaded chats",
      "modelfile switcher"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/ollama-ecosystem/ollama-slack-bot",
    "summary": "Slack Bolt app that adds /ollama slash command and direct-message chats, supports file uploads for RAG and ephemeral responses.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Bolt JS",
      "Socket-Mode ready",
      "upload PDF for context"
    ]
  },
  {
    "title": "ollama-vscode",
    "url": "https://github.com/ollama-ecosystem/ollama-vscode",
    "summary": "VS Code extension that adds a private AI sidebar: chat, inline refactor, explain code, generate tests, all powered by local Ollama models.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "sidebar UI",
      "inline code actions",
      "multi-model switcher"
    ]
  },
  {
    "title": "ollama-reddit",
    "url": "https://www.reddit.com/r/ollama/",
    "summary": "Active subreddit for Ollama: model announcements, troubleshooting, performance tips, and community Modelfile sharing.",
    "source": "reddit",
    "date": "2024-05-10",
    "highlights": [
      "25 k members",
      "daily tips",
      "Modelfile recipes"
    ]
  }
]