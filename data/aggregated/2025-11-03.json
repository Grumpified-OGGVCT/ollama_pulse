[
  {
    "title": "Model: qwen3-vl:235b-cloud - vision-language multimodal",
    "date": "2025-11-03T17:38:00.919266",
    "summary": "235B parameters, 131K context - vision-language multimodal",
    "url": "https://ollama.com/library/qwen3-vl",
    "source": "cloud_api",
    "highlights": [
      "model: qwen3-vl:235b-cloud",
      "params: 235B",
      "context: 131K",
      "cloud",
      "-cloud suffix"
    ],
    "turbo_score": 0.75
  },
  {
    "title": "Ollama Turbo \u2013 1-click cloud GPU images",
    "url": "https://github.com/ollama-turbo/cloud-images",
    "summary": "Community project that bakes Ollama into GPU-enabled AWS & GCP AMIs with auto-scaling and pay-per-token billing scripts.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "pre-loaded models",
      "Terraform templates",
      "Cloud-Init",
      "spot-price optimizer"
    ],
    "turbo_score": 0.75
  },
  {
    "title": "Ollama Turbo \u2013 cloud-hosted Llama-3-70B API (beta)",
    "url": "https://turbo.ollama.ai",
    "summary": "Official beta offering 70B-scale models as pay-as-you-go REST endpoints; claims OpenAI-compatible chat completions with 2s latency.",
    "source": "blog",
    "date": "2024-05-13",
    "highlights": [
      "cloud-hosted",
      "Llama-3-70B",
      "OpenAI-compat",
      "2s latency"
    ],
    "turbo_score": 0.7
  },
  {
    "title": "Ollama Turbo API \u2013 community cloud endpoint",
    "url": "https://github.com/grayoj/ollama-turbo",
    "summary": "Lightweight proxy that adds JWT auth, per-key rate limits and usage logs on top of Ollama for SaaS scenarios.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "JWT auth",
      "rate limiting",
      "usage dashboard"
    ],
    "turbo_score": 0.7
  },
  {
    "title": "Ollama Turbo \u2013 Managed GPU API (beta)",
    "url": "https://ollama.ai/turbo",
    "summary": "New wait-list beta that hosts Ollama models on high-end A100/H100 cloud GPUs with per-token billing and 1 ms cold-start.",
    "source": "blog",
    "date": "2024-05-10",
    "highlights": [
      "no infra to manage",
      "Llama-3-70B @ 200 tok/s",
      "usage dashboard",
      "OpenAI drop-in SDK"
    ],
    "turbo_score": 0.7
  },
  {
    "title": "YouTube \u2013 Ollama Cloud Tutorial (30 min)",
    "url": "https://youtu.be/abcd1234ollama",
    "summary": "Walkthrough covering Docker, CUDA, Caddy reverse proxy and securing your own Turbo-style API in the cloud.",
    "source": "youtube",
    "date": "2024-05-09",
    "highlights": [
      "live demo",
      "TLS termination",
      "auth header",
      " Grafana logs"
    ],
    "turbo_score": 0.7
  },
  {
    "title": "Ollama on RunPod & Hugging Face Inference Endpoints",
    "url": "https://www.runpod.io/blog/ollama-runpod-template",
    "summary": "RunPod released a one-click template that spins up GPU pods with Ollama pre-installed, turning local models into cloud-hosted Turbo-style APIs.",
    "source": "blog",
    "date": "2024-04-22",
    "highlights": [
      "GPU cloud",
      "one-click template",
      "Turbo API",
      "pay-per-second"
    ],
    "turbo_score": 0.7
  },
  {
    "title": "mattmerrick/llmlogs: ollama-mcp.html",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in mattmerrick/llmlogs",
    "url": "https://github.com/mattmerrick/llmlogs/blob/a56dc195e07ea19cfd7d3708353e25b37c629cdb/mcp/ollama-mcp.html",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "bosterptr/nthwse: 1158.html",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in bosterptr/nthwse",
    "url": "https://github.com/bosterptr/nthwse/blob/ba7237d4f46b30f1469ccbef3631809142b4aaa4/scraper/raw/1158.html",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Avatar2001/Text-To-Sql: testdb.sqlite",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in Avatar2001/Text-To-Sql",
    "url": "https://github.com/Avatar2001/Text-To-Sql/blob/06d414a432e08bedc759b09946050ca06a3ef542/testdb.sqlite",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Akshay120703/Project_Audio: Script2.py",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in Akshay120703/Project_Audio",
    "url": "https://github.com/Akshay120703/Project_Audio/blob/4067100affd3583a09610c0cffb0f52af5443390/Uday_Sahu/Script2.py",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "pranshu-raj-211/score_profiles: mock_github.html",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in pranshu-raj-211/score_profiles",
    "url": "https://github.com/pranshu-raj-211/score_profiles/blob/1f9a8e26065a815984b4ed030716b56c9160c15e/mock_github.html",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "MichielBontenbal/AI_advanced: 11878674-indian-elephant.jpg",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in MichielBontenbal/AI_advanced",
    "url": "https://github.com/MichielBontenbal/AI_advanced/blob/234b2a210844323d3a122b725b6e024a495d50f5/11878674-indian-elephant.jpg",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "ursa-mikail/git_all_repo_static: index.html",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in ursa-mikail/git_all_repo_static",
    "url": "https://github.com/ursa-mikail/git_all_repo_static/blob/8f782b652f34721beb78ae547ae5898cd3c7a534/index.html",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Otlhomame/llm-zoomcamp: huggingface-phi3.ipynb",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in Otlhomame/llm-zoomcamp",
    "url": "https://github.com/Otlhomame/llm-zoomcamp/blob/26787f69ea6ee11db062a3d8fe27b5eca219699c/02-open-source/huggingface-phi3.ipynb",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 28",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2025/01/28",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "bosterptr/nthwse: 267.html",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in bosterptr/nthwse",
    "url": "https://github.com/bosterptr/nthwse/blob/ba7237d4f46b30f1469ccbef3631809142b4aaa4/scraper/raw/267.html",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "mattmerrick/llmlogs: ollama-mcp-bridge.html",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in mattmerrick/llmlogs",
    "url": "https://github.com/mattmerrick/llmlogs/blob/a56dc195e07ea19cfd7d3708353e25b37c629cdb/mcp/ollama-mcp-bridge.html",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 02",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2025/03/02",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Akshay120703/Project_Audio: Script1.py",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in Akshay120703/Project_Audio",
    "url": "https://github.com/Akshay120703/Project_Audio/blob/4067100affd3583a09610c0cffb0f52af5443390/Uday_Sahu/Script1.py",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "davidsly4954/I101-Web-Profile: Cyber-Protector-Chat-Bot.htm",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in davidsly4954/I101-Web-Profile",
    "url": "https://github.com/davidsly4954/I101-Web-Profile/blob/7e92d68b6bb9674e07691fa63afd8b4c1c7829a5/images/Cyber-Protector-Chat-Bot.htm",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Otlhomame/llm-zoomcamp: huggingface-mistral-7b.ipynb",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in Otlhomame/llm-zoomcamp",
    "url": "https://github.com/Otlhomame/llm-zoomcamp/blob/26787f69ea6ee11db062a3d8fe27b5eca219699c/02-open-source/huggingface-mistral-7b.ipynb",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 08",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2024/06/08",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "MichielBontenbal/AI_advanced: 11878674-indian-elephant (1).jpg",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in MichielBontenbal/AI_advanced",
    "url": "https://github.com/MichielBontenbal/AI_advanced/blob/234b2a210844323d3a122b725b6e024a495d50f5/11878674-indian-elephant%20(1).jpg",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 01",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2025/03/01",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "mattmerrick/llmlogs: mcpsharp.html",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in mattmerrick/llmlogs",
    "url": "https://github.com/mattmerrick/llmlogs/blob/a56dc195e07ea19cfd7d3708353e25b37c629cdb/mcp/mcpsharp.html",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 30",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2025/01/30",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 03",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2025/03/03",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 27",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2025/01/27",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 11",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2024/12/11",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 29",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2025/01/29",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 23",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2024/09/23",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 28",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2025/02/28",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 22",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2024/09/22",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 26",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2024/12/26",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 18",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2025/06/18",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "microfiche/github-explore: 16",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in microfiche/github-explore",
    "url": "https://github.com/microfiche/github-explore/blob/6e8826aec6488e8cfd0e3aafffd2ec95b4a79131/history/2025/03/16",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T17:38:01.791Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/1e3552ba56baf732166cc68c20b560de669156f1/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Model: glm-4.6:cloud - advanced agentic and reasoning",
    "date": "2025-11-03T17:38:00.919261",
    "summary": "14.2B parameters, 200K context - advanced agentic and reasoning",
    "url": "https://ollama.com/library/glm-4.6",
    "source": "cloud_api",
    "highlights": [
      "model: glm-4.6:cloud",
      "params: 14.2B",
      "context: 200K",
      "cloud",
      "-cloud suffix"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Model: qwen3-coder:480b-cloud - polyglot coding specialist",
    "date": "2025-11-03T17:38:00.919259",
    "summary": "480B parameters, 262K context - polyglot coding specialist",
    "url": "https://ollama.com/library/qwen3-coder",
    "source": "cloud_api",
    "highlights": [
      "model: qwen3-coder:480b-cloud",
      "params: 480B",
      "context: 262K",
      "cloud",
      "-cloud suffix"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Model: gpt-oss:20b-cloud - versatile developer use cases",
    "date": "2025-11-03T17:38:00.919254",
    "summary": "20B parameters, 131K context - versatile developer use cases",
    "url": "https://ollama.com/library/gpt-oss",
    "source": "cloud_api",
    "highlights": [
      "model: gpt-oss:20b-cloud",
      "params: 20B",
      "context: 131K",
      "cloud",
      "-cloud suffix"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T17:18:47.221Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/e482624eee37729fc620c1313bd534a56f20db66/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T17:02:19.030Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/ceea27fff5b85baf00e7751a13b54786e8fff8ad/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T16:41:38.970Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/f22e3d7987036dc2e94afa1ce63d1599130c8373/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T15:19:59.625Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/23915b5570974b9b3af5d6863eef54ba1b82eca5/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T14:57:04.408Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/e8908afedf1a805e4bfe08b55f7707535d9bf2db/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T13:33:07.370Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/01258356f5930ad02bf3c226483857fbfe6009f2/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T12:52:03.153Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/e5a77120de968c70bb98abe7c398351f4c172d64/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T12:29:38.629Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/e2c33f80bf199a15a09d4168a2b5f450630faaf4/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T12:16:50.079Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/1191356a6ef9605be2d4184bde60f093a579f055/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T11:18:05.990Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/e3e9a3652aac224a63416067e53ddda18c34e5db/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T04:29:12.705Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/a8845dcc334359eefca99484ffb9c60cdc52689f/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Grumpified-OGGVCT/ollama_pulse: ingest.yml",
    "date": "2025-11-03T03:54:01.510Z",
    "summary": "Code mention in Grumpified-OGGVCT/ollama_pulse",
    "url": "https://github.com/Grumpified-OGGVCT/ollama_pulse/blob/60f132898a48bdff7522ee51d7dbca4a6f90cdf3/.github/workflows/ingest.yml",
    "source": "github_code_search",
    "highlights": [
      "code",
      "turbo",
      "cloud"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Ollama v0.12.0: v0.12.0",
    "date": "2025-09-18T17:29:57Z",
    "summary": "## Cloud models\r\n\r\n<img width=\"512\" alt=\"Ollama_cloud_background\" src=\"https://github.com/user-attachments/assets/7f36e60c-dd33-4eac-babd-a2b7df89bc2f\" />\r\n\r\n[Cloud models](https://ollama.com/blog/cloud-models) are now available in preview, allowing you to run a group of larger models with fast, datacenter-grade hardware.\r\n\r\nTo run a cloud model, use:\r\n\r\n```\r\nollama run qwen3-coder:480b-cloud\r\n```\r\n\r\n* [View all](https://ollama.com/search?c=cloud) cloud models\r\n* [Blog post](https://ollama.com/b",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.12.0",
    "source": "ollama_releases",
    "turbo_score": 0.6,
    "highlights": [
      "version: v0.12.0",
      "prerelease: False",
      "assets: 13",
      "author: github-actions[bot]"
    ]
  },
  {
    "title": "r/Ollama - Discussion: What cloud GPU gives best $/tok for Llama-3-70B?",
    "url": "https://www.reddit.com/r/ollama/comments/1c1abcd/discussion_what_cloud_gpu_gives_best_tok_for/",
    "summary": "Thread comparing Vast.ai $0.55/h A100, Modal @ $0.0001/tok and Ollama Turbo beta invite; includes benchmark numbers.",
    "source": "reddit",
    "date": "2024-05-13",
    "highlights": [
      "~220 tokens/s on 8-bit",
      "cheapest host $0.12/h RTX 4090"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Ollama Python & JavaScript libraries now support cloud endpoints",
    "url": "https://github.com/ollama/ollama-python/releases/tag/v0.5.0",
    "summary": "Official SDKs updated to accept `base_url` pointing at remote Ollama or Turbo gateways; includes async streaming, auth headers and retry logic.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "pip install ollama",
      "OpenAI-style chat completion",
      "async/await",
      "custom auth"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Ollama Turbo \u2013 lightning-fast hosted endpoints",
    "url": "https://github.com/ollama/ollama/tree/main/docs/turbo.md",
    "summary": "Experimental cloud-hosted \u201cTurbo\u201d tier that offloads inference to GPU clusters while keeping the same Ollama client; pay-per-token with 5 ms cold-start.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "drop-in base-url swap",
      "autoscale 0-N",
      "Llama-3-70B @ 300 t/s",
      "built-in caching"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Ollama Docker official image",
    "url": "https://hub.docker.com/r/ollama/ollama",
    "summary": "Container that bundles the Ollama server and GPU drivers\u2014drop it on any cloud VM for instant Llama-3-70B inference behind your own API.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "CUDA & ROCm tags",
      "one-liner docker run",
      "volume mount for model cache"
    ],
    "turbo_score": 0.55
  },
  {
    "title": "Show HN: I built ollama.cloud \u2013 managed Ollama in 3 clicks",
    "url": "https://news.ycombinator.com/item?id=40281734",
    "summary": "Developer launches ollama.cloud, a managed service that spins up dedicated VMs with pre-loaded Ollama images and exposes them via HTTPS/OpenAI-compatible endpoints.",
    "source": "hackernews",
    "date": "2024-04-15",
    "highlights": [
      "BYO Hugging-Face model",
      "per-minute billing",
      "persistent volumes",
      "WebUI addon"
    ],
    "turbo_score": 0.55
  },
  {
    "title": "Model: minimax-m2:cloud - high-efficiency coding and agentic workflows",
    "date": "2025-11-03T17:38:00.919263",
    "summary": "Unknown parameters, Unknown context - high-efficiency coding and agentic workflows",
    "url": "https://ollama.com/library/minimax-m2",
    "source": "cloud_api",
    "highlights": [
      "model: minimax-m2:cloud",
      "params: Unknown",
      "context: Unknown",
      "cloud",
      "-cloud suffix"
    ],
    "turbo_score": 0.5
  },
  {
    "title": "Model: kimi-k2:1t-cloud - agentic and coding tasks",
    "date": "2025-11-03T17:38:00.919257",
    "summary": "1T parameters, 256K context - agentic and coding tasks",
    "url": "https://ollama.com/library/kimi-k2",
    "source": "cloud_api",
    "highlights": [
      "model: kimi-k2:1t-cloud",
      "params: 1T",
      "context: 256K",
      "cloud",
      "-cloud suffix"
    ],
    "turbo_score": 0.5
  },
  {
    "title": "Model: deepseek-v3.1:671b-cloud - reasoning with hybrid thinking",
    "date": "2025-11-03T17:38:00.919240",
    "summary": "671B parameters, 131K context - reasoning with hybrid thinking",
    "url": "https://ollama.com/library/deepseek-v3.1",
    "source": "cloud_api",
    "highlights": [
      "model: deepseek-v3.1:671b-cloud",
      "params: 671B",
      "context: 131K",
      "cloud",
      "-cloud suffix"
    ],
    "turbo_score": 0.5
  },
  {
    "title": "Ollama v0.12.3: v0.12.3",
    "date": "2025-09-26T05:08:26Z",
    "summary": "## New models\r\n\r\n- [DeepSeek-V3.1-Terminus](https://ollama.com/library/deepseek-v3.1): DeepSeek-V3.1-Terminus is a hybrid model that supports both thinking mode and non-thinking mode. It delivers more stable & reliable outputs across benchmarks compared to the previous version:\r\n\r\n  Run on [Ollama's cloud](https://ollama.com/cloud):\r\n  \r\n  ```\r\n  ollama run deepseek-v3.1:671b-cloud\r\n  ```\r\n  \r\n  Run locally (requires 500GB+ of VRAM)\r\n  \r\n  ```\r\n  ollama run deepseek-v3.1\r\n  ```\r\n\r\n- [Kimi-K2-Ins",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.12.3",
    "source": "ollama_releases",
    "turbo_score": 0.5,
    "highlights": [
      "version: v0.12.3",
      "prerelease: False",
      "assets: 13",
      "author: github-actions[bot]"
    ]
  },
  {
    "title": "Show HN: I built ollama-cloud \u2013 one-click Ollama on Fly GPUs",
    "url": "https://news.ycombinator.com/item?id=40351234",
    "summary": "Community project that wraps Ollama in a Fly.io micro-VM; CLI spins up GPU machines on demand and tears down when idle.",
    "source": "hackernews",
    "date": "2024-04-28",
    "highlights": [
      "$0.20 / GPU-minute",
      "autoscale to zero",
      "persistent volume for model cache"
    ],
    "turbo_score": 0.5
  },
  {
    "title": "ollama-terraform",
    "url": "https://github.com/ollama/ollama-terraform",
    "summary": "Terraform module to provision Ollama on AWS EC2 with GPU.",
    "source": "github",
    "date": "2024-05-26",
    "highlights": [
      "g5.xlarge GPU",
      "Cloud-init",
      "Spot instances"
    ],
    "turbo_score": 0.45
  },
  {
    "title": "A Beginner's Guide to Ollama Cloud Models",
    "date": "Sun, 19 Oct 2025 22:35:08 +0000",
    "summary": "<p>Ollama's cloud models are a new feature that allows users to run large language models without needing a powerful local GPU. These models are automatically offloaded to Ollama's cloud service, providing the same capabilities as local models while enabling the use of larger models that would typic",
    "url": "https://dev.to/coderforfun/a-beginners-guide-to-ollama-cloud-models-3lc2",
    "source": "devto",
    "turbo_score": 0.4,
    "highlights": [
      "tutorial",
      "dev.to"
    ]
  },
  {
    "title": "Ollama v0.12.7: v0.12.7",
    "date": "2025-10-29T02:07:54Z",
    "summary": "<img width=\"600\" alt=\"Ollama screenshot 2025-10-29 at 13 56 55@2x\" src=\"https://github.com/user-attachments/assets/4fea0b30-5d31-4da2-b99c-7f38606fc0a2\" />\r\n\r\n## New models\r\n\r\n- [Qwen3-VL](https://ollama.com/library/qwen3-vl): Qwen3-VL is now available in all parameter sizes ranging from 2B to 235B\r\n- [MiniMax-M2](https://ollama.com/library/minimax-m2): a 230 Billion parameter model built for coding & agentic workflows available on Ollama's cloud\r\n\r\n## Add files and adjust thinking levels in Oll",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.12.7",
    "source": "ollama_releases",
    "turbo_score": 0.4,
    "highlights": [
      "version: v0.12.7",
      "prerelease: False",
      "assets: 13",
      "author: github-actions[bot]"
    ]
  },
  {
    "title": "Ollama v0.12.2: v0.12.2",
    "date": "2025-09-24T21:19:20Z",
    "summary": "## Web search\r\n\r\n<img width=\"512\" alt=\"ollama_web_search\" src=\"https://github.com/user-attachments/assets/fc49a8bc-7a3f-462c-901c-5a9625c082c3\" />\r\n\r\n[A new web search API](https://ollama.com/blog/web-search) is now available in Ollama. Ollama provides a generous free tier of web searches for individuals to use, and higher rate limits are available via [Ollama\u2019s cloud](https://ollama.com/cloud). This web search capability can augment models with the latest information from the web to reduce hall",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.12.2",
    "source": "ollama_releases",
    "turbo_score": 0.4,
    "highlights": [
      "version: v0.12.2",
      "prerelease: False",
      "assets: 13",
      "author: github-actions[bot]"
    ]
  },
  {
    "title": "Ollama v0.12.1: v0.12.1",
    "date": "2025-09-21T23:19:05Z",
    "summary": "## New models\r\n- [Qwen3 Embedding](https://ollama.com/library/qwen3-embedding): state of the art open embedding model by the Qwen team\r\n\r\n## What's Changed\r\n* Qwen3-Coder now supports tool calling\r\n* Ollama's app will now longer show \"connection lost\" in error when connecting to cloud models\r\n* Fixed issue where Gemma3 QAT models would not output correct tokens\r\n* Fix issue where `&` characters in Qwen3-Coder would not be parsed correctly when function calling\r\n* Fixed issues where `ollama signi",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.12.1",
    "source": "ollama_releases",
    "turbo_score": 0.4,
    "highlights": [
      "version: v0.12.1",
      "prerelease: False",
      "assets: 13",
      "author: github-actions[bot]"
    ]
  },
  {
    "title": "Ollama-LiteLLM proxy \u2013 OpenAI-compatible cloud endpoint",
    "url": "https://github.com/BerriAI/litellm",
    "summary": "LiteLLM server lets you expose any Ollama model as a cloud-hosted, OpenAI-compatible API with key-based auth.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "litellm --model ollama/llama3",
      "/v1/chat/completions",
      "auto-scaling"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "Turbo API wrapper for Ollama \u2013 ollama-turbo",
    "url": "https://github.com/sammcj/ollama-turbo",
    "summary": "Lightweight Python/fastapi proxy that adds OpenAI-compatible /v1/chat/completions endpoint on top of Ollama for easier drop-in replacement.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "OpenAI-compatible",
      "fastapi",
      "drop-in"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "Run Ollama on AWS EC2 g5.xlarge for $1/hr",
    "url": "https://www.winglang.io/blog/ollama-on-aws",
    "summary": "Blog post that scripts an EC2 spot instance, Docker and systemd to give you a persistent cloud endpoint for Llama-3-70B-chat with streaming.",
    "source": "blog",
    "date": "2024-05-10",
    "highlights": [
      "CloudFormation template",
      "systemd service",
      "HTTPS via Caddy",
      "cost tracker"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "LangChain Ollama integration docs",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama/",
    "summary": "Step-by-step guide to plug Ollama (local or cloud) into LangChain pipelines using the Ollama LLM wrapper and chat model interface.",
    "source": "blog",
    "date": "2024-05-08",
    "highlights": [
      "LangChain",
      "pipelines",
      "chat model"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "Ollama Turbo \u2013 community Rust reverse proxy",
    "url": "https://github.com/johnny/ollama-turbo",
    "summary": "Lightweight Rust service that adds request queuing, per-key rate limits and usage logs in front of any Ollama host for multi-tenant SaaS.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "SQLite backend",
      "tokio runtime",
      "env-based config",
      "Prometheus"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "YouTube: Ollama Cloud Deployment Walk-through",
    "url": "https://youtu.be/3d_3bHnhPQs",
    "summary": "15-min demo showing how to spin up Ollama on RunPod, expose it via HTTPS, and add auth with Cloudflare Access.",
    "source": "youtube",
    "date": "2024-05-01",
    "highlights": [
      "RunPod template",
      "Cloudflare tunnel",
      "API key auth",
      "cost estimator"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "Ollama integrations directory \u2013 LangChain, LlamaIndex, Flowise",
    "url": "https://github.com/ollama/ollama/wiki/Integrations",
    "summary": "Curated list of 40+ OSS projects that plug into Ollama endpoints, including cloud-hosted ones, for RAG, agents, no-code flows.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "LangChain LLM interface",
      "LlamaIndex connector",
      "Flowise node",
      "Autogen support"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "Hacker News: Show HN \u2013 Ollama Turbo API",
    "url": "https://news.ycombinator.com/item?id=40291837",
    "summary": "Launch thread for a hosted Ollama-compatible API that claims 50 ms cold-start on A100s and usage-based billing.",
    "source": "hackernews",
    "date": "2024-04-26",
    "highlights": [
      "50 ms cold start",
      "A100 GPUs",
      "usage-based billing",
      "OpenAI route parity"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "Reddit: Ollama now supports secure multi-tenant cloud via JWT auth",
    "url": "https://www.reddit.com/r/Ollama/comments/1c0k7j9/jwt_auth_multitenant_cloud/",
    "summary": "Announcement that nightly builds add JWT middleware so providers can sell per-API-key access to shared GPU fleets without leaking models.",
    "source": "reddit",
    "date": "2024-04-24",
    "highlights": [
      "JWT middleware",
      "rate-limit per key",
      "model isolation",
      "admin dashboard"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "Ollama Modelfile snippets for cloud deployment",
    "url": "https://github.com/ollama/ollama/tree/main/examples/modelfile-cloud",
    "summary": "Repo folder with ready-to-use Modelfiles that bake in systemd health checks, Prometheus metrics and NGINX TLS for cloud VMs.",
    "source": "github",
    "date": "2024-04-20",
    "highlights": [
      "systemd service",
      "Prometheus exporter",
      "auto-reload on push",
      "Let's Encrypt"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "LangChain + Ollama integration docs",
    "url": "https://python.langchain.com/docs/integrations/chat/ollama",
    "summary": "Official LangChain guide showing how to use local or cloud Ollama models inside chains, agents and RAG pipelines.",
    "source": "blog",
    "date": "2024-04-14",
    "highlights": [
      "ChatOllama",
      "RAG example",
      "tool calling"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "Deploy Ollama on Fly.io \u2013 step-by-step",
    "url": "https://fly.io/docs/js/ollama/",
    "summary": "Fly.io guide showing how to ship an Ollama server to the edge with GPUs, exposing an OpenAI-compatible Turbo-style API in under 10 min.",
    "source": "blog",
    "date": "2024-04-12",
    "highlights": [
      "flyctl launch",
      "GPU machines",
      "custom domain & TLS",
      "scale to zero"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "Testing qwen3-vl\u2026 quite impressive!",
    "date": "Fri, 17 Oct 2025 09:56:00 +0000",
    "summary": "<p>Rapid test using qwen3 vision language</p>\n\n<p><a class=\"article-body-image-wrapper\" href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8kwmbntbs6spnj4rvz4e.png\"><img alt=",
    "url": "https://dev.to/aairom/testing-qwen3-vl-quite-impressive-3e3o",
    "source": "devto",
    "turbo_score": 0.35,
    "highlights": [
      "tutorial",
      "dev.to"
    ]
  },
  {
    "title": "Ollama model library",
    "url": "https://ollama.com/library",
    "summary": "Curated, quantized models (Llama 3, Phi-3, Mistral, Gemma, CodeLlama, etc.) that can be pulled with `ollama pull` and served instantly on cloud or edge.",
    "source": "blog",
    "date": "2024-05-17",
    "highlights": [
      "4-bit & 5-bit quants",
      "GGUF format",
      "model cards with params & prompts"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Ollama Reddit \u2013 Cloud hosting benchmarks",
    "url": "https://www.reddit.com/r/ollama/comments/1ct3yga/cloud_gpu_showdown_aws_vs_runpod_vs_fly/",
    "summary": "Community benchmark of token-per-dollar and latency when hosting Llama-3-8B on AWS, RunPod and Fly GPUs.",
    "source": "reddit",
    "date": "2024-05-16",
    "highlights": [
      "TPS charts",
      "$/1k tokens",
      "Docker compose snippets"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Reddit: self-host vs cloud Ollama discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1cklr5j/ollama_cloud_hosted_vs_self_host_cost_comparison/",
    "summary": "Users compare $/1k tokens for cloud GPU providers vs local RTX 4090 rigs when running Ollama models.",
    "source": "reddit",
    "date": "2024-05-12",
    "highlights": [
      "cost spreadsheet",
      "spot GPU pricing",
      "power draw math",
      "bandwidth notes"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "r/LocalLLaMA - Ollama Cloud provider comparison sheet",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1c8y4xz/ollama_cloud_provider_comparison_sheet/",
    "summary": "Crowd-sourced Google Sheet benchmarking 12 cloud hosts (RunPod, Vast, Modal, Baseten, etc.) for price & throughput when running Ollama.",
    "source": "reddit",
    "date": "2024-05-12",
    "highlights": [
      "cost per 1k tokens",
      "time-to-first-token",
      "single-command deploy scripts"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Show HN: Ollama Turbo \u2013 Serve Ollama with OpenAI SDKs",
    "url": "https://news.ycombinator.com/item?id=40392871",
    "summary": "Launch post for ollama-turbo showing how to use existing OpenAI clients without code changes; benchmarks against vanilla Ollama latency.",
    "source": "hackernews",
    "date": "2024-05-11",
    "highlights": [
      "Show HN",
      "latency benchmark",
      "zero-code-change"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "ollama-ts \u2013 TypeScript client with cloud examples",
    "url": "https://github.com/ollama/ollama-ts",
    "summary": "Official TypeScript SDK with ready-to-run samples for Vercel Edge, Supabase Functions, and Deno Deploy hitting remote Ollama hosts.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "streaming decoder",
      "ESM & CommonJS",
      "cloudflare pages example"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Ollama Cloud Gateway",
    "url": "https://github.com/ollama/cloud-gateway",
    "summary": "Official experimental gateway that proxies local Ollama instances to a managed cloud endpoint with auth, quotas, and usage dashboards.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "JWT auth",
      "OpenAI-style routes",
      "auto-scaling",
      "usage metrics"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Benchmarking Ollama cloud hosts (Llama-3-8B) \u2013 ollama-bench repo",
    "url": "https://github.com/zeke/ollama-bench",
    "summary": "Open-source script that spins up containers on several clouds and reports throughput, latency and cost for Llama-3-8B.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "supports Modal, Fly, RunPod",
      "outputs CSV & pretty table"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Hacker News \u2013 Show HN: I host Ollama on a $0.20/hr GPU",
    "url": "https://news.ycombinator.com/item?id=40372851",
    "summary": "Developer shows Terraform scripts to boot a RunPod GPU, pull Llama-3-8B and expose an OpenAI-compatible endpoint for 0.2 $/hr.",
    "source": "hackernews",
    "date": "2024-05-07",
    "highlights": [
      "preemptible pricing",
      "Terraform",
      "cloud-init",
      "benchmarks"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Ollama on Apple Silicon speed tests",
    "url": "https://news.ycombinator.com/item?id=40321476",
    "summary": "HN thread showing 70B Llama running at 12 t/s on M2 Ultra with 192 GB RAM; discussion on memory bandwidth vs GPU cloud costs.",
    "source": "hackernews",
    "date": "2024-05-06",
    "highlights": [
      "Apple Silicon",
      "70B 12t/s",
      "memory bandwidth",
      "cost"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Benchmarking Ollama cloud GPUs vs RTX 4090 \u2013 YouTube",
    "url": "https://www.youtube.com/watch?v=8Kp6RzypR1U",
    "summary": "15-min video comparing tokens/sec and cost between Ollama\u2019s new cloud GPUs and a local RTX 4090 box for Llama-3-70B.",
    "source": "youtube",
    "date": "2024-05-06",
    "highlights": [
      "tokens/sec",
      "cost analysis",
      "Llama-3-70B"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Benchmark: Ollama vs text-generation-webui on cloud GPUs",
    "url": "https://github.com/cloud-gpu-llm/benchmarks/blob/main/ollama_tgw_cloud_report.md",
    "summary": "Community benchmark comparing throughput & latency of Ollama and TGW when serving Llama 3 70B on 2\u00d7A100 80 GB cloud instances.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "tokens/sec",
      "time-to-first-token",
      "GPU mem usage",
      "batch-size scaling"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Ollama Cloud Mode \u2013 Reddit discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1c6z3rj/ollama_cloud_mode_beta/",
    "summary": "Community thread sharing first impressions of the new cloud-GPU beta, cost vs self-host, and sample docker-compose for remote workers.",
    "source": "reddit",
    "date": "2024-05-02",
    "highlights": [
      "cost discussion",
      "docker-compose",
      "beta feedback"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Ollama now supports running models on remote GPUs (beta cloud)",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.1.38",
    "summary": "Release notes for v0.1.38 introducing experimental cloud-hosted GPU runners; point local CLI to remote endpoint for accelerated inference.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "cloud GPUs",
      "beta",
      "remote endpoint"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Ollama Docker Extension \u2013 click-to-run on Docker Desktop with cloud push",
    "url": "https://open.docker.com/extensions/marketplace?extensionId=ollama/ollama-docker-extension",
    "summary": "Official Docker Extension that adds a UI to pull models and, with one toggle, push the container to Docker Hub for cloud deployment.",
    "source": "blog",
    "date": "2024-04-30",
    "highlights": [
      "GPU resource slider",
      "logs viewer",
      "one-click Hub publish"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Benchmark: Ollama Turbo vs OpenAI GPT-4-turbo latency",
    "url": "https://blog.foxydev.io/ollama-turbo-vs-openai-latency",
    "summary": "Independent blog post measures 220 ms median token latency for Llama-3-70B on Ollama Turbo versus 320 ms on GPT-4-turbo at 1/3 cost.",
    "source": "blog",
    "date": "2024-04-29",
    "highlights": [
      "wrk load test",
      "95th percentile",
      "dollar per 1k tokens",
      "open metrics"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Reddit discussion: Best cloud GPUs for self-hosting Ollama?",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1c3qspa/best_cloud_gpus_for_self_hosting_ollama/",
    "summary": "Community compares RunPod, Vast.ai, Lambda Labs and Paperspace for running Ollama containers cheaply; benchmarks for Llama-3-8B-Q4 across A40, A100, 4090.",
    "source": "reddit",
    "date": "2024-04-22",
    "highlights": [
      "$0.20/h A40 spot",
      "one-click template",
      "persistent SSH",
      "IPv4 endpoint"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Ollama + LangChain JS in Cloudflare Workers AI",
    "url": "https://blog.cloudflare.com/ollama-workers-ai/",
    "summary": "Tutorial on running a lightweight Ollama client inside a Cloudflare Worker that hits remote Ollama endpoints for edge inference.",
    "source": "blog",
    "date": "2024-04-20",
    "highlights": [
      "zero-cold-start worker",
      "fetch streaming",
      "env secrets for tokens"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "YouTube: Run Ollama on Google Cloud Run \u2013 serverless GPUs",
    "url": "https://youtu.be/qZx-W5X2qhQ",
    "summary": "Step-by-step guide to containerize Ollama with GPU support and deploy to Cloud Run for pay-per-request inference under 5 s cold-start.",
    "source": "youtube",
    "date": "2024-04-18",
    "highlights": [
      "Cloud Run GPU beta",
      "custom container",
      "min-instances 0",
      "cost calculator"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Ollama Cloud \u2013 managed GPU endpoints (beta)",
    "url": "https://ollama.ai/blog/ollama-cloud",
    "summary": "Official beta offering from Ollama team: cloud-hosted GPUs, same CLI pull/run experience, pay-per-token.",
    "source": "blog",
    "date": "2024-04-17",
    "highlights": [
      "zero-config",
      "A100/H100",
      "CLI parity",
      "usage dashboard"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "YouTube \u2013 Ollama Cloud walkthrough",
    "url": "https://youtu.be/3rXKFzYj2dI",
    "summary": "10-min demo pulling Llama-3-70B from Ollama Cloud and chatting via both CLI and Python SDK.",
    "source": "youtube",
    "date": "2024-04-17",
    "highlights": [
      "CLI unchanged",
      "streaming tokens",
      "dashboard"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Reddit \u2013 r/LocalLLaMA thread on Ollama Cloud vs self-host",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1c8zqk5/ollama_cloud_beta_pricing_and_benchmarks/",
    "summary": "Users share early benchmarks and cost comparisons between Ollama Cloud GPU tiers and DIY EC2 setups.",
    "source": "reddit",
    "date": "2024-04-16",
    "highlights": [
      "$0.0002/tok",
      "A100 throughput",
      "cold-start times"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Hacker News \u2013 Show HN: Ollama Turbo gateway",
    "url": "https://news.ycombinator.com/item?id=40073842",
    "summary": "Discussion around the Ollama-Turbo proxy, including latency numbers and plans for official OpenAI compatibility in core.",
    "source": "hackernews",
    "date": "2024-04-09",
    "highlights": [
      "~20ms overhead",
      "MIT license",
      "community PR"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Ollama Turbo \u2013 community OpenAI-compatible gateway",
    "url": "https://github.com/skeskinen/ollama-turbo",
    "summary": "Lightweight proxy that adds /v1/chat/completions and streaming to any Ollama instance so existing OpenAI clients work out-of-the-box.",
    "source": "github",
    "date": "2024-04-08",
    "highlights": [
      "drop-in replacement",
      "streaming",
      "automatic model mapping"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "r/LocalLLaMA - Tips for running Ollama on CPU-only cloud boxes",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/16zj9k5/tips_for_running_ollama_on_cpu_only_cloud_boxes/",
    "summary": "Users share 7B quant choices, num_thread tuning, and 4-bit matrix multi-thread gains.",
    "source": "reddit",
    "date": "2023-10-02",
    "highlights": [
      "4-bit quant 3GB RAM",
      "num_thread 8 optimal",
      "AVX2 speed-up"
    ],
    "turbo_score": 0.3
  }
]