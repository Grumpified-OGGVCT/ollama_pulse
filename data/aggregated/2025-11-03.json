[
  {
    "title": "Ollama Turbo \u2013 1-click cloud GPU images",
    "url": "https://github.com/ollama-turbo/cloud-images",
    "summary": "Community project that bakes Ollama into GPU-enabled AWS & GCP AMIs with auto-scaling and pay-per-token billing scripts.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "pre-loaded models",
      "Terraform templates",
      "Cloud-Init",
      "spot-price optimizer"
    ],
    "turbo_score": 0.75
  },
  {
    "title": "Ollama v0.12.0: v0.12.0",
    "date": "2025-09-18T17:29:57Z",
    "summary": "## Cloud models\r\n\r\n<img width=\"512\" alt=\"Ollama_cloud_background\" src=\"https://github.com/user-attachments/assets/7f36e60c-dd33-4eac-babd-a2b7df89bc2f\" />\r\n\r\n[Cloud models](https://ollama.com/blog/cloud-models) are now available in preview, allowing you to run a group of larger models with fast, datacenter-grade hardware.\r\n\r\nTo run a cloud model, use:\r\n\r\n```\r\nollama run qwen3-coder:480b-cloud\r\n```\r\n\r\n* [View all](https://ollama.com/search?c=cloud) cloud models\r\n* [Blog post](https://ollama.com/b",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.12.0",
    "source": "ollama_releases",
    "turbo_score": 0.6,
    "highlights": [
      "version: v0.12.0",
      "prerelease: False",
      "assets: 13",
      "author: github-actions[bot]"
    ]
  },
  {
    "title": "Ollama Discord bot (cloud-hosted)",
    "url": "https://github.com/ollama-discord/bot",
    "summary": "Open-source Discord bot that hits a managed Ollama Turbo endpoint so every guild can chat with Llama 3 without self-hosting.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "slash commands",
      "role-based access",
      "per-guild quotas",
      "streaming replies"
    ],
    "turbo_score": 0.6
  },
  {
    "title": "Model: qwen3-vlThe most powerful vision-language model in the Qwen model family to date.visioncloud2b4b8b30b32b235b64KPulls59TagsUpdated4 days ago",
    "date": "2025-11-03T01:21:49.738113",
    "summary": "Ollama model available in library: qwen3-vlThe most powerful vision-language model in the Qwen model family to date.visioncloud2b4b8b30b32b235b64KPulls59TagsUpdated4 days ago",
    "url": "https://ollama.com/library/qwen3-vl",
    "source": "ollama_models",
    "turbo_score": 0.55,
    "highlights": [
      "model registry",
      "ollama library"
    ]
  },
  {
    "title": "Ollama v0.12.3: v0.12.3",
    "date": "2025-09-26T05:08:26Z",
    "summary": "## New models\r\n\r\n- [DeepSeek-V3.1-Terminus](https://ollama.com/library/deepseek-v3.1): DeepSeek-V3.1-Terminus is a hybrid model that supports both thinking mode and non-thinking mode. It delivers more stable & reliable outputs across benchmarks compared to the previous version:\r\n\r\n  Run on [Ollama's cloud](https://ollama.com/cloud):\r\n  \r\n  ```\r\n  ollama run deepseek-v3.1:671b-cloud\r\n  ```\r\n  \r\n  Run locally (requires 500GB+ of VRAM)\r\n  \r\n  ```\r\n  ollama run deepseek-v3.1\r\n  ```\r\n\r\n- [Kimi-K2-Ins",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.12.3",
    "source": "ollama_releases",
    "turbo_score": 0.5,
    "highlights": [
      "version: v0.12.3",
      "prerelease: False",
      "assets: 13",
      "author: github-actions[bot]"
    ]
  },
  {
    "title": "ollama-terraform",
    "url": "https://github.com/ollama/ollama-terraform",
    "summary": "Terraform module to provision Ollama on AWS EC2 with GPU.",
    "source": "github",
    "date": "2024-05-26",
    "highlights": [
      "g5.xlarge GPU",
      "Cloud-init",
      "Spot instances"
    ],
    "turbo_score": 0.45
  },
  {
    "title": "A Beginner's Guide to Ollama Cloud Models",
    "date": "Sun, 19 Oct 2025 22:35:08 +0000",
    "summary": "<p>Ollama's cloud models are a new feature that allows users to run large language models without needing a powerful local GPU. These models are automatically offloaded to Ollama's cloud service, providing the same capabilities as local models while enabling the use of larger models that would typic",
    "url": "https://dev.to/coderforfun/a-beginners-guide-to-ollama-cloud-models-3lc2",
    "source": "devto",
    "turbo_score": 0.4,
    "highlights": [
      "tutorial",
      "dev.to"
    ]
  },
  {
    "title": "Model: glm-4.6Advanced agentic, reasoning and coding capabilities.cloud14.2KPulls1TagUpdated2 weeks ago",
    "date": "2025-11-03T01:21:49.738204",
    "summary": "Ollama model available in library: glm-4.6Advanced agentic, reasoning and coding capabilities.cloud14.2KPulls1TagUpdated2 weeks ago",
    "url": "https://ollama.com/library/glm-4.6",
    "source": "ollama_models",
    "turbo_score": 0.4,
    "highlights": [
      "model registry",
      "ollama library"
    ]
  },
  {
    "title": "Model: qwen3-coderAlibaba's performant long context models for agentic and coding tasks.toolscloud30b480b597.8KPulls10TagsUpdated1 month ago",
    "date": "2025-11-03T01:21:49.738159",
    "summary": "Ollama model available in library: qwen3-coderAlibaba's performant long context models for agentic and coding tasks.toolscloud30b480b597.8KPulls10TagsUpdated1 month ago",
    "url": "https://ollama.com/library/qwen3-coder",
    "source": "ollama_models",
    "turbo_score": 0.4,
    "highlights": [
      "model registry",
      "ollama library"
    ]
  },
  {
    "title": "Model: gpt-ossOpenAI\u2019s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.toolsthinkingcloud20b120b3.9MPulls5TagsUpdated3 weeks ago",
    "date": "2025-11-03T01:21:49.738080",
    "summary": "Ollama model available in library: gpt-ossOpenAI\u2019s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.toolsthinkingcloud20b120b3.9MPulls5TagsUpdated3 weeks ago",
    "url": "https://ollama.com/library/gpt-oss",
    "source": "ollama_models",
    "turbo_score": 0.4,
    "highlights": [
      "model registry",
      "ollama library"
    ]
  },
  {
    "title": "Ollama v0.12.7: v0.12.7",
    "date": "2025-10-29T02:07:54Z",
    "summary": "<img width=\"600\" alt=\"Ollama screenshot 2025-10-29 at 13 56 55@2x\" src=\"https://github.com/user-attachments/assets/4fea0b30-5d31-4da2-b99c-7f38606fc0a2\" />\r\n\r\n## New models\r\n\r\n- [Qwen3-VL](https://ollama.com/library/qwen3-vl): Qwen3-VL is now available in all parameter sizes ranging from 2B to 235B\r\n- [MiniMax-M2](https://ollama.com/library/minimax-m2): a 230 Billion parameter model built for coding & agentic workflows available on Ollama's cloud\r\n\r\n## Add files and adjust thinking levels in Oll",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.12.7",
    "source": "ollama_releases",
    "turbo_score": 0.4,
    "highlights": [
      "version: v0.12.7",
      "prerelease: False",
      "assets: 13",
      "author: github-actions[bot]"
    ]
  },
  {
    "title": "Ollama v0.12.2: v0.12.2",
    "date": "2025-09-24T21:19:20Z",
    "summary": "## Web search\r\n\r\n<img width=\"512\" alt=\"ollama_web_search\" src=\"https://github.com/user-attachments/assets/fc49a8bc-7a3f-462c-901c-5a9625c082c3\" />\r\n\r\n[A new web search API](https://ollama.com/blog/web-search) is now available in Ollama. Ollama provides a generous free tier of web searches for individuals to use, and higher rate limits are available via [Ollama\u2019s cloud](https://ollama.com/cloud). This web search capability can augment models with the latest information from the web to reduce hall",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.12.2",
    "source": "ollama_releases",
    "turbo_score": 0.4,
    "highlights": [
      "version: v0.12.2",
      "prerelease: False",
      "assets: 13",
      "author: github-actions[bot]"
    ]
  },
  {
    "title": "Ollama v0.12.1: v0.12.1",
    "date": "2025-09-21T23:19:05Z",
    "summary": "## New models\r\n- [Qwen3 Embedding](https://ollama.com/library/qwen3-embedding): state of the art open embedding model by the Qwen team\r\n\r\n## What's Changed\r\n* Qwen3-Coder now supports tool calling\r\n* Ollama's app will now longer show \"connection lost\" in error when connecting to cloud models\r\n* Fixed issue where Gemma3 QAT models would not output correct tokens\r\n* Fix issue where `&` characters in Qwen3-Coder would not be parsed correctly when function calling\r\n* Fixed issues where `ollama signi",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.12.1",
    "source": "ollama_releases",
    "turbo_score": 0.4,
    "highlights": [
      "version: v0.12.1",
      "prerelease: False",
      "assets: 13",
      "author: github-actions[bot]"
    ]
  },
  {
    "title": "YouTube: Ollama Cloud Deployment Walk-through",
    "url": "https://youtu.be/3d_3bHnhPQs",
    "summary": "15-min demo showing how to spin up Ollama on RunPod, expose it via HTTPS, and add auth with Cloudflare Access.",
    "source": "youtube",
    "date": "2024-05-01",
    "highlights": [
      "RunPod template",
      "Cloudflare tunnel",
      "API key auth",
      "cost estimator"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "Hacker News: Show HN \u2013 Ollama Turbo API",
    "url": "https://news.ycombinator.com/item?id=40291837",
    "summary": "Launch thread for a hosted Ollama-compatible API that claims 50 ms cold-start on A100s and usage-based billing.",
    "source": "hackernews",
    "date": "2024-04-26",
    "highlights": [
      "50 ms cold start",
      "A100 GPUs",
      "usage-based billing",
      "OpenAI route parity"
    ],
    "turbo_score": 0.4
  },
  {
    "title": "Testing qwen3-vl\u2026 quite impressive!",
    "date": "Fri, 17 Oct 2025 09:56:00 +0000",
    "summary": "<p>Rapid test using qwen3 vision language</p>\n\n<p><a class=\"article-body-image-wrapper\" href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8kwmbntbs6spnj4rvz4e.png\"><img alt=",
    "url": "https://dev.to/aairom/testing-qwen3-vl-quite-impressive-3e3o",
    "source": "devto",
    "turbo_score": 0.35,
    "highlights": [
      "tutorial",
      "dev.to"
    ]
  },
  {
    "title": "Model: qwenQwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters0.5b1.8b4b7b14b32b72b110b5MPulls379TagsUpdated1 year ago",
    "date": "2025-11-03T01:21:49.738556",
    "summary": "Ollama model available in library: qwenQwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters0.5b1.8b4b7b14b32b72b110b5MPulls379TagsUpdated1 year ago",
    "url": "https://ollama.com/library/qwen",
    "source": "ollama_models",
    "turbo_score": 0.3,
    "highlights": [
      "model registry",
      "ollama library"
    ]
  },
  {
    "title": "Model: deepseek-v3.1DeepSeek-V3.1-Terminus is a hybrid model that supports both thinking mode and non-thinking mode.toolsthinkingcloud671b126.6KPulls8TagsUpdated1 month ago",
    "date": "2025-11-03T01:21:49.738273",
    "summary": "Ollama model available in library: deepseek-v3.1DeepSeek-V3.1-Terminus is a hybrid model that supports both thinking mode and non-thinking mode.toolsthinkingcloud671b126.6KPulls8TagsUpdated1 month ago",
    "url": "https://ollama.com/library/deepseek-v3.1",
    "source": "ollama_models",
    "turbo_score": 0.3,
    "highlights": [
      "model registry",
      "ollama library"
    ]
  },
  {
    "title": "Reddit: self-host vs cloud Ollama discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1cklr5j/ollama_cloud_hosted_vs_self_host_cost_comparison/",
    "summary": "Users compare $/1k tokens for cloud GPU providers vs local RTX 4090 rigs when running Ollama models.",
    "source": "reddit",
    "date": "2024-05-12",
    "highlights": [
      "cost spreadsheet",
      "spot GPU pricing",
      "power draw math",
      "bandwidth notes"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Ollama Cloud Gateway",
    "url": "https://github.com/ollama/cloud-gateway",
    "summary": "Official experimental gateway that proxies local Ollama instances to a managed cloud endpoint with auth, quotas, and usage dashboards.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "JWT auth",
      "OpenAI-style routes",
      "auto-scaling",
      "usage metrics"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "Benchmark: Ollama vs text-generation-webui on cloud GPUs",
    "url": "https://github.com/cloud-gpu-llm/benchmarks/blob/main/ollama_tgw_cloud_report.md",
    "summary": "Community benchmark comparing throughput & latency of Ollama and TGW when serving Llama 3 70B on 2\u00d7A100 80 GB cloud instances.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "tokens/sec",
      "time-to-first-token",
      "GPU mem usage",
      "batch-size scaling"
    ],
    "turbo_score": 0.3
  },
  {
    "title": "r/LocalLLaMA - Tips for running Ollama on CPU-only cloud boxes",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/16zj9k5/tips_for_running_ollama_on_cpu_only_cloud_boxes/",
    "summary": "Users share 7B quant choices, num_thread tuning, and 4-bit matrix multi-thread gains.",
    "source": "reddit",
    "date": "2023-10-02",
    "highlights": [
      "4-bit quant 3GB RAM",
      "num_thread 8 optimal",
      "AVX2 speed-up"
    ],
    "turbo_score": 0.3
  }
]