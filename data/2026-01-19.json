[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: self-contained Llama 2, Mistral, Gemma, Phi-3, etc. runner with a REST API and CLI for macOS, Linux, Windows.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "built-in model library",
      "Modelfile DSL",
      "GPU/CPU fallback",
      "OpenAI-compatible /api/chat endpoint"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official TypeScript/JavaScript client for the Ollama API\u2014works in Node, Deno, Bun and browsers.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Promise-based",
      "streaming support",
      "zero native deps",
      "npm i ollama"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client and asyncio bindings for Ollama; chat, generate, embed, pull, list models.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "sync & async APIs",
      "PyPI: ollama",
      "type hints",
      "pull/progress callbacks"
    ]
  },
  {
    "title": "langchain-ollama (PyPI)",
    "url": "https://pypi.org/project/langchain-ollama/",
    "summary": "LangChain adapter giving ChatOllama, OllamaEmbeddings, OllamaFunctions and tool-calling support.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install langchain-ollama",
      "tool invocation",
      "streaming",
      "batch embed"
    ]
  },
  {
    "title": "ollama-rs",
    "url": "https://github.com/pepperoni21/ollama-rs",
    "summary": "Community Rust crate for Ollama\u2019s REST API with async/tokio and streaming support.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "crates.io: ollama-rs",
      "strong typing",
      "chat/generate/embed",
      "vision models"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java / Kotlin SDK for Ollama with builder-style requests, streaming, and Maven Central availability.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Maven: ollama4j",
      "Spring integration examples",
      "async callbacks"
    ]
  },
  {
    "title": "ollama-webui (Ollama-WebUI)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Full-featured ChatGPT-style web frontend for Ollama\u2014Docker one-liner, multi-user, RAG uploads.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "offline PDF/txt RAG",
      "Dark/Light themes",
      "admin panel",
      "OpenAI drop-in"
    ]
  },
  {
    "title": "continue",
    "url": "https://github.com/continuedev/continue",
    "summary": "VS Code & JetBrains plugin that adds local code-completion via Ollama models plus inline chat and RAG.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "inline autocomplete",
      "bring-your-own Ollama model",
      "embeddings for codebase RAG"
    ]
  },
  {
    "title": "chainlit + Ollama cookbook",
    "url": "https://github.com/Chainlit/cookbook/tree/main/ollama",
    "summary": "Ready-to-run Chainlit app showing streaming chat with Llama 3 served by Ollama.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pip install chainlit",
      "real-time token streaming",
      "UI in 20 lines"
    ]
  },
  {
    "title": "haystack-ollama",
    "url": "https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/ollama",
    "summary": "Deepset Haystack 2.x integration offering OllamaGenerator and OllamaChatGenerator nodes.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pipeline YAML config",
      "RAG ready",
      "embedders coming soon"
    ]
  },
  {
    "title": "ollama-copilot (obsidian-ollama)",
    "url": "https://github.com/michaelpeterswa/obsidian-ollama",
    "summary": "Obsidian plugin that brings local LLM summarization, rewriting and Q&A via Ollama.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "command palette",
      "template prompts",
      "no cloud calls"
    ]
  },
  {
    "title": "ollama-cli (rich TUI)",
    "url": "https://github.com/jmorganca/ollama-cli",
    "summary": "Community Rich-based terminal UI for interactive model picking, chat history, markdown rendering.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "keyboard nav",
      "conversation save",
      "pipx install"
    ]
  },
  {
    "title": "ollama-codex",
    "url": "https://github.com/ollama-codex/ollama-codex",
    "summary": "Drop-in replacement for OpenAI Codex using Ollama\u2019s /api/generate\u2014powers CLI coding assistant.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "edit/insert modes",
      "multiple model support",
      "Docker image"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU node selection and PVC.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "autoscaling HPA",
      "nvidia-device-plugin",
      "configMap for env vars"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/valayDave/ollama-docker-compose",
    "summary": "Compose stack bundling Ollama + Open WebUI + Postgres metadata store for easy local/team labs.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "one-command up",
      "volume caching",
      "ARM64 & x86 images"
    ]
  },
  {
    "title": "llama-index-llms-ollama",
    "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/llms/llama-index-llms-ollama",
    "summary": "LlamaIndex native LLM integration enabling RAG pipelines backed by any Ollama model.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "pip install llama-index-llms-ollama",
      "streaming",
      "structured output"
    ]
  },
  {
    "title": "ollama-rag-chatbot (Streamlit)",
    "url": "https://github.com/yankeeboy76/ollama-rag-chatbot",
    "summary": "Streamlit app that ingests PDF/CSV and answers questions using Ollama + Chroma vector store.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "drag-and-drop files",
      "selectable models",
      "Dockerfile included"
    ]
  },
  {
    "title": "discord-ollama-bot",
    "url": "https://github.com/mikegirkin/discord-ollama-bot",
    "summary": "Discord.py bot that streams Ollama responses in threads with slash commands and model switching.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "async streaming",
      "/chat /model /reset commands",
      "env token config"
    ]
  },
  {
    "title": "ollama-npm (unofficial wrapper)",
    "url": "https://www.npmjs.com/package/ollama-npm",
    "summary": "Lightweight community npm package wrapping Ollama REST endpoints for Node apps.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "TypeScript defs",
      "ESM & CJS",
      "zero deps"
    ]
  },
  {
    "title": "r/ollama \u2013 model discussions",
    "url": "https://www.reddit.com/r/ollama/comments/1c3kq9k/just_dropped_llama3_8b_instruct_qlora/",
    "summary": "Active subreddit sharing new quantized models, benchmarks and GPU memory tips for Ollama.",
    "source": "reddit",
    "date": "2024-05-13",
    "highlights": [
      "user-made GGUFs",
      "performance tables",
      "install guides"
    ]
  }
]