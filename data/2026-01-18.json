[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: get up and running with Llama 2, Mistral, Gemma, and other large language models locally.",
    "source": "github",
    "date": "2023-06-26",
    "highlights": [
      "self-contained LLM runner",
      "macOS/Linux/Windows",
      "Docker image",
      "REST API"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python library for Ollama: chat, generate, embed, pull & manage models with a few lines of code.",
    "source": "github",
    "date": "2023-10-02",
    "highlights": [
      "sync/async clients",
      "streaming support",
      "PyPI: ollama"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama; works in Node, Bun, Deno and browsers (via proxy).",
    "source": "github",
    "date": "2023-09-12",
    "highlights": [
      "npm: ollama",
      "Promise & stream APIs",
      "full TypeScript defs"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/ollama",
    "summary": "LangChain integration package letting you use any Ollama model as an LLM or embeddings provider.",
    "source": "github",
    "date": "2023-11-15",
    "highlights": [
      "pip: langchain-ollama",
      "chat & embed endpoints",
      "tool-calling support"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI for Ollama (formerly Ollama-WebUI): themes, code highlighting, model manager, RAG.",
    "source": "github",
    "date": "2023-10-05",
    "highlights": [
      "Docker one-liner",
      "OpenAI-compatible API toggle",
      "multi-user support"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama with fluent builder API, streaming, and Android compatibility.",
    "source": "github",
    "date": "2023-11-20",
    "highlights": [
      "Maven Central",
      "async & reactive streams",
      "Spring Boot starter"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/gbagnoli/ollama-rb",
    "summary": "Community Ruby gem wrapping Ollama\u2019s REST API; supports chat, generate, pull and list operations.",
    "source": "github",
    "date": "2023-12-01",
    "highlights": [
      "RubyGems: ollama",
      "streaming support",
      "RSpec test suite"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/salty-flower/ollama-cli",
    "summary": "Cross-platform TUI (Rust) for chatting, managing models and viewing logs without leaving the terminal.",
    "source": "github",
    "date": "2024-01-10",
    "highlights": [
      "keyboard-driven UI",
      "download progress bars",
      "config profiles"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "VS Code extension that turns any Ollama model into a local GitHub Copilot replacement.",
    "source": "github",
    "date": "2023-12-18",
    "highlights": [
      "inline completions",
      "custom prompt templates",
      "no cloud calls"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration to use Ollama models as generators or chatters in production pipelines.",
    "source": "github",
    "date": "2023-11-08",
    "highlights": [
      "pip install farm-haystack[ollama]",
      "RAG ready",
      "batch inference"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official Docker image and compose stacks (CPU/GPU) for running Ollama server plus optional WebUI.",
    "source": "github",
    "date": "2023-07-14",
    "highlights": [
      "single-command GPU",
      "volume caching",
      "AMD ROCm support"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community Helm chart for deploying Ollama on Kubernetes with GPU node-selector and PVC templates.",
    "source": "github",
    "date": "2023-12-03",
    "highlights": [
      "autoscaling HPA",
      "configurable models list",
      "ingress ready"
    ]
  },
  {
    "title": "ollama-model-hub",
    "url": "https://github.com/technomancer/ollama-model-hub",
    "summary": "Curated collection of custom Modelfiles (Llava, CodeLlama, SQLCoder, etc.) ready to pull & build.",
    "source": "github",
    "date": "2024-01-22",
    "highlights": [
      "pre-baked prompts",
      "quantization recipes",
      "CI build badges"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ivanfioravivi/ollama-rag",
    "summary": "Minimal RAG template using Ollama + Chroma + LangChain to chat with local PDFs or notes.",
    "source": "github",
    "date": "2023-11-30",
    "highlights": [
      "Dockerized",
      "Gradio UI",
      "embeddings caching"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/jakob777/ollama-discord-bot",
    "summary": "Self-hostable Discord bot that lets servers chat with any Ollama model via slash commands.",
    "source": "github",
    "date": "2023-12-12",
    "highlights": [
      "threading support",
      "rate limiting",
      "model hot-swap"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/NixOS/nixpkgs/pulls?q=ollama",
    "summary": "Nix flake and package definitions to build Ollama with CUDA or ROCm support reproducibly.",
    "source": "github",
    "date": "2024-02-01",
    "highlights": [
      "declarative GPU",
      "module service",
      "cacheable binaries"
    ]
  },
  {
    "title": "ollama-spring-boot-starter",
    "url": "https://github.com/ollama4j/ollama-spring-boot-starter",
    "summary": "Spring Boot auto-configuration injecting an Ollama4j client bean for enterprise Java apps.",
    "source": "github",
    "date": "2024-01-15",
    "highlights": [
      "application.yml config",
      "Micrometer metrics",
      "WebFlux integration"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/ollama-csharp",
    "summary": "Community .NET SDK for Ollama with strongly-typed requests, streaming IAsyncEnumerable and DI support.",
    "source": "github",
    "date": "2023-12-20",
    "highlights": [
      "NuGet: Ollama",
      "ASP.NET Core sample",
      "Blazor chat demo"
    ]
  },
  {
    "title": "ollama-rust-sdk",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Async Rust crate wrapping Ollama\u2019s REST API with tokio, serde and native TLS.",
    "source": "github",
    "date": "2024-02-05",
    "highlights": [
      "crates.io: ollama-rs",
      "SSE streaming",
      "examples folder"
    ]
  },
  {
    "title": "ollama-experiments",
    "url": "https://github.com/mneedham/ollama-experiments",
    "summary": "Collection of notebooks and scripts exploring function-calling, vision models and quantized performance.",
    "source": "github",
    "date": "2024-01-28",
    "highlights": [
      "Jupyter notebooks",
      "benchmark graphs",
      "Modelfile recipes"
    ]
  }
]