[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Ollama project: get up and running with Llama 3, Mistral, Gemma, and other large language models locally.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-contained binary",
      "model management CLI",
      "REST & Go APIs",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client library for Ollama; chat, generate, embed, pull, and manage models with a few lines of code.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip install ollama",
      "sync/async APIs",
      "streaming support",
      "built-in embedding helper"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node and browsers; interact with the local Ollama server via fetch or WebSocket streams.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "npm install ollama",
      "Promise & async iterator APIs",
      "TypeScript types included"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/langchain-ollama",
    "summary": "LangChain integration package exposing Ollama models as LLM, chat, and embedding components.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "pip install langchain-ollama",
      "drop-in replacement for OpenAI",
      "streaming & JSON mode",
      "supports tools"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/llama-assistant/ollama-webui",
    "summary": "Clean open-source web UI for chatting with any Ollama model; runs entirely in browser via the Ollama REST API.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "no backend needed",
      "markdown & code highlighting",
      "multi-model chats",
      "Docker image ready"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Lightweight RAG (retrieval-augmented generation) demo that pairs Ollama with sentence-transformers & Chroma.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "ingest PDFs or text",
      "local embeddings",
      "streaming answers",
      "Gradio UI"
    ]
  },
  {
    "title": "ollama-cli-chat",
    "url": "https://github.com/sugarforever/ollama-cli-chat",
    "summary": "Terminal UI chat client written in Go; supports persistent sessions, syntax highlighting, and markdown rendering.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "go install",
      "vim key-bindings",
      "session history",
      "multi-line input"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/fostermaier/ollama-copilot",
    "summary": "VS Code extension that turns any Ollama model into a local GitHub Copilot alternative; inline suggestions & chat panel.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "no API keys",
      "configurable model per language",
      "streaming completions",
      "open-source"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU & PVC support.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "autoscaling HPA",
      "CUDA device plugin",
      "model pre-load init-container",
      "Prometheus metrics"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/ollama",
    "summary": "Haystack 2.0 integration providing OllamaGenerator and OllamaChatGenerator nodes for building local LLM pipelines.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pip install ollama-haystack",
      "native streaming",
      "compatible with Haystack RAG pipelines"
    ]
  },
  {
    "title": "ollama-n8n-node",
    "url": "https://github.com/n8n-io/n8n/tree/master/packages/nodes-base/nodes/Ollama",
    "summary": "Community n8n node that lets you call Ollama models inside no-code workflows; supports chat & completion operations.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "drag-and-drop",
      "credential-less",
      "JSON & binary input",
      "streaming toggle"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/JVM client library for Ollama; offers synchronous & async APIs, Kotlin coroutines, and Spring Boot starter.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Maven Central",
      "reactive streams",
      "micrometer metrics",
      "Spring Boot auto-configuration"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/Ollama",
    "summary": "Unofficial C# SDK for Ollama; targets .NET 6+ with strongly-typed request/response models and IAsyncEnumerable streaming.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "NuGet install",
      "dependency injection friendly",
      "cancellation tokens",
      "unit-tested"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Rust crate providing async wrappers over the Ollama HTTP API with tokio and serde.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "crates.io",
      "async/await",
      "strong typing",
      "examples for chat & embed"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/rizerphe/ollama-discord",
    "summary": "Self-hosted Discord bot that brings local LLM power to any server; supports slash commands, system prompts, and per-user quotas.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Docker compose",
      "rate limiting",
      "model hot-swap",
      "threading support"
    ]
  },
  {
    "title": "ollama-django",
    "url": "https://github.com/mjbommar/ollama-django",
    "summary": "Reusable Django app exposing Ollama models as async views and Django REST framework endpoints.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "channels & WebSockets",
      "admin panel integration",
      "caching layer",
      "OpenAPI schema"
    ]
  },
  {
    "title": "ollama-elixir",
    "url": "https://github.com/carbonium-ai/ollama-elixir",
    "summary": "Elixir OTP client for Ollama with GenServer-backed connection pooling and LiveView chat examples.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Hex.pm package",
      "back-pressure",
      "telemetry events",
      "Phoenix LiveView demo"
    ]
  },
  {
    "title": "ollama-express",
    "url": "https://github.com/ollama-express/ollama-express",
    "summary": "Starter repo showing how to build a secure Express.js proxy in front of Ollama with JWT auth, rate limits, and OpenAI-compatible routes.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "OpenAI drop-in",
      "Redis rate limiter",
      "Dockerfile",
      "Swagger UI"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/suyash/ollama-slack-bot",
    "summary": "Lightweight Slack bot server written in Python that listens for mentions and replies using any Ollama model.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Socket Mode",
      "threaded replies",
      "model switch via emoji",
      "environment config"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hintergrund/ollama-obsidian",
    "summary": "Obsidian plugin that adds an \u201cAsk Ollama\u201d command to summarize notes, brainstorm ideas, or rewrite text using a local model.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "no cloud costs",
      "templater integration",
      "custom prompts",
      "streaming into editor"
    ]
  }
]