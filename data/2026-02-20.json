[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and server for running large language models locally (Llama 2, Mistral, Gemma, etc.) with a simple pull/run workflow.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "self-contained binary",
      "Docker image",
      "OpenAI-compatible API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "First-party Python client library that wraps Ollama\u2019s REST API for chat, generate, embed, pull, and list operations.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip install ollama",
      "sync & async APIs",
      "type hints",
      "streaming support"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript SDK for Node, Deno, and browsers to interact with a local Ollama server.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "npm i ollama",
      "ESM + CJS",
      "Promise & stream interfaces",
      "zero dependencies"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain adapter providing Ollama LLM, embedder, and chat wrappers for quick integration into LangChain chains.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "pip install langchain-ollama",
      "chat model",
      "embeddings",
      "tool calling"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI (formerly Ollama-WebUI) that turns any local Ollama instance into a ChatGPT-like interface.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "Docker one-liner",
      "multi-user, RBAC",
      "RAG uploads",
      "OpenAI-compat endpoint"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Unofficial Java/Kotlin client for Ollama with fluent builder APIs, streaming, and Android compatibility.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Maven Central",
      "async & sync",
      "Android tested",
      "Kotlin extensions"
    ]
  },
  {
    "title": "ollama-rs",
    "url": "https://github.com/pepperoni21/ollama-rs",
    "summary": "Community Rust crate providing strongly-typed async wrappers over Ollama\u2019s REST endpoints.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "crates.io",
      "tokio/async",
      "serde types",
      "streaming chat"
    ]
  },
  {
    "title": "ollama-chat",
    "url": "https://github.com/jmorganca/ollama-chat",
    "summary": "Minimal reference chat app (Next.js + Vercel) showing how to stream Ollama responses to a web front-end.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "Next.js",
      "/api/chat proxy",
      "Server-Sent Events",
      "Dockerfile included"
    ]
  },
  {
    "title": "ollama-gui",
    "url": "https://github.com/ollama/ollama-gui",
    "summary": "Lightweight PyQt5 desktop GUI to select models, set parameters, and chat with Ollama without a browser.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "cross-platform binary",
      "model manager",
      "parameter sliders",
      "markdown rendering"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/ollama",
    "summary": "Haystack 2.x integration offering OllamaGenerator and OllamaChatGenerator nodes for retrieval-augmented pipelines.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip install ollama-haystack",
      "RAG ready",
      "streaming",
      "custom prompt templates"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Experimental VS Code extension that routes GitHub Copilot chat requests to a local Ollama model.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "VS Code marketplace",
      "configurable model",
      "inline chat",
      "status-bar toggle"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sugarforever/ollama-cli",
    "summary": "Community-enhanced CLI with conversation history, prompt templates, and role-playing shortcuts.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "conversation save/load",
      "/ personas",
      "syntax highlight",
      "pipx install"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Official Helm chart for deploying Ollama server and models onto Kubernetes clusters with GPU support.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "Helm repo",
      "GPU node selector",
      "pvc model cache",
      "autoscaling HPA"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official container images (cuda/rocm/cpu) and compose examples for running Ollama in Docker & Podman.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "official images",
      "CUDA 12",
      "ROCm 6",
      "rootless mode"
    ]
  },
  {
    "title": "ollama-nvim",
    "url": "https://github.com/nomnivore/ollama.nvim",
    "summary": "Neovim plugin that adds :Ollama prompt, code-gen, and chat windows using floating terminals or buffers.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "lazy.nvim support",
      "streaming answers",
      "custom prompts",
      "keymaps"
    ]
  },
  {
    "title": "ollama-elixir",
    "url": "https://github.com/gergo/elixir-ollama",
    "summary": "Unofficial Elixir wrapper with Finch HTTP client and GenServer-backed process pools for chat & embed.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Hex.pm package",
      "OTP supervision",
      "streaming",
      "configurable endpoints"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/gboudreau/ollama-rb",
    "summary": "Community Ruby gem exposing Ollama REST APIs with ActiveModel-like interfaces and Faraday underneath.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "RubyGems",
      "async HTTP",
      "model management",
      "chat blocks"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama-discord/ollama-bot",
    "summary": "Self-hosted Discord bot that brings local LLM chat, image-to-text, and moderation tools to any server.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "slash commands",
      "threaded chats",
      "mod log",
      "Docker compose"
    ]
  },
  {
    "title": "ollama-django",
    "url": "https://github.com/taranjeet/ollama-django",
    "summary": "Django app + REST API example showing how to integrate Ollama models into a Python web backend.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Django REST",
      "streaming HttpResponse",
      "model cache",
      "admin panel"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/ollama-csharp",
    "summary": "Community .NET SDK and Blazor sample demonstrating chat and image reasoning with a local Ollama server.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "NuGet package",
      ".NET 8",
      "Blazor WASM",
      "Vision support"
    ]
  }
]