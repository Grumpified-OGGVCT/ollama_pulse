[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Ollama repo: a lightweight, extensible framework for running Llama 2, Mistral, Gemma and other LLMs locally with a simple CLI and REST API.",
    "source": "github",
    "date": "2024-04-15",
    "highlights": [
      "CLI",
      "REST API",
      "macOS/Linux/Windows",
      "Docker image",
      "Model library"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama. Chat, generate, embed, pull and manage models with a few lines of code.",
    "source": "github",
    "date": "2024-04-10",
    "highlights": [
      "pip install ollama",
      "sync/async",
      "embeddings",
      "streaming"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node and browsers. Same API surface as the Python library.",
    "source": "github",
    "date": "2024-04-08",
    "highlights": [
      "npm i ollama",
      "TypeScript",
      "streaming",
      "browser/Node"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain integration via Ollama LLM wrapper. Drop-in replacement for OpenAI in chains, agents, RAG, tool-calling, etc.",
    "source": "blog",
    "date": "2024-03-25",
    "highlights": [
      "pip install langchain-ollama",
      "tool-calling",
      "RAG",
      "agents"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Full-featured ChatGPT-style web UI for Ollama. Multi-model chats, code highlighting, RAG, admin panel, dark mode.",
    "source": "github",
    "date": "2024-04-12",
    "highlights": [
      "Docker",
      "file upload",
      "RAG",
      "admin panel",
      "PWA"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama. Fluent builder API, streaming, embeddings, custom options.",
    "source": "github",
    "date": "2024-04-05",
    "highlights": [
      "Maven Central",
      "Kotlin DSL",
      "streaming",
      "embeddings"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/jmorganca/ollama-rb",
    "summary": "Community-maintained Ruby gem for Ollama. Supports chat, generate, pull, delete and listing.",
    "source": "github",
    "date": "2024-03-30",
    "highlights": [
      "gem install ollama",
      "Ruby 3.x",
      "streaming"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/salty-flower/ollama-cli",
    "summary": "Interactive TUI for Ollama written in Rust. Browse, chat, and manage models inside the terminal.",
    "source": "github",
    "date": "2024-04-01",
    "highlights": [
      "Rust",
      "TUI",
      "keyboard shortcuts",
      "conversation history"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "VS Code extension that turns any Ollama model into a GitHub Copilot replacement. Inline suggestions, chat panel.",
    "source": "github",
    "date": "2024-04-09",
    "highlights": [
      "VS Code",
      "inline completions",
      "chat panel",
      "configurable model"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for running Ollama on Kubernetes with GPU support and horizontal scaling.",
    "source": "github",
    "date": "2024-03-28",
    "highlights": [
      "Helm",
      "GPU nodes",
      "autoscaling",
      "persistent storage"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official container images (CPU & GPU) published to Docker Hub. One-liner to run Llama 2 in a container.",
    "source": "github",
    "date": "2024-04-14",
    "highlights": [
      "docker run ollama/ollama",
      "CUDA",
      "rocm",
      "rootless"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama-haystack/ollama-haystack",
    "summary": "Haystack integration for Ollama. Use local models in pipelines for QA, summarization, semantic search.",
    "source": "github",
    "date": "2024-03-22",
    "highlights": [
      "pip install ollama-haystack",
      "pipelines",
      "embedders",
      "rankers"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/ollama-csharp",
    "summary": ".NET client for Ollama with async streaming, DI-friendly, built-in Polly retries.",
    "source": "github",
    "date": "2024-04-03",
    "highlights": [
      "NuGet",
      "NET 8",
      "IAsyncEnumerable",
      "DI",
      "retries"
    ]
  },
  {
    "title": "ollama-spring-boot-starter",
    "url": "https://github.com/ollama4j/ollama-spring-boot-starter",
    "summary": "Spring Boot auto-configuration for Ollama. Inject Ollama4j client with YAML config, actuator endpoints.",
    "source": "github",
    "date": "2024-03-27",
    "highlights": [
      "Spring Boot",
      "YAML config",
      "actuator",
      "auto-configuration"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ivanfioravivi/ollama-rag",
    "summary": "Minimal RAG template using Ollama + Chroma + LangChain. Load PDFs, ask questions offline.",
    "source": "github",
    "date": "2024-03-31",
    "highlights": [
      "RAG",
      "Chroma",
      "PDF ingestion",
      "offline",
      "Docker Compose"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/jmorganca/ollama-discord",
    "summary": "Discord bot that streams Ollama responses into channels. Slash commands, role-based model access.",
    "source": "github",
    "date": "2024-04-02",
    "highlights": [
      "Discord.py",
      "slash commands",
      "streaming",
      "role ACL"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/NixOS/nixpkgs/blob/master/pkgs/tools/misc/ollama",
    "summary": "Nix derivation for Ollama. Declarative installs on NixOS with GPU drivers and model caching.",
    "source": "github",
    "date": "2024-04-11",
    "highlights": [
      "nixpkgs",
      "declarative",
      "GPU",
      "systemd service"
    ]
  },
  {
    "title": "ollama-distroless",
    "url": "https://github.com/ollama-distroless/ollama-distroless",
    "summary": "Security-hardened distroless container for Ollama. Minimal attack surface, read-only rootfs, non-root user.",
    "source": "github",
    "date": "2024-03-29",
    "highlights": [
      "distroless",
      "non-root",
      "read-only",
      "CVE scanning"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama-obsidian/ollama-obsidian",
    "summary": "Obsidian plugin that adds an Ollama sidebar for summarizing notes, brainstorming, and Q&A.",
    "source": "github",
    "date": "2024-04-06",
    "highlights": [
      "Obsidian",
      "sidebar",
      "summarize",
      "local LLM"
    ]
  },
  {
    "title": "ollama-gpt-prompt",
    "url": "https://www.npmjs.com/package/ollama-gpt-prompt",
    "summary": "NPM utility that wraps Ollama to act like OpenAI\u2019s CLI prompt. Zero-config drop-in for npm scripts.",
    "source": "npm",
    "date": "2024-03-26",
    "highlights": [
      "npx ollama-gpt-prompt",
      "zero-config",
      "OpenAI compatible"
    ]
  }
]