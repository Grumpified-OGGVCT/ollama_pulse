[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: get up and running with Llama 3, Mistral, Gemma and other large language models locally.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-contained LLM runner",
      "macOS/Linux/Windows binaries",
      "Docker image",
      "built-in model registry"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama: chat, generate, embed, pull and manage models with a few lines of code.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pip install ollama",
      "sync/async APIs",
      "streaming support",
      "type hints"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node and browsers; same API surface as the Python SDK.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "npm i ollama",
      "ESM & CommonJS",
      "browser-compatible",
      "Promise/async iterators"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/ollama",
    "summary": "LangChain integration package exposing Ollama models as standard LangChain LLMs and embedders.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install langchain-ollama",
      "chat & embed endpoints",
      "native streaming",
      "callback support"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (chat UI, model management, multi-user, OpenAI-compatible endpoint).",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Docker one-liner",
      "dark/light themes",
      "code highlighting",
      "conversation history"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/llama-assistant/ollama-cli",
    "summary": "Interactive terminal UI for chatting with Ollama models, written in Go; supports markdown and syntax highlighting.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "TUI with bubbles",
      "multi-model sessions",
      "command palette",
      "fuzzy model search"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "VS Code extension bringing local Ollama models into GitHub Copilot-style inline suggestions and chat sidebar.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "inline completions",
      "chat panel",
      "configurable model per task",
      "status-bar indicator"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU support and horizontal scaling.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "GPU node selector",
      "persistent volume for models",
      "ingress & serviceMonitor",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration letting you use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "pip install ollama-haystack",
      "Generator & Embedder components",
      "streaming support"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/JVM client for Ollama (Kotlin friendly) exposing synchronous, reactive and streaming APIs.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Maven Central",
      "Spring Boot starter",
      "Kotlin coroutines",
      "model management"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Official Ruby gem for Ollama: simple client covering chat, generate, pull and embed endpoints.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "gem install ollama",
      "Faraday backend",
      "streaming blocks",
      "RSpec test suite"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/ollama/ollama-csharp",
    "summary": "Community-maintained C#/.NET client for Ollama with async streaming and dependency injection helpers.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "NuGet package",
      ".NET 6+ target",
      "IAsyncEnumerable streaming",
      "DI extensions"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/valiantlynx/ollama-docker-compose",
    "summary": "Collection of ready-made docker-compose stacks bundling Ollama with Open-WebUI, LiteLLM proxy, and GPU flags.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "GPU runtime toggle",
      "nginx reverse proxy",
      "persistent volumes",
      "env-file templates"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hintergrund-obsidian/ollama-obsidian",
    "summary": "Obsidian plugin that adds a local AI assistant sidebar powered by any Ollama model.",
    "source": "github",
    "date": "2024-04-24",
    "highlights": [
      "vault-aware context",
      "custom prompts",
      "model switcher",
      "offline operation"
    ]
  },
  {
    "title": "ollama-logseq",
    "url": "https://github.com/omagdy-logseq/ollama-logseq",
    "summary": "Logseq plugin for invoking Ollama models to summarise blocks, brainstorm or translate inside your knowledge base.",
    "source": "github",
    "date": "2024-04-22",
    "highlights": [
      "slash command",
      "selected-block context",
      "configurable system prompt",
      "streaming insert"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/ollama-nix/ollama-nix",
    "summary": "Nix flake providing reproducible builds of Ollama server, CLI and WebUI for macOS and Linux.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "CUDA & ROCm variants",
      "module for NixOS",
      "declarative model cache",
      "binary cache"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Rust crate offering strongly typed async client for the Ollama REST API with Tokio and SSE streaming.",
    "source": "github",
    "date": "2024-04-23",
    "highlights": [
      "crates.io release",
      "serde schemas",
      "tokio streams",
      "examples folder"
    ]
  },
  {
    "title": "ollama-vscode",
    "url": "https://github.com/lfom/ollama-vscode",
    "summary": "Lightweight VS Code extension that adds Ollama chat and code-generation commands to the command palette.",
    "source": "github",
    "date": "2024-04-20",
    "highlights": [
      "no external deps",
      "configurable prompt templates",
      "status-bar quick pick",
      "multi-model support"
    ]
  },
  {
    "title": "ollama-litellm",
    "url": "https://github.com/BerriAI/litellm/tree/main/litellm/llms/ollama",
    "summary": "LiteLLM adapter letting you call Ollama models through the same OpenAI-style interface as 100+ other providers.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "OpenAI compatibility",
      "automatic retry",
      "budget tracking",
      "proxy server mode"
    ]
  },
  {
    "title": "ollama-huggingface",
    "url": "https://github.com/ollama-huggingface/ollama-huggingface",
    "summary": "Community project that syncs Hugging Face model repos into Ollama\u2019s model cache with automatic Modelfile generation.",
    "source": "github",
    "date": "2024-04-21",
    "highlights": [
      "HF hub integration",
      "auto-quantisation",
      "progressive download",
      "CLI & library"
    ]
  }
]