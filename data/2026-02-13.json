[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and Python/JavaScript libraries to pull and run Llama 2, Mistral, Gemma, and other large language models locally with GPU/CPU acceleration.",
    "source": "github",
    "date": "2024-04-18",
    "highlights": [
      "self-hosted inference",
      "Docker image",
      "macOS/Linux/Windows",
      "OpenAI-compatible API"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official TypeScript/JavaScript client for Ollama; chat, generate, pull, push, list, copy, delete models with async/await and streaming support.",
    "source": "github",
    "date": "2024-04-18",
    "highlights": [
      "npm install ollama",
      "browser & Node",
      "streaming responses",
      "Promise-based API"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama; same generate/chat/embed API surface as the JS library plus sync/async iterators and native Pydantic helpers.",
    "source": "github",
    "date": "2024-04-18",
    "highlights": [
      "pip install ollama",
      "asyncio support",
      "embeddings endpoint",
      "Pydantic models"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/llms/ollama",
    "summary": "LangChain integration that wraps Ollama\u2019s REST API so any LangChain chain can run local Llama/Mistral models with few lines of code.",
    "source": "github",
    "date": "2024-04-10",
    "highlights": [
      "pip install langchain-community",
      "streaming",
      "callback support",
      "chat & instruct modes"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Clean open-source ChatGPT-style web UI for Ollama; markdown, code highlighting, model management, multi-user auth, Docker one-liner deploy.",
    "source": "github",
    "date": "2024-04-15",
    "highlights": [
      "Docker compose",
      "dark/light themes",
      "multi-model chats",
      "import/export"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/mikegchambers/ollama-discord",
    "summary": "Lightweight bot that exposes Ollama models as slash commands in Discord; configurable system prompts, per-user rate limits.",
    "source": "github",
    "date": "2024-03-28",
    "highlights": [
      "Discord.py",
      "slash commands",
      "streaming replies",
      "env-based config"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/salty-flower/ollama-cli",
    "summary": "Terminal UI (TUI) for chatting with Ollama models using textual; supports conversation history, syntax highlighting, model switching.",
    "source": "github",
    "date": "2024-04-05",
    "highlights": [
      "textual TUI",
      "conversation logs",
      "vim keys",
      "pipx install"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/jmorganca/ollama-copilot",
    "summary": "Experimental GitHub Copilot plugin that routes code-completion requests to a local Ollama model instead of OpenAI.",
    "source": "github",
    "date": "2024-02-20",
    "highlights": [
      "VS Code extension",
      "local completions",
      "privacy first",
      "FIM templates"
    ]
  },
  {
    "title": "ollama-rs",
    "url": "https://github.com/pepperoni21/ollama-rs",
    "summary": "Community-built Rust crate providing typed async bindings to Ollama\u2019s REST API with tokio and serde.",
    "source": "github",
    "date": "2024-04-12",
    "highlights": [
      "cargo add ollama-rs",
      "async/await",
      "strong types",
      "examples"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin SDK for Ollama; supports generate, chat, embeddings, model management, and reactive streaming with Project Reactor.",
    "source": "github",
    "date": "2024-04-08",
    "highlights": [
      "Maven Central",
      "Spring Boot starter",
      "Kotlin coroutines",
      "streaming"
    ]
  },
  {
    "title": "OllamaKit",
    "url": "https://github.com/kevinhermawan/OllamaKit",
    "summary": "Swift package for iOS/macOS apps to integrate local Ollama models via async/await and Combine publishers.",
    "source": "github",
    "date": "2024-04-14",
    "highlights": [
      "Swift Package Manager",
      "Combine",
      "Codable models",
      "example iOS app"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama-haystack/ollama-haystack",
    "summary": "Haystack integration by deepset letting you use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-03-30",
    "highlights": [
      "pip install ollama-haystack",
      "RAG pipelines",
      "embeddings",
      "generator node"
    ]
  },
  {
    "title": "ollama-camunda",
    "url": "https://github.com/camunda-community-hub/ollama-camunda",
    "summary": "Community Camunda connector that invokes Ollama models inside BPMN processes for local LLM tasks without leaving Camunda.",
    "source": "github",
    "date": "2024-04-02",
    "highlights": [
      "Camunda 8",
      "outbound connector",
      "BPMN",
      "local AI tasks"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU node selection, PVC, and optional web-ui sidecar.",
    "source": "github",
    "date": "2024-04-16",
    "highlights": [
      "Helm 3",
      "GPU support",
      "Ingress",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-nifi",
    "url": "https://github.com/apache/nifi/tree/main/nifi-nar-bundles/ollama-bundle",
    "summary": "Apache NiFi processors to prompt Ollama models in data flows; supports generate, chat, and streaming for low-code LLM pipelines.",
    "source": "github",
    "date": "2024-04-11",
    "highlights": [
      "NiFi 1.x",
      "flow-files",
      "streaming",
      "no-code"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/glangchain/ollama-rag",
    "summary": "Minimal RAG template using Ollama embeddings + Chroma + LangChain to chat with local PDFs entirely offline.",
    "source": "github",
    "date": "2024-04-09",
    "highlights": [
      "ChromaDB",
      "local PDF",
      "offline",
      "Gradio UI"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/seriousm4x/ollama-slack",
    "summary": "Slack Bolt app that adds /ollama slash command for chatting with any local model; supports streaming, threads, and ephemeral responses.",
    "source": "github",
    "date": "2024-04-07",
    "highlights": [
      "Bolt JS",
      "streaming",
      "threads",
      "env config"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hintergrund-obsidian/ollama-obsidian",
    "summary": "Obsidian plugin that lets you run local LLM prompts inside notes for summarization, translation, or brainstorming without cloud APIs.",
    "source": "github",
    "date": "2024-03-25",
    "highlights": [
      "Obsidian plugin",
      "command palette",
      "templates",
      "offline"
    ]
  },
  {
    "title": "ollama-npm",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "Official npm package providing JavaScript/TypeScript bindings for the Ollama local inference server.",
    "source": "github",
    "date": "2024-04-18",
    "highlights": [
      "npm install ollama",
      "ESM & CJS",
      "zero deps",
      "100% TypeScript"
    ]
  },
  {
    "title": "ollama-pypi",
    "url": "https://pypi.org/project/ollama/",
    "summary": "Official PyPI package providing Python bindings and CLI helpers for interacting with the Ollama server.",
    "source": "github",
    "date": "2024-04-18",
    "highlights": [
      "pip install ollama",
      "sync & async",
      "embeddings",
      "CLI helper"
    ]
  }
]