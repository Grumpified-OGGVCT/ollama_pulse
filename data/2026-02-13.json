[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Ollama\u2019s official CLI and server that lets you pull, run, and manage large language models locally (Llama 2, Mistral, Gemma, etc.) with a single command.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "self-contained binary",
      "Docker-like model hub",
      "OpenAI-compatible REST API"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama: chat, generate, embed, pull, and manage models with a few lines of code.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "sync & async clients",
      "streaming support",
      "PyPI: ollama"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node and browsers; chat, generate, and pull models via Ollama\u2019s REST API.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "zero dependencies",
      "ESM & CommonJS",
      "npm: ollama"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration that wraps Ollama models as LLMs and embeddings for chains, agents, and RAG pipelines.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip: langchain-ollama",
      "streaming",
      "native embed support"
    ]
  },
  {
    "title": "ollama-webui (Ollama-WebUI)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI (now \u201cOpen WebUI\u201d) that talks to your local Ollama instance; supports RAG, multi-user, and plugins.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "Docker image",
      "file upload & vector DB",
      "role-based access"
    ]
  },
  {
    "title": "ollama-cli-copilot",
    "url": "https://github.com/jmorganca/ollama-cli-copilot",
    "summary": "Lightweight CLI copilot that uses Ollama models for inline code suggestions and shell commands.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "fzf integration",
      "configurable prompts",
      "no API keys"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Minimal retrieval-augmented-generation example that combines Ollama embeddings + Chroma to chat with local documents.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "pure Python",
      "Chroma vector store",
      ".pdf & .txt ingestion"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/ripgrim/ollama-discord-bot",
    "summary": "Self-hostable Discord bot that streams answers from any Ollama model in chat channels.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "slash commands",
      "streaming replies",
      "env-file config"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama with fluent API, streaming, and async support; Maven Central artifact.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "Java 11+",
      "Kotlin coroutines",
      "Maven: ollama4j"
    ]
  },
  {
    "title": "ollama-copilot.vim",
    "url": "https://github.com/metatexx/ollama-copilot.vim",
    "summary": "Vim plugin that brings Ollama-powered code completion and inline chat to Neovim.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Lua config",
      "async jobs",
      "select models per filetype"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart to deploy Ollama on Kubernetes with GPU node selection and autoscaling.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "GPU support",
      "configurable models list",
      "PVC caching"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/ollama",
    "summary": "Haystack 2.0 integration providing OllamaGenerator and OllamaEmbedder for building LLM pipelines.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "pip: haystack-ai[ollama]",
      "pipeline YAML support",
      "streaming"
    ]
  },
  {
    "title": "ollama-curl-examples",
    "url": "https://github.com/ollama/ollama/tree/main/examples",
    "summary": "Official collection of curl snippets and language examples showing how to call the Ollama REST API.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "generate & chat",
      "JSON mode",
      "streaming tokens"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/technosophos/ollama-slack-bot",
    "summary": "Simple Slack bot server that forwards channel messages to Ollama and streams responses back.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Socket-Mode",
      "Bolt framework",
      "Dockerfile included"
    ]
  },
  {
    "title": "ollama-cpp-pybind",
    "url": "https://github.com/abetlen/ollama-cpp-pybind",
    "summary": "Experimental pybind11 wrapper around Ollama\u2019s C++ core for ultra-low-latency local inference in Python.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "zero-copy tensors",
      "pip installable",
      "GPU fallback"
    ]
  },
  {
    "title": "ollama-streamlit-chat",
    "url": "https://github.com/SauravMaheshkar/ollama-streamlit",
    "summary": "Streamlit chat UI that connects to Ollama via the Python SDK and supports markdown, code blocks, and LaTeX.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "session memory",
      "themes",
      "requirements.txt only"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hinterdup/ollama-obsidian",
    "summary": "Obsidian plugin that lets you query local Ollama models from within notes for summarization and Q&A.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "command palette",
      "templater support",
      "offline"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Community Rust crate providing strongly-typed async client for Ollama\u2019s REST API.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "tokio based",
      "crates.io: ollama-rs",
      "Serde models"
    ]
  },
  {
    "title": "ollama-django",
    "url": "https://github.com/djunh1/ollama-django",
    "summary": "Reusable Django app that exposes Ollama models as async HTTP endpoints with built-in rate-limiting and admin UI.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "DRF serializers",
      "Redis cache",
      "admin actions"
    ]
  },
  {
    "title": "ollama-elixir",
    "url": "https://github.com/thmsmlr/ollama-elixir",
    "summary": "Elixir library wrapping Ollama for real-time chat and embeddings inside Phoenix/LiveView applications.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Hex.pm: ollama",
      "GenServer pooling",
      "LiveView hooks"
    ]
  }
]