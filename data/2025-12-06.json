[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama CLI and server that lets you run Llama 2, Mistral, Gemma, and other large language models locally with a simple command-line interface.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-contained binary",
      "Docker image",
      "macOS/Linux/Windows",
      "OpenAI-compatible API"
    ]
  },
  {
    "title": "ollama/ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama, providing a simple async/sync API to chat with any model served by the local Ollama instance.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "async support",
      "streaming responses",
      "Pydantic models",
      "PyPI package"
    ]
  },
  {
    "title": "ollama/ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama, usable in Node.js and browsers, with full TypeScript definitions and streaming support.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "npm package",
      "browser & Node.js",
      "TypeScript",
      "streaming chat"
    ]
  },
  {
    "title": "jmorganca/ollama",
    "url": "https://pypi.org/project/ollama/",
    "summary": "PyPI landing for the official ollama-python package; install via pip to integrate Ollama models into any Python script or Jupyter notebook.",
    "source": "blog",
    "date": "2024-05-07",
    "highlights": [
      "pip install ollama",
      "zero config",
      "Jupyter ready",
      "chat & embed endpoints"
    ]
  },
  {
    "title": "ollama-webui/ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (formerly Ollama-GUI) with chat history, model management, multi-user support, and OpenAI-style API compatibility.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Docker image",
      "dark/light themes",
      "import/export chats",
      "admin panel"
    ]
  },
  {
    "title": "otobrglez/ollama-cli",
    "url": "https://github.com/otobrglez/ollama-cli",
    "summary": "Rust-based interactive CLI wrapper around Ollama with syntax highlighting, conversation history, and prompt templates.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Rust binary",
      "REPL mode",
      "prompt templates",
      "conversation save/load"
    ]
  },
  {
    "title": "richawo/ollama-instructor",
    "url": "https://github.com/richawo/ollama-instructor",
    "summary": "Lightweight TypeScript library that adds OpenAI-style function-calling / tool-use to any Ollama model without fine-tuning.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "npm package",
      "function calling",
      "Zod schemas",
      "streaming support"
    ]
  },
  {
    "title": "kevinwatt/ollama-copilot",
    "url": "https://github.com/kevinwatt/ollama-copilot",
    "summary": "VS Code extension that turns any Ollama model into a GitHub Copilot replacement with inline autocomplete and chat sidebar.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "VS Code marketplace",
      "inline completions",
      "chat sidebar",
      "custom models"
    ]
  },
  {
    "title": "ollama-models/ollama-models",
    "url": "https://github.com/ollama-models/ollama-models",
    "summary": "Community-curated list of custom Modelfiles for quantized Llama, Mistral, CodeLlama, and vision models ready to import into Ollama.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Modelfile recipes",
      "GGUF links",
      "vision models",
      "quantization tips"
    ]
  },
  {
    "title": "sammcj/ollama-chat",
    "url": "https://github.com/sammcj/ollama-chat",
    "summary": "Minimal React chat app bootstrapped with Vite that connects to a local Ollama server via the OpenAI-compatible endpoint.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "React + Vite",
      "streaming chat",
      "Dockerfile",
      "OpenAI endpoint"
    ]
  },
  {
    "title": "mneedham/ollama-sdm",
    "url": "https://github.com/mneedham/ollama-sdm",
    "summary": "Neo4j plugin that lets you call Ollama models from Cypher queries for knowledge-graph-enhanced RAG and entity extraction.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Neo4j SDM",
      "Cypher procedures",
      "RAG workflows",
      "entity extraction"
    ]
  },
  {
    "title": "langchain-ai/langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "Official LangChain integration package providing LLM, chat, and embedding wrappers around Ollama for rapid RAG chain prototyping.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install langchain-ollama",
      "RAG templates",
      "embeddings",
      "streaming"
    ]
  },
  {
    "title": "r2d2-go/ollama-discord",
    "url": "https://github.com/r2d2-go/ollama-discord",
    "summary": "Self-hostable Discord bot that brings local Ollama models into any server with slash commands, thread support, and admin whitelisting.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "Discord.py",
      "slash commands",
      "thread chats",
      "admin whitelist"
    ]
  },
  {
    "title": "teddybear082/ollama-streamlit",
    "url": "https://github.com/teddybear082/ollama-streamlit",
    "summary": "Streamlit chat UI for Ollama featuring multi-model sidebar, token-speed metrics, and one-click Docker deployment for data-scientists.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Streamlit",
      "token metrics",
      "Docker",
      "multi-model"
    ]
  },
  {
    "title": "ggerganov/llama.cpp #ollama",
    "url": "https://github.com/ggerganov/llama.cpp/discussions/categories/ollama",
    "summary": "Active GitHub Discussion category where llama.cpp and Ollama communities share GGUF quantization tips, benchmarks, and feature requests.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "GGUF quantization",
      "benchmarks",
      "feature requests",
      "cross-community"
    ]
  },
  {
    "title": "Reddit r/ollama",
    "url": "https://www.reddit.com/r/ollama/",
    "summary": "Subreddit dedicated to Ollama with setup guides, model recommendations, troubleshooting, and showcase of community projects and integrations.",
    "source": "reddit",
    "date": "2024-05-10",
    "highlights": [
      "setup guides",
      "model reviews",
      "troubleshooting",
      "showcase"
    ]
  },
  {
    "title": "Hacker News: Show HN: Ollama \u2013 Run Llama 2 locally",
    "url": "https://news.ycombinator.com/item?id=37066938",
    "summary": "Launch discussion on Hacker News covering Ollama\u2019s design choices, performance on Apple Silicon, and comparisons to llama.cpp and text-generation-webui.",
    "source": "hackernews",
    "date": "2023-08-11",
    "highlights": [
      "Apple Silicon perf",
      "design choices",
      "vs llama.cpp",
      "community tips"
    ]
  },
  {
    "title": "npm: ollama",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "Official npm package for ollama-js; zero-dependency TypeScript client downloaded 50k+ times/month for browser and Node.js integrations.",
    "source": "blog",
    "date": "2024-05-08",
    "highlights": [
      "50k weekly downloads",
      "zero deps",
      "TypeScript",
      "browser + Node.js"
    ]
  }
]