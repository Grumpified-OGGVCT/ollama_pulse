[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and server for running large language models locally (Llama 2, Mistral, Gemma, etc.) with a simple pull-run workflow.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "self-contained binary",
      "model library",
      "OpenAI-compatible API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK that wraps the Ollama REST API for chatting, embedding, and model management.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip install ollama",
      "sync/async clients",
      "streaming responses",
      "embeddings support"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript SDK for Node and browsers to interact with the Ollama server.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "npm install ollama",
      "Promise & stream APIs",
      "type definitions",
      "browser compatible"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://pypi.org/project/langchain-ollama/",
    "summary": "LangChain integration package exposing Ollama models as standard LangChain LLMs and embedders.",
    "source": "pypi",
    "date": "2024-05-12",
    "highlights": [
      "pip install langchain-ollama",
      "chat & embed interfaces",
      "callback support",
      "community provider"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Open-source ChatGPT-style web interface for Ollama featuring folder organization, code highlighting, and multi-model chats.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "docker-compose ready",
      "dark/light themes",
      "markdown & code blocks",
      "role-based chats"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/saltytears/ollama-cli",
    "summary": "Enhanced interactive CLI wrapper around ollama with persistent chat sessions and command history.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "readline support",
      "session save/load",
      "syntax highlighting",
      "pip installable"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "VS Code extension that brings local Ollama models into GitHub Copilot-style inline suggestions.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "inline completions",
      "configurable model",
      "status-bar toggle",
      "open-source"
    ]
  },
  {
    "title": "ollama-chat",
    "url": "https://github.com/jmorganca/ollama-chat",
    "summary": "Minimal React chat app bootstrapped with Vite that streams responses from a local Ollama server.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Vite + React",
      "server-sent events",
      "Dockerfile included",
      "customizable prompts"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration by the community allowing Ollama models to be used as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "pip install ollama-haystack",
      "generator & embedder nodes",
      "streaming support",
      "Haystack 2.x ready"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Minimal Retrieval-Augmented-Generation example using Ollama embeddings + Chroma vector store + LlamaIndex.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "LlamaIndex integration",
      "Chroma persistence",
      "PDF ingestion script",
      "Gradio UI"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama-discord/ollama-discord",
    "summary": "Self-hostable Discord bot that streams Ollama model replies into any channel with slash commands.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "slash commands",
      "streaming replies",
      "/model switch",
      "Docker image"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otw/ollama-helm",
    "summary": "Helm chart to deploy Ollama server and models onto Kubernetes clusters with GPU node selection.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "GPU node affinity",
      "model pre-pull job",
      "service monitor",
      "configurable replicas"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Java/JVM client library for Ollama offering synchronous, asynchronous, and reactive (RxJava) APIs.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Maven Central",
      "RxJava support",
      "model management API",
      "Kotlin-friendly"
    ]
  },
  {
    "title": "ollama.nvim",
    "url": "https://github.com/nanoteer/ollama.nvim",
    "summary": "Neovim plugin that sends selected text or buffers to Ollama and inserts the model response inline.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "visual selection",
      "key-mappable",
      "split or replace",
      "Lua config"
    ]
  },
  {
    "title": "ollama-streamlit",
    "url": "https://github.com/randomor/ollama-streamlit",
    "summary": "Streamlit chat UI for Ollama with conversation memory, token metrics, and model selector sidebar.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "session memory",
      "token counters",
      "sidebar model picker",
      "pip install streamlit-ollama"
    ]
  }
]