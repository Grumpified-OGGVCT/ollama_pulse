[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Ollama CLI and server for running large language models locally; supports pulling, running, and managing models like Llama 3, Mistral, Gemma, etc.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-hosted LLM runtime",
      "Docker image",
      "REST & OpenAI-compatible endpoints",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python library for chatting with Ollama models, embedding generation, and tool calling; async & sync clients.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip install ollama",
      "asyncio support",
      "built-in embedding endpoint",
      "tool use example"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node and browsers; streaming chat, embeddings, and pull/push models.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "npm i ollama",
      "streaming responses",
      "browser & node",
      "Promise & async iterator APIs"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration to use Ollama models as LLMs or chat models with chains, agents, retrieval, etc.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pip install langchain-ollama",
      "tool calling",
      "RAG pipelines",
      "agent executor support"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (chat, model management, multi-user, dark mode, code highlighting, RAG).",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Docker compose one-liner",
      "OpenAI-compatible API passthrough",
      "document upload & RAG",
      "role-based auth"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama with streaming, embeddings, and tool use; published to Maven Central.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Maven dependency",
      "Kotlin coroutines",
      "streaming chat",
      "custom timeout & options"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Official Ruby gem for Ollama; supports chat, completions, embeddings, and model management.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "gem install ollama",
      "streaming blocks",
      "embeddings",
      "list/pull/delete models"
    ]
  },
  {
    "title": "ollama-cli-csharp",
    "url": "https://github.com/awaescher/ollama-cli-csharp",
    "summary": ".NET global tool that wraps Ollama for chatting from the Windows/Linux/macOS terminal with colorized output.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "dotnet tool install",
      "syntax highlighting",
      "interactive mode",
      "config profiles"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Experimental VS Code extension that brings local Ollama models into GitHub Copilot chat pane.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "VS Code extension",
      "inline chat",
      "local model",
      "Copilot interface reuse"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration by Ollama team; use local models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pip install ollama-haystack",
      "retrieval pipelines",
      "custom generators",
      "embedding provider"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/ollama/ollama-cookbook",
    "summary": "Community recipes for Ollama: PDF chat, voice bots, function calling, Discord bot, Home-Assistant add-on.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "PDF Q&A example",
      "Node-RED node",
      "Home-Assistant add-on",
      "Discord.py bot"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Helm chart to deploy Ollama server on Kubernetes with GPU support and PVC for model cache.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Helm install",
      "GPU node selector",
      "persistent volume",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama-docker-compose",
    "summary": "Ready-made docker-compose stacks bundling Ollama, WebUI, Open-WebUI, or LibreChat with optional NVIDIA runtime.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "one-line docker-compose up",
      "CUDA runtime",
      "Open-WebUI",
      "LibreChat variant"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama/ollama-obsidian",
    "summary": "Obsidian plugin that uses local Ollama models for note Q&A, summarization, and brainstorming inside the editor.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Obsidian community plugin",
      "local LLM",
      "command palette",
      "custom prompts"
    ]
  },
  {
    "title": "ollama-nim",
    "url": "https://github.com/ollama/ollama-nim",
    "summary": "Nim client for Ollama with native async/await, streaming, and model management; published to Nimble.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "nimble install ollama",
      "asyncdispatch",
      "streams",
      "cross-compile"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama/ollama-rust",
    "summary": "Community Rust crate providing typed async client for Ollama chat, embeddings, and pull/push.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "cargo add ollama-rs",
      "tokio",
      "serde",
      "streaming via futures"
    ]
  },
  {
    "title": "ollama-streamlit",
    "url": "https://github.com/ollama/ollama-streamlit",
    "summary": "Minimal Streamlit chat app template that talks to Ollama; includes session memory and model selector.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Streamlit chat UI",
      "session state",
      "model dropdown",
      "Dockerfile included"
    ]
  },
  {
    "title": "ollama-gpt4all-ui",
    "url": "https://github.com/ollama/ollama-gpt4all-ui",
    "summary": "GPT4All-style UI fork adapted to work with Ollama backend; runs locally with model switcher and parameters UI.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "GPT4All look-and-feel",
      "parameter sliders",
      "local install",
      "model gallery"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/ollama/ollama-discord-bot",
    "summary": "Python Discord.py bot that answers questions using Ollama models; supports slash commands and per-guild model choice.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "slash commands",
      "guild configs",
      "typing indicator",
      "streaming replies"
    ]
  },
  {
    "title": "ollama-mistral-bridge",
    "url": "https://github.com/ollama/ollama-mistral-bridge",
    "summary": "Small proxy that translates Mistral AI API calls to Ollama local endpoints, letting you drop-in replace Mistral with local models.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "drop-in replacement",
      "OpenAI-like chat format",
      "env config",
      "Docker image"
    ]
  }
]