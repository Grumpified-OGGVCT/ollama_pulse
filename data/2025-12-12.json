[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Ollama CLI and server for running Llama 2, Mistral, Gemma and other large language models locally with one command.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "CLI",
      "Docker",
      "macOS/Linux/Windows",
      "REST API",
      "model library"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python library for Ollama; chat, generate, embed and pull models from Python scripts.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "pip install ollama",
      "async support",
      "embeddings",
      "chat interface"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama; works in Node and browsers via the Ollama REST API.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "npm i ollama",
      "TypeScript types",
      "streaming",
      "browser compatible"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/ollama",
    "summary": "LangChain integration for Ollama; use any LangChain abstraction with local Ollama models.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "pip install langchain-ollama",
      "LLM/Chat/Embeddings",
      "tool calling",
      "RAG"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (ChatGPT-style) with model management, RAG, voice input & multi-user support.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Docker",
      "OpenAI-compatible API",
      "document upload",
      "dark mode",
      "PWA"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama; chat, generate, pull, list and delete models from JVM applications.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Maven Central",
      "async",
      "streaming",
      "Spring Boot starter"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/gbaptista/ollama-rb",
    "summary": "Ruby gem for Ollama; simple idiomatic interface to chat and generate with local models.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "gem install ollama",
      "streaming",
      "Faraday adapter",
      "Rails ready"
    ]
  },
  {
    "title": "ollama-cli (npm)",
    "url": "https://www.npmjs.com/package/ollama-cli",
    "summary": "Lightweight Node CLI wrapper around the Ollama REST API for quick prompting and model ops.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "npm i -g ollama-cli",
      "interactive chat",
      "model listing",
      "streaming"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "Turn any Ollama model into a GitHub Copilot-style code-completion backend for VS Code.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "VS Code extension",
      "inline completions",
      "FIM templates",
      "configurable"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration to use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "pip install ollama-haystack",
      "RAG pipelines",
      "embeddings",
      "generator"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/ollama-csharp",
    "summary": ".NET SDK for Ollama; chat and generate with local models from C# and Blazor apps.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "NuGet",
      "async/await",
      "streaming",
      "Blazor sample"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Rust crate providing typed async bindings to the Ollama REST API.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "crates.io",
      "tokio",
      "serde",
      "streaming"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU and PVC support.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Helm",
      "GPU node-selector",
      "Ingress",
      "persistence",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-litellm-proxy",
    "url": "https://github.com/BerriAI/litellm/tree/main/proxy_server#ollama",
    "summary": "LiteLLM proxy exposing Ollama models via OpenAI-compatible endpoints for any client.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "OpenAI format",
      "load-balancing",
      "pip install litellm",
      "Docker"
    ]
  },
  {
    "title": "ollama-gptcli",
    "url": "https://github.com/kharvd/ollama-gptcli",
    "summary": "Minimal Python CLI that wraps Ollama with conversation memory and syntax highlighting.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "pip install ollama-gptcli",
      "pygments",
      "conversation history",
      "multi-line"
    ]
  },
  {
    "title": "ollama-vscode",
    "url": "https://github.com/lf-/ollama-vscode",
    "summary": "VS Code extension that adds Ollama chat and code-generation panels inside the editor.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "sidebar chat",
      "code insertion",
      "custom prompts",
      "status-bar"
    ]
  },
  {
    "title": "ollama-streamlit",
    "url": "https://github.com/jannikmi/ollama-streamlit",
    "summary": "Streamlit chat UI for Ollama with session memory, model selector and Docker one-liner.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "pip install ollama-streamlit",
      "session state",
      "Docker",
      "themes"
    ]
  },
  {
    "title": "ollama-telegram-bot",
    "url": "https://github.com/ruecat/ollama-telegram",
    "summary": "Self-hosted Telegram bot that chats with any Ollama model and supports group conversations.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Docker",
      "group chats",
      "/model command",
      "markdown support"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/jakobdylanc/llcord",
    "summary": "Discord.py bot that streams Ollama responses in Discord threads with slash commands.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "streaming replies",
      "/chat command",
      "thread isolation",
      "Docker"
    ]
  },
  {
    "title": "ollama-rag-langchain",
    "url": "https://github.com/gabacode/ollama-rag-langchain",
    "summary": "End-to-end RAG template using Ollama embeddings + LLM, Chroma vector store and LangChain.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "PDF ingestion",
      "Chroma",
      "streaming answers",
      "Docker Compose"
    ]
  }
]