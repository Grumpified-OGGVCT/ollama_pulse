[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: lightweight, extensible framework for running Llama 2, Mistral, Gemma and other LLMs locally with a simple CLI and REST API.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-contained binaries",
      "model library",
      "OpenAI-compatible API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama; chat, generate, pull, push, list models from Node or browsers.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "npm install ollama",
      "Promise-based",
      "browser & Node",
      "TypeScript defs"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama; chat, embed, pull, list, delete models with native async support.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install ollama",
      "async/await",
      "embeddings endpoint",
      "streaming responses"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package offering Ollama LLM and embeddings components.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "pip install langchain-ollama",
      "LLM & Embeddings",
      "chat models",
      "community support"
    ]
  },
  {
    "title": "ollama-webui (ollama-web/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich, ChatGPT-style web UI for Ollama with multi-model chat, file uploads, dark mode and admin panel.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "Docker ready",
      "file/chat history",
      "admin settings",
      "responsive"
    ]
  },
  {
    "title": "ollama-cli-copilot",
    "url": "https://github.com/sigma67/ollama-cli-copilot",
    "summary": "Terminal copilot that pipes shell questions to a local Ollama model and suggests one-liner commands.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "bash/zsh/fish",
      "no cloud calls",
      "configurable model",
      "safe dry-run"
    ]
  },
  {
    "title": "ollama-copilot.vim",
    "url": "https://github.com/gaardhus/ollama-copilot.vim",
    "summary": "Neovim/Vim plugin that brings local Ollama completions inline like GitHub Copilot.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Lua config",
      "streaming insert",
      "choose model",
      "no API keys"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/thinkdifferent/ollama-rag",
    "summary": "Minimal RAG template using Ollama for local embeddings + generation, Chroma vector store and PDF ingestion.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "ChromaDB",
      "PDF loader",
      "streamlit UI",
      "100% local"
    ]
  },
  {
    "title": "ollama-nuxt-chat",
    "url": "https://github.com/jacoblee/ollama-nuxt-chat",
    "summary": "Nuxt 3 starter that exposes a ChatGPT-like interface backed by Ollama over REST.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Nuxt 3 SSR",
      "auto-reload",
      "markdown render",
      "Dockerfile"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Java/Kotlin client for Ollama; supports chat, generate, pull, show, copy, delete with fluent API.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Maven Central",
      "Kotlin DSL",
      "reactive streams",
      "Spring Boot examples"
    ]
  },
  {
    "title": "ollama-node",
    "url": "https://www.npmjs.com/package/ollama-node",
    "summary": "Zero-dependency Node.js client for Ollama (community alternative), TypeScript included.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "npm i ollama-node",
      "ESM/CJS",
      "typedocs",
      "no axios"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration providing OllamaGenerator and OllamaEmbedding components for pipelines.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "pip install farm-haystack[ollama]",
      "pipeline nodes",
      "batching",
      "open source"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/mxyng/ollama-discord-bot",
    "summary": "Self-host Discord bot that answers prompts using any Ollama model; supports slash commands and thread context.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "discord.py",
      "thread memory",
      "/ask slash",
      "easy .env config"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hinterdup/ollama-obsidian",
    "summary": "Obsidian plugin to run local LLM prompts on selected notes or selections via Ollama.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "command palette",
      "template prompts",
      "custom model",
      "no cloud"
    ]
  },
  {
    "title": "ollama-streamlit",
    "url": "https://github.com/tanchongmin/ollama-streamlit",
    "summary": "Streamlit app template for chatting with Ollama models, includes sidebar model picker and token-speed metrics.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "one-file app",
      "token/s chart",
      "session state",
      "requirements.txt"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/ollama-csharp/ollama-csharp",
    "summary": "Community C#/.NET client for Ollama with async enumerable streaming and dependency injection helpers.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "NuGet package",
      ".NET 6+",
      "IAsyncEnumerable",
      "DI extensions"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama-rust/ollama-rs",
    "summary": "Rust crate providing strongly typed Ollama API bindings with tokio and streaming support.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "cargo add ollama-rs",
      "tokio",
      "serde",
      "examples"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Helm chart to deploy Ollama on Kubernetes with optional GPU node-selector and PVC model cache.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "GPU support",
      "nvidia plugin",
      "PVC cache",
      "values.yaml"
    ]
  },
  {
    "title": "ollama-rustic",
    "url": "https://crates.io/crates/ollama-rustic",
    "summary": "Lightweight alternative Rust client published on crates.io; mirrors official endpoints.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "no unsafe",
      "minimal deps",
      "MIT",
      "examples"
    ]
  },
  {
    "title": "ollama-rag-redis",
    "url": "https://github.com/redis-developer/ollama-rag-redis",
    "summary": "Redis sample showing how to build a RAG system combining Redis vector search and Ollama for fully local Q&A.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Redis Stack",
      "vector similarity",
      "pdf ingestion",
      "FastAPI"
    ]
  }
]