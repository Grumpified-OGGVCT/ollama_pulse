[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: get up and running with Llama 3, Mistral, Gemma, and other large language models locally.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "self-hosted LLM runtime",
      "Docker/CLI installers",
      "model library pulls"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama; chat, embed, pull models from Node or the browser.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "npm install ollama",
      "Promise-based API",
      "browser & Node support"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client; chat, generate, embed, list, pull, push models via simple async/sync calls.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "pip install ollama",
      "asyncio support",
      "embeddings API"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/ollama",
    "summary": "LangChain integration to use any Ollama model as an LLM or embeddings provider in chains & agents.",
    "source": "github",
    "date": "2024-05-17",
    "highlights": [
      "pip install langchain-ollama",
      "chat & embed wrappers",
      "tool-calling beta"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (chat UI, model manager, multi-user, dark mode, mobile friendly).",
    "source": "github",
    "date": "2024-05-16",
    "highlights": [
      "Docker one-liner",
      "OpenAI-compatible API proxy",
      "chat history"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/ollama4j/ollama4j",
    "summary": "Java/Kotlin client for Ollama; sync/async APIs, POJO mapping, Spring Boot starters available.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "Maven Central",
      "Kotlin coroutines",
      "Spring Boot starter"
    ]
  },
  {
    "title": "ollama-rs",
    "url": "https://github.com/ollama/ollama-rs",
    "summary": "Rust crate providing typed, async bindings to the Ollama HTTP API with tokio.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "crates.io/ollama-rs",
      "tokio/async",
      "strong typing"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/fostermaier/ollama-copilot",
    "summary": "VS Code extension that turns any Ollama model into a Copilot-style inline code assistant.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "inline completions",
      "configurable model",
      "status-bar toggle"
    ]
  },
  {
    "title": "ollama-chatbot-ui",
    "url": "https://github.com/jmorganca/ollama-chatbot-ui",
    "summary": "Minimal React chatbot front-end that streams responses from a local Ollama instance.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "Vite React template",
      "streaming fetch",
      "theme switch"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/ollama/ollama-cli",
    "summary": "Community-enhanced CLI wrapper with REPL mode, conversation persistence, and syntax highlighting.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "REPL",
      "conversation save/load",
      "syntax highlight"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart to deploy Ollama on Kubernetes with GPU & PVC support.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "GPU node selector",
      "PVC model cache",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-gui",
    "url": "https://github.com/tazz4843/ollama-gui",
    "summary": "Lightweight Tauri-based desktop GUI for Ollama (Windows, macOS, Linux) with tray icon.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Tauri/Rust",
      "system tray",
      "native executables"
    ]
  },
  {
    "title": "ollama-nvidia-docker",
    "url": "https://github.com/ollama/ollama-nvidia-docker",
    "summary": "Pre-built Docker Compose stack enabling GPU acceleration with official NVIDIA runtime.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "docker-compose",
      "nvidia runtime",
      "one-command GPU"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hintergrund/ollama-obsidian",
    "summary": "Obsidian plugin that lets you query local Ollama models from within notes for summarization & Q&A.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Obsidian plugin",
      "command palette",
      "template variables"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-ollama",
    "summary": "Haystack integration to use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "pip install haystack-ollama",
      "pipeline node",
      "embeddings support"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/ollama-discord/ollama-discord-bot",
    "summary": "Self-hostable Discord bot that streams Ollama replies into any channel with slash commands.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "slash commands",
      "reply streaming",
      "role-based access"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/royjhan/ollama-slack-bot",
    "summary": "Lightweight Slack Bolt app that answers questions using a local Ollama model in threads.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Bolt JS",
      "threaded replies",
      "mention trigger"
    ]
  },
  {
    "title": "ollama-elixir",
    "url": "https://github.com/econnolly/ollama-elixir",
    "summary": "Elixir OTP client for Ollama with GenServer behavior for back-pressure and streaming.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Hex.pm package",
      "GenServer",
      "back-pressure"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/sagikazarmark/ollama-dagger",
    "summary": "Dagger module that spins up Ollama as a CI service for testing LLM-powered features in pipelines.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "Dagger module",
      "CI service",
      "pipeline testing"
    ]
  },
  {
    "title": "ollama-rf",
    "url": "https://github.com/ollama-rf/ollama-robot-framework",
    "summary": "Robot Framework library enabling keyword-driven testing and automation with Ollama models.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Robot Framework",
      "keyword library",
      "acceptance testing"
    ]
  }
]