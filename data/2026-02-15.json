[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama CLI and server that lets you run Llama 2, Mistral, Gemma, and other large language models locally with a single command.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-contained binary",
      "macOS/Linux/Windows",
      "Docker image",
      "OpenAI-compatible API"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python library for Ollama\u2014install via pip and chat with local models in two lines of code.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip install ollama",
      "sync/async clients",
      "streaming support",
      "PyPI package"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node and browsers; published on npm as \u2018ollama\u2019.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "npm install ollama",
      "TypeScript types",
      "browser & Node",
      "streaming responses"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/llms/ollama",
    "summary": "LangChain integration that wraps Ollama models as standard LangChain LLMs and chat models.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "LangChain LLM",
      "chat wrapper",
      "pip install langchain-community",
      "callbacks support"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Full-featured ChatGPT-style web UI for Ollama (now renamed \u2018Open WebUI\u2019); runs in Docker and connects to local Ollama.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "Docker ready",
      "dark/light themes",
      "multi-model chat",
      "RAG uploads"
    ]
  },
  {
    "title": "Open WebUI (formerly ollama-webui)",
    "url": "https://github.com/open-webui/open-webui",
    "summary": "Successor project\u2014open-source WebUI for Ollama with plugins, RAG, user auth, and admin panels.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "pip/npm installable",
      "extensible plugins",
      "built-in RAG",
      "LDAP/OAuth"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-core-integrations/tree/main/integrations/ollama",
    "summary": "Haystack 2.x integration that lets you use Ollama models as generators in Haystack pipelines.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Haystack generator",
      "pip install ollama-haystack",
      "streaming",
      "pipeline nodes"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Experimental GitHub Copilot-style code-completion plugin that uses Ollama models locally.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "VS Code extension",
      "local completions",
      "custom model choice",
      "FIM templates"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Community-maintained Helm chart for deploying Ollama on Kubernetes clusters.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Helm chart",
      "GPU node selector",
      "persistent volumes",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Minimal RAG example that pairs Ollama with llama-index to chat over your documents offline.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "llama-index",
      "offline embeddings",
      "PDF support",
      "Gradio UI"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/NixOS/nixpkgs/tree/master/pkgs/tools/misc/ollama",
    "summary": "Nix package definition for Ollama; install with \u2018nix-env -iA nixpkgs.ollama\u2019.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Nix flake",
      "reproducible builds",
      "ARM64/x86_64",
      "systemd service"
    ]
  },
  {
    "title": "ollama-terraform",
    "url": "https://github.com/ollama/ollama-terraform",
    "summary": "Terraform module to provision GPU-enabled EC2 instances with Ollama pre-installed.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "AWS GPU AMI",
      "user-data bootstrap",
      "spot instances",
      "security group"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama-docker-compose",
    "summary": "Ready-to-use docker-compose stacks bundling Ollama with Open WebUI, PostgreSQL, and RAG extras.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "compose profiles",
      "GPU runtime",
      "volume mounts",
      "env file"
    ]
  },
  {
    "title": "ollama-cli-chat",
    "url": "https://github.com/sigma67/ollama-cli-chat",
    "summary": "Lightweight interactive CLI chat client written in Go that speaks to Ollama\u2019s API.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Go binary",
      "readline history",
      "markdown output",
      "multi-line prompts"
    ]
  },
  {
    "title": "ollama-streamlit-chat",
    "url": "https://github.com/richard-gyiko/ollama-streamlit-chat",
    "summary": "Streamlit chat app template that connects to local Ollama for quick prototyping.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "pip install streamlit",
      "session memory",
      "model selector",
      "Dockerfile"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hintergrund/ollama-obsidian",
    "summary": "Obsidian plugin that adds an Ollama sidebar to summarize notes or ask questions offline.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "Obsidian plugin",
      "offline Q&A",
      "custom prompts",
      "community plugin list"
    ]
  },
  {
    "title": "ollama-fastapi",
    "url": "https://github.com/1rgs/ollama-fastapi",
    "summary": "FastAPI service exposing extra endpoints (bulk chat, batch embed) on top of Ollama.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "FastAPI wrapper",
      "/batch_chat",
      "async pooling",
      "OpenAPI docs"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Unofficial Rust crate (ollama-rs) providing strongly-typed bindings to the Ollama API.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "crates.io/ollama-rs",
      "tokio client",
      "serde models",
      "examples folder"
    ]
  },
  {
    "title": "ollama-dotnet",
    "url": "https://github.com/awaescher/ollama-dotnet",
    "summary": "Community .NET SDK for Ollama with integration packages for ASP.NET Core and Blazor.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "NuGet OllamaSharp",
      "Blazor chat demo",
      "dependency injection",
      "streaming"
    ]
  },
  {
    "title": "ollama-reddit-community",
    "url": "https://www.reddit.com/r/ollama/",
    "summary": "Active subreddit for sharing models, troubleshooting, and showcasing Ollama-based projects.",
    "source": "reddit",
    "date": "2024-05-11",
    "highlights": [
      "model tips",
      "hardware threads",
      "GUI showcases",
      "modding guides"
    ]
  }
]