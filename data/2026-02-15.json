[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and Python/JS libraries to pull and run Llama 2, Mistral, Gemma, Phi, etc. locally with a built-in model registry and OpenAI-compatible REST API.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "built-in model library",
      "OpenAI-compatible API",
      "macOS/Linux/Windows",
      "extensible modelfile format"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK that wraps the Ollama REST API; chat, generate, embed, pull, create, list, delete models with async support.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "async/await",
      "pip install ollama",
      "100 % coverage of REST endpoints",
      "type hints"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node and browsers; same generate/chat/embed API surface as the Python SDK.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "npm install ollama",
      "ESM + CJS bundles",
      "typed",
      "streaming support"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package providing Ollama LLM, embed and chat wrappers with standard LangChain interfaces.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip install langchain-ollama",
      "streaming",
      "callback handlers",
      "native embed"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Full-featured ChatGPT-style web UI for Ollama (now renamed \u201cOpen WebUI\u201d); markdown, code highlighting, model manager, RAG, multi-user.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "docker run",
      "rag via document upload",
      "dark/light themes",
      "admin panel"
    ]
  },
  {
    "title": "continue/continue",
    "url": "https://github.com/continuedev/continue",
    "summary": "VS Code & JetBrains plugin that adds local AI coding assistant; ships with Ollama driver so you can use Code Llama, WizardCoder, etc. offline.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "inline autocomplete",
      "/edit commands",
      "free & open-source",
      "privacy-first"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/haystack",
    "summary": "Haystack integration by Ollama team; provides Generator and Embedder components so you can build RAG pipelines with local models.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install ollama-haystack",
      "compatible with Haystack 2.0",
      "streaming",
      "embed"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Lightweight proxy that turns any Ollama model into a GitHub Copilot-compatible server for use in Copilot clients.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "self-hosted copilot",
      "open-source",
      "no MS account needed",
      "vscode plugin ready"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama/ollama-discord",
    "summary": "Discord bot that lets a server chat with local Ollama models; slash commands, thread support, modelfile switching.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "docker image",
      "configurable prefix",
      "moderation hooks",
      "streaming replies"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Minimal retrieval-augmented-generation example using Ollama for both embeddings and chat with Chroma vector store.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "single-file demo",
      "Chroma integration",
      "pdf ingestion",
      "streaming answers"
    ]
  },
  {
    "title": "ollama-nim",
    "url": "https://github.com/ollama/nim-ollama",
    "summary": "Community-maintained Nim wrapper for the Ollama REST API; supports generate, chat, pull, create, delete endpoints.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Nimble package",
      "static binary friendly",
      "full async",
      "examples"
    ]
  },
  {
    "title": "ollama-dotnet",
    "url": "https://github.com/ollama/ollama-dotnet",
    "summary": "Unofficial .NET SDK for Ollama; strongly-typed models, dependency-injection helpers, and streaming IAsyncEnumerable support.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "NuGet: Ollama",
      "C# 11",
      "DI extensions",
      "unit-tested"
    ]
  },
  {
    "title": "ollama-rs",
    "url": "https://github.com/ollama/ollama-rs",
    "summary": "Community Rust crate providing async generate/chat/embed functions with tokio and reqwest.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "crates.io: ollama-rs",
      "tokio native",
      "serde types",
      "examples"
    ]
  },
  {
    "title": "ollama-streamlit",
    "url": "https://github.com/ollama/ollama-streamlit",
    "summary": "Streamlit chat app template that talks to Ollama; includes session memory, model selector, and markdown rendering.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "one-click run",
      "session persistence",
      "responsive UI",
      "docker ready"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Helm chart to deploy Ollama server and models on Kubernetes with GPU node-selector and PVC support.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "Helm repo",
      "GPU scheduling",
      "configmaps for modelfiles",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/ollama/dagger-ollama",
    "summary": "Dagger module that caches and serves Ollama models inside CI pipelines so you can run LLM-based tests without external calls.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "Daggerverse",
      "CI caching",
      "no API keys",
      "GitHub Actions example"
    ]
  },
  {
    "title": "ollama-rust-tutorial",
    "url": "https://github.com/twobitt/ollama-rust-tutorial",
    "summary": "Step-by-step repo showing how to build a Tauri desktop app that bundles Ollama for fully offline LLM chat.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "Tauri + React",
      "bundled binaries",
      "cross-platform builds",
      "MIT"
    ]
  },
  {
    "title": "ollama-cli-chat",
    "url": "https://github.com/kevinw/ollama-cli-chat",
    "summary": "Terminal UI chat client written in Go with bubbletea; features markdown, syntax highlighting, and conversation history.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "single binary",
      "vim keys",
      "history file",
      "themes"
    ]
  },
  {
    "title": "ollama-models",
    "url": "https://github.com/ollama/ollama-models",
    "summary": "Community-curated collection of custom Modelfiles for CodeLlama-70b, WizardMath, Zephyr, and quantized variants.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "ready-to-use modelfiles",
      "GGUF links",
      "quantization matrix",
      "PR welcome"
    ]
  },
  {
    "title": "ollama-rag-langchain-tutorial",
    "url": "https://github.com/gkamradt/ollama-rag-langchain-tutorial",
    "summary": "YouTube companion repo that walks through building a PDF question-answering bot with Ollama, LangChain and Chroma.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "step-by-step notebook",
      "video link",
      "PDF ingestion",
      "streaming output"
    ]
  }
]