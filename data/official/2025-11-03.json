[
  {
    "title": "ollama/ollama \u2013 GitHub",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official repo for Ollama: run Llama 3, Mistral, Gemma and other large language models locally with a single command. Includes Docker image and REST API.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "docker",
      "REST API",
      "local inference",
      "Modelfile"
    ]
  },
  {
    "title": "Ollama WebUI \u2013 intuitive browser interface",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Self-hosted ChatGPT-like UI that talks to any Ollama endpoint (local or cloud) via JS fetch, supports multi-model chats and markdown export.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "ChatGPT-like",
      "markdown export",
      "multi-model"
    ]
  },
  {
    "title": "Ollama Turbo \u2013 1-click cloud GPU images",
    "url": "https://github.com/ollama-turbo/cloud-images",
    "summary": "Community project that bakes Ollama into GPU-enabled AWS & GCP AMIs with auto-scaling and pay-per-token billing scripts.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "pre-loaded models",
      "Terraform templates",
      "Cloud-Init",
      "spot-price optimizer"
    ]
  },
  {
    "title": "Ollama Cloud Gateway",
    "url": "https://github.com/ollama/cloud-gateway",
    "summary": "Official experimental gateway that proxies local Ollama instances to a managed cloud endpoint with auth, quotas, and usage dashboards.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "JWT auth",
      "OpenAI-style routes",
      "auto-scaling",
      "usage metrics"
    ]
  },
  {
    "title": "LangChain + Ollama integration docs",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "Step-by-step guide to use Ollama models inside LangChain chains, agents and retrieval pipelines; includes streaming and JSON mode.",
    "source": "blog",
    "date": "2024-05-11",
    "highlights": [
      "LangChain",
      "streaming",
      "JSON mode",
      "RAG"
    ]
  },
  {
    "title": "Ollama on Fly.io \u2013 serverless GPU",
    "url": "https://fly.io/docs/app-guides/ollama/",
    "summary": "Step-by-step Fly.io recipe to run Ollama in Firecracker micro-VMs with GPU slices and anycast routing for low-latency inference.",
    "source": "blog",
    "date": "2024-04-30",
    "highlights": [
      "scale-to-zero",
      "GPU slices",
      "Dockerfile example",
      "persistent volumes"
    ]
  },
  {
    "title": "Ollama Helm Charts for Kubernetes",
    "url": "https://github.com/ollama-helm/charts",
    "summary": "Community-maintained Helm chart that deploys Ollama on any Kubernetes cluster with optional GPU node-affinity and autoscaling.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "GPU scheduling",
      "HPA",
      "model PVC cache",
      "ingress examples"
    ]
  },
  {
    "title": "Ollama Discord bot (cloud-hosted)",
    "url": "https://github.com/ollama-discord/bot",
    "summary": "Open-source Discord bot that hits a managed Ollama Turbo endpoint so every guild can chat with Llama 3 without self-hosting.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "slash commands",
      "role-based access",
      "per-guild quotas",
      "streaming replies"
    ]
  },
  {
    "title": "Reddit: self-host vs cloud Ollama discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1cklr5j/ollama_cloud_hosted_vs_self_host_cost_comparison/",
    "summary": "Users compare $/1k tokens for cloud GPU providers vs local RTX 4090 rigs when running Ollama models.",
    "source": "reddit",
    "date": "2024-05-12",
    "highlights": [
      "cost spreadsheet",
      "spot GPU pricing",
      "power draw math",
      "bandwidth notes"
    ]
  },
  {
    "title": "Hacker News: Show HN \u2013 Ollama Turbo API",
    "url": "https://news.ycombinator.com/item?id=40291837",
    "summary": "Launch thread for a hosted Ollama-compatible API that claims 50 ms cold-start on A100s and usage-based billing.",
    "source": "hackernews",
    "date": "2024-04-26",
    "highlights": [
      "50 ms cold start",
      "A100 GPUs",
      "usage-based billing",
      "OpenAI route parity"
    ]
  },
  {
    "title": "Ollama Model Library \u2013 official registry",
    "url": "https://ollama.com/library",
    "summary": "Curated registry of 100+ ready-to-run GGUF models (Llama 3, Phi-3, Mistral, CodeLlama, etc.) with one-line pull commands and parameter notes.",
    "source": "blog",
    "date": "2024-05-14",
    "highlights": [
      "GGUF",
      "one-line pull",
      "100+ models"
    ]
  },
  {
    "title": "YouTube: Ollama Cloud Deployment Walk-through",
    "url": "https://youtu.be/3d_3bHnhPQs",
    "summary": "15-min demo showing how to spin up Ollama on RunPod, expose it via HTTPS, and add auth with Cloudflare Access.",
    "source": "youtube",
    "date": "2024-05-01",
    "highlights": [
      "RunPod template",
      "Cloudflare tunnel",
      "API key auth",
      "cost estimator"
    ]
  },
  {
    "title": "Ollama Python & JavaScript libraries",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official PyPI package to pull, push and chat with any Ollama model from Python; includes async client and streaming support.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "PyPI",
      "async",
      "streaming"
    ]
  },
  {
    "title": "Ollama-js \u2013 npm wrapper",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official npm package providing the same API surface as the Python client for Node/Browser use, works against local or cloud endpoints.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "npm",
      "browser",
      "cloud endpoint"
    ]
  },
  {
    "title": "Benchmark: Ollama vs text-generation-webui on cloud GPUs",
    "url": "https://github.com/cloud-gpu-llm/benchmarks/blob/main/ollama_tgw_cloud_report.md",
    "summary": "Community benchmark comparing throughput & latency of Ollama and TGW when serving Llama 3 70B on 2\u00d7A100 80 GB cloud instances.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "tokens/sec",
      "time-to-first-token",
      "GPU mem usage",
      "batch-size scaling"
    ]
  },
  {
    "title": "Ollama on RunPod & Hugging Face Inference Endpoints",
    "url": "https://www.runpod.io/blog/ollama-runpod-template",
    "summary": "RunPod released a one-click template that spins up GPU pods with Ollama pre-installed, turning local models into cloud-hosted Turbo-style APIs.",
    "source": "blog",
    "date": "2024-04-22",
    "highlights": [
      "GPU cloud",
      "one-click template",
      "Turbo API",
      "pay-per-second"
    ]
  },
  {
    "title": "Ollama modelfile hub \u2013 ollama-models",
    "url": "https://github.com/ollama/ollama-models",
    "summary": "Community-curated Modelfiles for 150+ models (Llama-3, Phi-3, StarCoder, etc.) with quantized sizes and ready-to-use prompts.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Modelfile",
      "quantized",
      "community",
      "pull & run"
    ]
  },
  {
    "title": "Ollama vs text-generation-webui Reddit discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1cf0o7u/ollama_vs_textgenerationwebui/",
    "summary": "Users compare ease-of-use, API speed and extension ecosystem; consensus: Ollama for simplicity, TGW for features.",
    "source": "reddit",
    "date": "2024-05-05",
    "highlights": [
      "ease-of-use",
      "API speed",
      "extensions",
      "community"
    ]
  },
  {
    "title": "Ollama Helm Chart for Kubernetes",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart with autoscaling, PVC, and optional cloud GPU node-affinity for running Ollama in k8s clusters.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Helm",
      "autoscaling",
      "GPU node-affinity"
    ]
  },
  {
    "title": "Ollama Turbo \u2013 cloud-hosted Llama-3-70B API (beta)",
    "url": "https://turbo.ollama.ai",
    "summary": "Official beta offering 70B-scale models as pay-as-you-go REST endpoints; claims OpenAI-compatible chat completions with 2s latency.",
    "source": "blog",
    "date": "2024-05-13",
    "highlights": [
      "cloud-hosted",
      "Llama-3-70B",
      "OpenAI-compat",
      "2s latency"
    ]
  },
  {
    "title": "Ollama Discord bots \u2013 ollama-bot",
    "url": "https://github.com/ripmehe/ollama-bot",
    "summary": "Simple Discord.py bot that calls local Ollama REST API; supports slash commands, system prompts and per-user model choice.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Discord",
      "slash commands",
      "per-user model",
      "system prompt"
    ]
  },
  {
    "title": "Ollama on Apple Silicon speed tests",
    "url": "https://news.ycombinator.com/item?id=40321476",
    "summary": "HN thread showing 70B Llama running at 12 t/s on M2 Ultra with 192 GB RAM; discussion on memory bandwidth vs GPU cloud costs.",
    "source": "hackernews",
    "date": "2024-05-06",
    "highlights": [
      "Apple Silicon",
      "70B 12t/s",
      "memory bandwidth",
      "cost"
    ]
  },
  {
    "title": "Ollama GPU benchmarking suite \u2013 ollama-bench",
    "url": "https://github.com/technovangelist/ollama-bench",
    "summary": "Automated benchmark that pulls 10 models, runs predefined prompts and exports tokens/sec across GPUs for easy comparison.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "benchmark",
      "tokens/sec",
      "GPU compare",
      "automated"
    ]
  },
  {
    "title": "Ollama MCP (Model Context Protocol) plugin",
    "url": "https://github.com/modelcontextprotocol/ollama-mcp",
    "summary": "Experimental plugin exposing Ollama models via the open MCP spec so they can be consumed by any MCP-compatible host (e.g., Claude Desktop).",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "MCP",
      "Claude Desktop",
      "open protocol",
      "experimental"
    ]
  },
  {
    "title": "Ollama now supports running models on remote GPUs (beta cloud)",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.1.38",
    "summary": "Release notes for v0.1.38 introducing experimental cloud-hosted GPU runners; point local CLI to remote endpoint for accelerated inference.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "cloud GPUs",
      "beta",
      "remote endpoint"
    ]
  },
  {
    "title": "Turbo API wrapper for Ollama \u2013 ollama-turbo",
    "url": "https://github.com/sammcj/ollama-turbo",
    "summary": "Lightweight Python/fastapi proxy that adds OpenAI-compatible /v1/chat/completions endpoint on top of Ollama for easier drop-in replacement.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "OpenAI-compatible",
      "fastapi",
      "drop-in"
    ]
  },
  {
    "title": "Ollama Cloud Mode \u2013 Reddit discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1c6z3rj/ollama_cloud_mode_beta/",
    "summary": "Community thread sharing first impressions of the new cloud-GPU beta, cost vs self-host, and sample docker-compose for remote workers.",
    "source": "reddit",
    "date": "2024-05-02",
    "highlights": [
      "cost discussion",
      "docker-compose",
      "beta feedback"
    ]
  },
  {
    "title": "Show HN: Ollama Turbo \u2013 Serve Ollama with OpenAI SDKs",
    "url": "https://news.ycombinator.com/item?id=40392871",
    "summary": "Launch post for ollama-turbo showing how to use existing OpenAI clients without code changes; benchmarks against vanilla Ollama latency.",
    "source": "hackernews",
    "date": "2024-05-11",
    "highlights": [
      "Show HN",
      "latency benchmark",
      "zero-code-change"
    ]
  },
  {
    "title": "LangChain Ollama integration docs",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama/",
    "summary": "Step-by-step guide to plug Ollama (local or cloud) into LangChain pipelines using the Ollama LLM wrapper and chat model interface.",
    "source": "blog",
    "date": "2024-05-08",
    "highlights": [
      "LangChain",
      "pipelines",
      "chat model"
    ]
  },
  {
    "title": "LlamaIndex Ollama connector",
    "url": "https://docs.llamaindex.ai/en/stable/examples/llm/ollama/",
    "summary": "Example notebooks showing how to use Ollama as the LLM backend for LlamaIndex retrieval-augmented-generation workflows.",
    "source": "blog",
    "date": "2024-05-07",
    "highlights": [
      "RAG",
      "notebooks",
      "retrieval"
    ]
  },
  {
    "title": "Deploy Ollama on Fly.io \u2013 community guide",
    "url": "https://github.com/fly-apps/ollama-fly",
    "summary": "Repo + blog post detailing how to run Ollama on Fly Machines with GPU support, persistent volumes, and WireGuard peer access.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "Fly.io",
      "GPU",
      "WireGuard"
    ]
  },
  {
    "title": "Benchmarking Ollama cloud GPUs vs RTX 4090 \u2013 YouTube",
    "url": "https://www.youtube.com/watch?v=8Kp6RzypR1U",
    "summary": "15-min video comparing tokens/sec and cost between Ollama\u2019s new cloud GPUs and a local RTX 4090 box for Llama-3-70B.",
    "source": "youtube",
    "date": "2024-05-06",
    "highlights": [
      "tokens/sec",
      "cost analysis",
      "Llama-3-70B"
    ]
  },
  {
    "title": "Ollama Modelfile deep-dive \u2013 creating custom models",
    "url": "https://github.com/ollama/ollama/blob/main/docs/modelfile.md",
    "summary": "Official documentation explaining how to write a Modelfile to bundle weights, system prompts, temperature and tensor-split options for sharing.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Modelfile",
      "system prompt",
      "tensor-split"
    ]
  }
]