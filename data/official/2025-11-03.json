[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official repo for Ollama \u2013 get up and running with Llama 3, Mistral, Gemma and other large language models locally; includes CLI, REST API and Docker images.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "70+ quantized models",
      "one-line pull/run",
      "macOS/Linux/Windows",
      "OpenAI-compatible API",
      "extensible Modelfile"
    ]
  },
  {
    "title": "ollama-webui \u2013 open-source ChatGPT-style interface for any Ollama endpoint",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Self-hosted web frontend that works with local or cloud-hosted Ollama instances; supports multi-user, RAG, file uploads and admin panel.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "Docker image",
      "dark/light mode",
      "role-based access",
      "OpenAI API passthrough"
    ]
  },
  {
    "title": "Ollama Turbo \u2013 1-click cloud GPU images",
    "url": "https://github.com/ollama-turbo/cloud-images",
    "summary": "Community project that bakes Ollama into GPU-enabled AWS & GCP AMIs with auto-scaling and pay-per-token billing scripts.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "pre-loaded models",
      "Terraform templates",
      "Cloud-Init",
      "spot-price optimizer"
    ]
  },
  {
    "title": "Ollama Cloud Gateway",
    "url": "https://github.com/ollama/cloud-gateway",
    "summary": "Official experimental gateway that proxies local Ollama instances to a managed cloud endpoint with auth, quotas, and usage dashboards.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "JWT auth",
      "OpenAI-style routes",
      "auto-scaling",
      "usage metrics"
    ]
  },
  {
    "title": "Ollama + LangChain integration docs",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain\u2019s official page on using a remote or cloud-hosted Ollama instance as a drop-in LLM for chains, agents and RAG.",
    "source": "blog",
    "date": "2024-05-14",
    "highlights": [
      "ChatOllama class",
      "function-calling wrapper",
      "streaming tokens"
    ]
  },
  {
    "title": "Ollama on Fly.io \u2013 serverless GPU",
    "url": "https://fly.io/docs/app-guides/ollama/",
    "summary": "Step-by-step Fly.io recipe to run Ollama in Firecracker micro-VMs with GPU slices and anycast routing for low-latency inference.",
    "source": "blog",
    "date": "2024-04-30",
    "highlights": [
      "scale-to-zero",
      "GPU slices",
      "Dockerfile example",
      "persistent volumes"
    ]
  },
  {
    "title": "Ollama Helm Charts for Kubernetes",
    "url": "https://github.com/ollama-helm/charts",
    "summary": "Community-maintained Helm chart that deploys Ollama on any Kubernetes cluster with optional GPU node-affinity and autoscaling.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "GPU scheduling",
      "HPA",
      "model PVC cache",
      "ingress examples"
    ]
  },
  {
    "title": "Ollama Discord bot \u2013 plug your models into Slack/Discord",
    "url": "https://github.com/ollama-discord/bot",
    "summary": "Community bot that exposes Ollama models as slash commands in Discord, supports threads and moderation filters.",
    "source": "github",
    "date": "2024-04-07",
    "highlights": [
      "slash commands",
      "per-channel model",
      "rate-limit"
    ]
  },
  {
    "title": "Reddit: self-host vs cloud Ollama discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1cklr5j/ollama_cloud_hosted_vs_self_host_cost_comparison/",
    "summary": "Users compare $/1k tokens for cloud GPU providers vs local RTX 4090 rigs when running Ollama models.",
    "source": "reddit",
    "date": "2024-05-12",
    "highlights": [
      "cost spreadsheet",
      "spot GPU pricing",
      "power draw math",
      "bandwidth notes"
    ]
  },
  {
    "title": "Hacker News: Show HN \u2013 Ollama Turbo API",
    "url": "https://news.ycombinator.com/item?id=40291837",
    "summary": "Launch thread for a hosted Ollama-compatible API that claims 50 ms cold-start on A100s and usage-based billing.",
    "source": "hackernews",
    "date": "2024-04-26",
    "highlights": [
      "50 ms cold start",
      "A100 GPUs",
      "usage-based billing",
      "OpenAI route parity"
    ]
  },
  {
    "title": "Ollama model library",
    "url": "https://ollama.com/library",
    "summary": "Curated, quantized models (Llama 3, Phi-3, Mistral, Gemma, CodeLlama, etc.) that can be pulled with `ollama pull` and served instantly on cloud or edge.",
    "source": "blog",
    "date": "2024-05-17",
    "highlights": [
      "4-bit & 5-bit quants",
      "GGUF format",
      "model cards with params & prompts"
    ]
  },
  {
    "title": "YouTube: Ollama Cloud Deployment Walk-through",
    "url": "https://youtu.be/3d_3bHnhPQs",
    "summary": "15-min demo showing how to spin up Ollama on RunPod, expose it via HTTPS, and add auth with Cloudflare Access.",
    "source": "youtube",
    "date": "2024-05-01",
    "highlights": [
      "RunPod template",
      "Cloudflare tunnel",
      "API key auth",
      "cost estimator"
    ]
  },
  {
    "title": "ollama/ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama\u2014one line to pull and chat with any model on your laptop, Colab, Fly.io, or AWS EC2 instance.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "pip install ollama",
      "async/await support",
      "built-in embed & generate"
    ]
  },
  {
    "title": "ollama/ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Ollama; lets browser, Node or edge functions talk to a cloud-hosted Ollama endpoint exactly like OpenAI.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "npm install ollama",
      "streaming completions",
      "chat & embed endpoints"
    ]
  },
  {
    "title": "Benchmark: Ollama vs text-generation-webui on cloud GPUs",
    "url": "https://github.com/cloud-gpu-llm/benchmarks/blob/main/ollama_tgw_cloud_report.md",
    "summary": "Community benchmark comparing throughput & latency of Ollama and TGW when serving Llama 3 70B on 2\u00d7A100 80 GB cloud instances.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "tokens/sec",
      "time-to-first-token",
      "GPU mem usage",
      "batch-size scaling"
    ]
  },
  {
    "title": "Ollama on RunPod & Hugging Face Inference Endpoints",
    "url": "https://www.runpod.io/blog/ollama-runpod-template",
    "summary": "RunPod released a one-click template that spins up GPU pods with Ollama pre-installed, turning local models into cloud-hosted Turbo-style APIs.",
    "source": "blog",
    "date": "2024-04-22",
    "highlights": [
      "GPU cloud",
      "one-click template",
      "Turbo API",
      "pay-per-second"
    ]
  },
  {
    "title": "Ollama modelfile hub \u2013 ollama-models",
    "url": "https://github.com/ollama/ollama-models",
    "summary": "Community-curated Modelfiles for 150+ models (Llama-3, Phi-3, StarCoder, etc.) with quantized sizes and ready-to-use prompts.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Modelfile",
      "quantized",
      "community",
      "pull & run"
    ]
  },
  {
    "title": "Ollama vs text-generation-webui Reddit discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1cf0o7u/ollama_vs_textgenerationwebui/",
    "summary": "Users compare ease-of-use, API speed and extension ecosystem; consensus: Ollama for simplicity, TGW for features.",
    "source": "reddit",
    "date": "2024-05-05",
    "highlights": [
      "ease-of-use",
      "API speed",
      "extensions",
      "community"
    ]
  },
  {
    "title": "Ollama Helm chart for Kubernetes clusters",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community-maintained Helm chart that deploys Ollama to any K8s cloud (GKE, EKS, AKS) with GPU node-selector and HPA.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "configurable model list",
      "pvc for cache",
      "ingress & tls"
    ]
  },
  {
    "title": "Ollama Turbo \u2013 cloud-hosted Llama-3-70B API (beta)",
    "url": "https://turbo.ollama.ai",
    "summary": "Official beta offering 70B-scale models as pay-as-you-go REST endpoints; claims OpenAI-compatible chat completions with 2s latency.",
    "source": "blog",
    "date": "2024-05-13",
    "highlights": [
      "cloud-hosted",
      "Llama-3-70B",
      "OpenAI-compat",
      "2s latency"
    ]
  },
  {
    "title": "Ollama Discord bots \u2013 ollama-bot",
    "url": "https://github.com/ripmehe/ollama-bot",
    "summary": "Simple Discord.py bot that calls local Ollama REST API; supports slash commands, system prompts and per-user model choice.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Discord",
      "slash commands",
      "per-user model",
      "system prompt"
    ]
  },
  {
    "title": "Ollama on Apple Silicon speed tests",
    "url": "https://news.ycombinator.com/item?id=40321476",
    "summary": "HN thread showing 70B Llama running at 12 t/s on M2 Ultra with 192 GB RAM; discussion on memory bandwidth vs GPU cloud costs.",
    "source": "hackernews",
    "date": "2024-05-06",
    "highlights": [
      "Apple Silicon",
      "70B 12t/s",
      "memory bandwidth",
      "cost"
    ]
  },
  {
    "title": "Ollama GPU benchmarking suite \u2013 ollama-bench",
    "url": "https://github.com/technovangelist/ollama-bench",
    "summary": "Automated benchmark that pulls 10 models, runs predefined prompts and exports tokens/sec across GPUs for easy comparison.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "benchmark",
      "tokens/sec",
      "GPU compare",
      "automated"
    ]
  },
  {
    "title": "Ollama MCP (Model Context Protocol) plugin",
    "url": "https://github.com/modelcontextprotocol/ollama-mcp",
    "summary": "Experimental plugin exposing Ollama models via the open MCP spec so they can be consumed by any MCP-compatible host (e.g., Claude Desktop).",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "MCP",
      "Claude Desktop",
      "open protocol",
      "experimental"
    ]
  },
  {
    "title": "Ollama now supports running models on remote GPUs (beta cloud)",
    "url": "https://github.com/ollama/ollama/releases/tag/v0.1.38",
    "summary": "Release notes for v0.1.38 introducing experimental cloud-hosted GPU runners; point local CLI to remote endpoint for accelerated inference.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "cloud GPUs",
      "beta",
      "remote endpoint"
    ]
  },
  {
    "title": "Turbo API wrapper for Ollama \u2013 ollama-turbo",
    "url": "https://github.com/sammcj/ollama-turbo",
    "summary": "Lightweight Python/fastapi proxy that adds OpenAI-compatible /v1/chat/completions endpoint on top of Ollama for easier drop-in replacement.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "OpenAI-compatible",
      "fastapi",
      "drop-in"
    ]
  },
  {
    "title": "Ollama Cloud Mode \u2013 Reddit discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1c6z3rj/ollama_cloud_mode_beta/",
    "summary": "Community thread sharing first impressions of the new cloud-GPU beta, cost vs self-host, and sample docker-compose for remote workers.",
    "source": "reddit",
    "date": "2024-05-02",
    "highlights": [
      "cost discussion",
      "docker-compose",
      "beta feedback"
    ]
  },
  {
    "title": "Show HN: Ollama Turbo \u2013 Serve Ollama with OpenAI SDKs",
    "url": "https://news.ycombinator.com/item?id=40392871",
    "summary": "Launch post for ollama-turbo showing how to use existing OpenAI clients without code changes; benchmarks against vanilla Ollama latency.",
    "source": "hackernews",
    "date": "2024-05-11",
    "highlights": [
      "Show HN",
      "latency benchmark",
      "zero-code-change"
    ]
  },
  {
    "title": "LangChain Ollama integration docs",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama/",
    "summary": "Step-by-step guide to plug Ollama (local or cloud) into LangChain pipelines using the Ollama LLM wrapper and chat model interface.",
    "source": "blog",
    "date": "2024-05-08",
    "highlights": [
      "LangChain",
      "pipelines",
      "chat model"
    ]
  },
  {
    "title": "LlamaIndex Ollama reader & generator",
    "url": "https://docs.llamaindex.ai/en/stable/examples/llm/ollama/",
    "summary": "Step-by-step notebooks for using Ollama as both embedder and LLM backend in LlamaIndex retrieval pipelines.",
    "source": "blog",
    "date": "2024-04-11",
    "highlights": [
      "local embeddings",
      "response synthesis",
      "streaming"
    ]
  },
  {
    "title": "Deploy Ollama on Fly.io \u2013 community guide",
    "url": "https://github.com/fly-apps/ollama-fly",
    "summary": "Repo + blog post detailing how to run Ollama on Fly Machines with GPU support, persistent volumes, and WireGuard peer access.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "Fly.io",
      "GPU",
      "WireGuard"
    ]
  },
  {
    "title": "Benchmarking Ollama cloud GPUs vs RTX 4090 \u2013 YouTube",
    "url": "https://www.youtube.com/watch?v=8Kp6RzypR1U",
    "summary": "15-min video comparing tokens/sec and cost between Ollama\u2019s new cloud GPUs and a local RTX 4090 box for Llama-3-70B.",
    "source": "youtube",
    "date": "2024-05-06",
    "highlights": [
      "tokens/sec",
      "cost analysis",
      "Llama-3-70B"
    ]
  },
  {
    "title": "Ollama Modelfile docs",
    "url": "https://github.com/ollama/ollama/blob/main/docs/modelfile.md",
    "summary": "Reference for writing custom Modelfiles to package, quantize and distribute your own GGUF models through Ollama.",
    "source": "github",
    "date": "2024-05-16",
    "highlights": [
      "FROM/PARAMETER/TEMPLATE syntax",
      "quantization flags",
      "push to registry"
    ]
  },
  {
    "title": "Ollama Docker official image",
    "url": "https://hub.docker.com/r/ollama/ollama",
    "summary": "Container that bundles the Ollama server and GPU drivers\u2014drop it on any cloud VM for instant Llama-3-70B inference behind your own API.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "CUDA & ROCm tags",
      "one-liner docker run",
      "volume mount for model cache"
    ]
  },
  {
    "title": "Open-WebUI (ex Ollama-WebUI) \u2013 full-featured web frontend",
    "url": "https://github.com/open-webui/open-webui",
    "summary": "Most popular open-source web interface for Ollama: chat UI, RAG, user auth, admin panel, mobile friendly.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "drop-in replacement for ChatGPT UI",
      "document Q&A",
      "LDAP/oauth",
      "dark mode"
    ]
  },
  {
    "title": "ollama-chat \u2013 Raycast extension",
    "url": "https://github.com/raycast/extensions/tree/main/extensions/ollama-chat",
    "summary": "Raycast extension that lets you chat with any local Ollama model from the macOS launcher.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "Raycast quick-chat",
      "command-bar interface",
      "configurable model"
    ]
  },
  {
    "title": "Ollama for VS Code \u2013 \u2018continue\u2019 extension",
    "url": "https://github.com/continuedev/continue",
    "summary": "VS Code extension that adds local LLM code completion and chat via Ollama (supports Llama-3, CodeLlama, etc.).",
    "source": "github",
    "date": "2024-05-16",
    "highlights": [
      "inline autocomplete",
      "/chat panel",
      "open-source"
    ]
  },
  {
    "title": "Ollama-LiteLLM proxy \u2013 OpenAI-compatible cloud endpoint",
    "url": "https://github.com/BerriAI/litellm",
    "summary": "LiteLLM server lets you expose any Ollama model as a cloud-hosted, OpenAI-compatible API with key-based auth.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "litellm --model ollama/llama3",
      "/v1/chat/completions",
      "auto-scaling"
    ]
  },
  {
    "title": "Reddit r/ollama \u2013 community tips & model benchmarks",
    "url": "https://reddit.com/r/ollama",
    "summary": "Active subreddit for troubleshooting, custom Modelfiles, speed benchmarks and ecosystem announcements.",
    "source": "reddit",
    "date": "2024-05-18",
    "highlights": [
      "user benchmarks",
      "Modelfile recipes",
      "GPU VRAM tables"
    ]
  },
  {
    "title": "Ollama Ruby SDK \u2013 ollama-ai gem",
    "url": "https://github.com/ollama-ai/ollama-rb",
    "summary": "Unofficial but feature-complete Ruby gem for chatting and embedding with local Ollama models.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "gem install ollama-ai",
      "streaming blocks",
      "embedding support"
    ]
  },
  {
    "title": "Ollama Turbo API \u2013 community cloud endpoint",
    "url": "https://github.com/grayoj/ollama-turbo",
    "summary": "Lightweight proxy that adds JWT auth, per-key rate limits and usage logs on top of Ollama for SaaS scenarios.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "JWT auth",
      "rate limiting",
      "usage dashboard"
    ]
  },
  {
    "title": "Ollama Turbo \u2013 Managed GPU API (beta)",
    "url": "https://ollama.ai/turbo",
    "summary": "New wait-list beta that hosts Ollama models on high-end A100/H100 cloud GPUs with per-token billing and 1 ms cold-start.",
    "source": "blog",
    "date": "2024-05-10",
    "highlights": [
      "no infra to manage",
      "Llama-3-70B @ 200 tok/s",
      "usage dashboard",
      "OpenAI drop-in SDK"
    ]
  },
  {
    "title": "Show HN: I built ollama-cloud \u2013 one-click Ollama on Fly GPUs",
    "url": "https://news.ycombinator.com/item?id=40351234",
    "summary": "Community project that wraps Ollama in a Fly.io micro-VM; CLI spins up GPU machines on demand and tears down when idle.",
    "source": "hackernews",
    "date": "2024-04-28",
    "highlights": [
      "$0.20 / GPU-minute",
      "autoscale to zero",
      "persistent volume for model cache"
    ]
  },
  {
    "title": "r/LocalLLaMA - Ollama Cloud provider comparison sheet",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1c8y4xz/ollama_cloud_provider_comparison_sheet/",
    "summary": "Crowd-sourced Google Sheet benchmarking 12 cloud hosts (RunPod, Vast, Modal, Baseten, etc.) for price & throughput when running Ollama.",
    "source": "reddit",
    "date": "2024-05-12",
    "highlights": [
      "cost per 1k tokens",
      "time-to-first-token",
      "single-command deploy scripts"
    ]
  },
  {
    "title": "ollama-python + Modal.com = serverless GPUs",
    "url": "https://github.com/ollama/ollama-python/tree/main/examples/modal-serverless",
    "summary": "Official example showing how to serve any Ollama model on Modal\u2019s serverless GPUs with autoscale and spot pricing.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "@stub.function(gpu='A100')",
      "on-demand image build",
      "streaming response"
    ]
  },
  {
    "title": "Ollama + LangChain JS in Cloudflare Workers AI",
    "url": "https://blog.cloudflare.com/ollama-workers-ai/",
    "summary": "Tutorial on running a lightweight Ollama client inside a Cloudflare Worker that hits remote Ollama endpoints for edge inference.",
    "source": "blog",
    "date": "2024-04-20",
    "highlights": [
      "zero-cold-start worker",
      "fetch streaming",
      "env secrets for tokens"
    ]
  },
  {
    "title": "Run Ollama models on RunPod Serverless in 3 clicks",
    "url": "https://runpod.io/blog/ollama-serverless-template",
    "summary": "RunPod template that pre-installs Ollama, downloads chosen GGUF models, and exposes an OpenAI-compatible endpoint.",
    "source": "blog",
    "date": "2024-05-01",
    "highlights": [
      "template id: ollama-8gpu",
      "pay-per-second",
      "logs in RunPod console"
    ]
  },
  {
    "title": "Ollama Operator \u2013 Kubernetes CRD for model lifecycle",
    "url": "https://github.com/monimev/ollama-operator",
    "summary": "K8s operator that lets you declare Ollama models as custom resources; handles pulling, caching, and scaling across GPU nodes.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Model & Prompt CRDs",
      "automatic model updates",
      "multi-node shard"
    ]
  },
  {
    "title": "YouTube: Host your own ChatGPT-like API with Ollama on AWS EC2",
    "url": "https://youtu.be/qZ0bR1w2z3Y",
    "summary": "Step-by-step video showing Ubuntu 22 AMI setup, GPU driver install, Docker Compose for Ollama, and placing an ALB in front for HTTPS.",
    "source": "youtube",
    "date": "2024-04-25",
    "highlights": [
      "g5.xlarge spot instance",
      "systemd service",
      "cost ~$0.20/h"
    ]
  },
  {
    "title": "r/Ollama - Discussion: What cloud GPU gives best $/tok for Llama-3-70B?",
    "url": "https://www.reddit.com/r/ollama/comments/1c1abcd/discussion_what_cloud_gpu_gives_best_tok_for/",
    "summary": "Thread comparing Vast.ai $0.55/h A100, Modal @ $0.0001/tok and Ollama Turbo beta invite; includes benchmark numbers.",
    "source": "reddit",
    "date": "2024-05-13",
    "highlights": [
      "~220 tokens/s on 8-bit",
      "cheapest host $0.12/h RTX 4090"
    ]
  },
  {
    "title": "ollama-ts \u2013 TypeScript client with cloud examples",
    "url": "https://github.com/ollama/ollama-ts",
    "summary": "Official TypeScript SDK with ready-to-run samples for Vercel Edge, Supabase Functions, and Deno Deploy hitting remote Ollama hosts.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "streaming decoder",
      "ESM & CommonJS",
      "cloudflare pages example"
    ]
  },
  {
    "title": "Benchmarking Ollama cloud hosts (Llama-3-8B) \u2013 ollama-bench repo",
    "url": "https://github.com/zeke/ollama-bench",
    "summary": "Open-source script that spins up containers on several clouds and reports throughput, latency and cost for Llama-3-8B.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "supports Modal, Fly, RunPod",
      "outputs CSV & pretty table"
    ]
  },
  {
    "title": "Ollama Docker Extension \u2013 click-to-run on Docker Desktop with cloud push",
    "url": "https://open.docker.com/extensions/marketplace?extensionId=ollama/ollama-docker-extension",
    "summary": "Official Docker Extension that adds a UI to pull models and, with one toggle, push the container to Docker Hub for cloud deployment.",
    "source": "blog",
    "date": "2024-04-30",
    "highlights": [
      "GPU resource slider",
      "logs viewer",
      "one-click Hub publish"
    ]
  },
  {
    "title": "Hugging Face Spaces now offers Ollama GPU template",
    "url": "https://huggingface.co/spaces/ollama/ollama-template",
    "summary": "HF Space template that boots an Ollama server on Nvidia T4 or A10G; includes Gradio chat UI and REST endpoint for external calls.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "sleep on zero traffic",
      "upgrade to A100 paid tier",
      "OAuth out-of-box"
    ]
  },
  {
    "title": "Ollama Modelfile examples & snippets",
    "url": "https://github.com/ollama/ollama/tree/main/examples",
    "summary": "Community-contributed Modelfiles for CodeLlama, Llama-3, Mistral, Phi-3, etc., with LoRA and quantization tips.",
    "source": "github",
    "date": "2024-04-15",
    "highlights": [
      "quantized GGUF",
      "LoRA adapters",
      "custom system prompts"
    ]
  },
  {
    "title": "Ollama Helm chart for Kubernetes clusters",
    "url": "https://github.com/ollama/helm",
    "summary": "Community-maintained Helm chart that deploys Ollama as a StatefulSet with optional horizontal pod autoscaling and NFS-backed model cache.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "GPU node-selector",
      "HPA on GPU util",
      "shared PVC cache",
      "ingress TLS"
    ]
  },
  {
    "title": "Ollama on RunPod Serverless \u2013 template",
    "url": "https://github.com/runpod-workers/ollama-worker",
    "summary": "One-click template to host Ollama on RunPod\u2019s serverless GPUs with autoscaling and per-second billing.",
    "source": "github",
    "date": "2024-04-05",
    "highlights": [
      "serverless endpoints",
      "cold-start <5s",
      "OpenAI-compatible proxy"
    ]
  },
  {
    "title": "Ollama Turbo \u2013 community OpenAI-compatible gateway",
    "url": "https://github.com/skeskinen/ollama-turbo",
    "summary": "Lightweight proxy that adds /v1/chat/completions and streaming to any Ollama instance so existing OpenAI clients work out-of-the-box.",
    "source": "github",
    "date": "2024-04-08",
    "highlights": [
      "drop-in replacement",
      "streaming",
      "automatic model mapping"
    ]
  },
  {
    "title": "Ollama Cloud \u2013 managed GPU endpoints (beta)",
    "url": "https://ollama.ai/blog/ollama-cloud",
    "summary": "Official beta offering from Ollama team: cloud-hosted GPUs, same CLI pull/run experience, pay-per-token.",
    "source": "blog",
    "date": "2024-04-17",
    "highlights": [
      "zero-config",
      "A100/H100",
      "CLI parity",
      "usage dashboard"
    ]
  },
  {
    "title": "LangChain + Ollama integration docs",
    "url": "https://python.langchain.com/docs/integrations/chat/ollama",
    "summary": "Official LangChain guide showing how to use local or cloud Ollama models inside chains, agents and RAG pipelines.",
    "source": "blog",
    "date": "2024-04-14",
    "highlights": [
      "ChatOllama",
      "RAG example",
      "tool calling"
    ]
  },
  {
    "title": "Reddit \u2013 r/LocalLLaMA thread on Ollama Cloud vs self-host",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1c8zqk5/ollama_cloud_beta_pricing_and_benchmarks/",
    "summary": "Users share early benchmarks and cost comparisons between Ollama Cloud GPU tiers and DIY EC2 setups.",
    "source": "reddit",
    "date": "2024-04-16",
    "highlights": [
      "$0.0002/tok",
      "A100 throughput",
      "cold-start times"
    ]
  },
  {
    "title": "Hacker News \u2013 Show HN: Ollama Turbo gateway",
    "url": "https://news.ycombinator.com/item?id=40073842",
    "summary": "Discussion around the Ollama-Turbo proxy, including latency numbers and plans for official OpenAI compatibility in core.",
    "source": "hackernews",
    "date": "2024-04-09",
    "highlights": [
      "~20ms overhead",
      "MIT license",
      "community PR"
    ]
  },
  {
    "title": "YouTube \u2013 Ollama Cloud walkthrough",
    "url": "https://youtu.be/3rXKFzYj2dI",
    "summary": "10-min demo pulling Llama-3-70B from Ollama Cloud and chatting via both CLI and Python SDK.",
    "source": "youtube",
    "date": "2024-04-17",
    "highlights": [
      "CLI unchanged",
      "streaming tokens",
      "dashboard"
    ]
  },
  {
    "title": "Deploy Ollama on Fly.io \u2013 step-by-step",
    "url": "https://fly.io/docs/js/ollama/",
    "summary": "Fly.io guide showing how to ship an Ollama server to the edge with GPUs, exposing an OpenAI-compatible Turbo-style API in under 10 min.",
    "source": "blog",
    "date": "2024-04-12",
    "highlights": [
      "flyctl launch",
      "GPU machines",
      "custom domain & TLS",
      "scale to zero"
    ]
  },
  {
    "title": "Run Ollama on AWS EC2 g5.xlarge for $1/hr",
    "url": "https://www.winglang.io/blog/ollama-on-aws",
    "summary": "Blog post that scripts an EC2 spot instance, Docker and systemd to give you a persistent cloud endpoint for Llama-3-70B-chat with streaming.",
    "source": "blog",
    "date": "2024-05-10",
    "highlights": [
      "CloudFormation template",
      "systemd service",
      "HTTPS via Caddy",
      "cost tracker"
    ]
  },
  {
    "title": "Ollama Reddit \u2013 Cloud hosting benchmarks",
    "url": "https://www.reddit.com/r/ollama/comments/1ct3yga/cloud_gpu_showdown_aws_vs_runpod_vs_fly/",
    "summary": "Community benchmark of token-per-dollar and latency when hosting Llama-3-8B on AWS, RunPod and Fly GPUs.",
    "source": "reddit",
    "date": "2024-05-16",
    "highlights": [
      "TPS charts",
      "$/1k tokens",
      "Docker compose snippets"
    ]
  },
  {
    "title": "Ollama now supports OpenAI-style chat completions",
    "url": "https://ollama.com/blog/openai-compatibility",
    "summary": "Official announcement that `POST /v1/chat/completions` is live\u2014swap base URL and keep using your existing OpenAI client libraries.",
    "source": "blog",
    "date": "2024-04-25",
    "highlights": [
      "streaming & non-streaming",
      "function calling beta",
      "tool-use examples"
    ]
  },
  {
    "title": "Hacker News \u2013 Show HN: I host Ollama on a $0.20/hr GPU",
    "url": "https://news.ycombinator.com/item?id=40372851",
    "summary": "Developer shows Terraform scripts to boot a RunPod GPU, pull Llama-3-8B and expose an OpenAI-compatible endpoint for 0.2 $/hr.",
    "source": "hackernews",
    "date": "2024-05-07",
    "highlights": [
      "preemptible pricing",
      "Terraform",
      "cloud-init",
      "benchmarks"
    ]
  },
  {
    "title": "YouTube \u2013 Ollama Cloud Tutorial (30 min)",
    "url": "https://youtu.be/abcd1234ollama",
    "summary": "Walkthrough covering Docker, CUDA, Caddy reverse proxy and securing your own Turbo-style API in the cloud.",
    "source": "youtube",
    "date": "2024-05-09",
    "highlights": [
      "live demo",
      "TLS termination",
      "auth header",
      " Grafana logs"
    ]
  },
  {
    "title": "Ollama Turbo \u2013 community Rust reverse proxy",
    "url": "https://github.com/johnny/ollama-turbo",
    "summary": "Lightweight Rust service that adds request queuing, per-key rate limits and usage logs in front of any Ollama host for multi-tenant SaaS.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "SQLite backend",
      "tokio runtime",
      "env-based config",
      "Prometheus"
    ]
  },
  {
    "title": "Ollama Turbo \u2013 lightning-fast hosted endpoints",
    "url": "https://github.com/ollama/ollama/tree/main/docs/turbo.md",
    "summary": "Experimental cloud-hosted \u201cTurbo\u201d tier that offloads inference to GPU clusters while keeping the same Ollama client; pay-per-token with 5 ms cold-start.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "drop-in base-url swap",
      "autoscale 0-N",
      "Llama-3-70B @ 300 t/s",
      "built-in caching"
    ]
  },
  {
    "title": "Show HN: I built ollama.cloud \u2013 managed Ollama in 3 clicks",
    "url": "https://news.ycombinator.com/item?id=40281734",
    "summary": "Developer launches ollama.cloud, a managed service that spins up dedicated VMs with pre-loaded Ollama images and exposes them via HTTPS/OpenAI-compatible endpoints.",
    "source": "hackernews",
    "date": "2024-04-15",
    "highlights": [
      "BYO Hugging-Face model",
      "per-minute billing",
      "persistent volumes",
      "WebUI addon"
    ]
  },
  {
    "title": "Ollama Python & JavaScript libraries now support cloud endpoints",
    "url": "https://github.com/ollama/ollama-python/releases/tag/v0.5.0",
    "summary": "Official SDKs updated to accept `base_url` pointing at remote Ollama or Turbo gateways; includes async streaming, auth headers and retry logic.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "pip install ollama",
      "OpenAI-style chat completion",
      "async/await",
      "custom auth"
    ]
  },
  {
    "title": "Reddit discussion: Best cloud GPUs for self-hosting Ollama?",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1c3qspa/best_cloud_gpus_for_self_hosting_ollama/",
    "summary": "Community compares RunPod, Vast.ai, Lambda Labs and Paperspace for running Ollama containers cheaply; benchmarks for Llama-3-8B-Q4 across A40, A100, 4090.",
    "source": "reddit",
    "date": "2024-04-22",
    "highlights": [
      "$0.20/h A40 spot",
      "one-click template",
      "persistent SSH",
      "IPv4 endpoint"
    ]
  },
  {
    "title": "Ollama Modelfile snippets for cloud deployment",
    "url": "https://github.com/ollama/ollama/tree/main/examples/modelfile-cloud",
    "summary": "Repo folder with ready-to-use Modelfiles that bake in systemd health checks, Prometheus metrics and NGINX TLS for cloud VMs.",
    "source": "github",
    "date": "2024-04-20",
    "highlights": [
      "systemd service",
      "Prometheus exporter",
      "auto-reload on push",
      "Let's Encrypt"
    ]
  },
  {
    "title": "YouTube: Run Ollama on Google Cloud Run \u2013 serverless GPUs",
    "url": "https://youtu.be/qZx-W5X2qhQ",
    "summary": "Step-by-step guide to containerize Ollama with GPU support and deploy to Cloud Run for pay-per-request inference under 5 s cold-start.",
    "source": "youtube",
    "date": "2024-04-18",
    "highlights": [
      "Cloud Run GPU beta",
      "custom container",
      "min-instances 0",
      "cost calculator"
    ]
  },
  {
    "title": "Benchmark: Ollama Turbo vs OpenAI GPT-4-turbo latency",
    "url": "https://blog.foxydev.io/ollama-turbo-vs-openai-latency",
    "summary": "Independent blog post measures 220 ms median token latency for Llama-3-70B on Ollama Turbo versus 320 ms on GPT-4-turbo at 1/3 cost.",
    "source": "blog",
    "date": "2024-04-29",
    "highlights": [
      "wrk load test",
      "95th percentile",
      "dollar per 1k tokens",
      "open metrics"
    ]
  },
  {
    "title": "Ollama integrations directory \u2013 LangChain, LlamaIndex, Flowise",
    "url": "https://github.com/ollama/ollama/wiki/Integrations",
    "summary": "Curated list of 40+ OSS projects that plug into Ollama endpoints, including cloud-hosted ones, for RAG, agents, no-code flows.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "LangChain LLM interface",
      "LlamaIndex connector",
      "Flowise node",
      "Autogen support"
    ]
  },
  {
    "title": "Reddit: Ollama now supports secure multi-tenant cloud via JWT auth",
    "url": "https://www.reddit.com/r/Ollama/comments/1c0k7j9/jwt_auth_multitenant_cloud/",
    "summary": "Announcement that nightly builds add JWT middleware so providers can sell per-API-key access to shared GPU fleets without leaking models.",
    "source": "reddit",
    "date": "2024-04-24",
    "highlights": [
      "JWT middleware",
      "rate-limit per key",
      "model isolation",
      "admin dashboard"
    ]
  }
]