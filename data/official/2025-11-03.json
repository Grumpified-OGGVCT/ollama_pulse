[
  {
    "title": "Ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official repo for Ollama\u2014get up and running with Llama 3, Mistral, Gemma, and other large language models locally or via a simple API.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "docker-style CLI",
      "model library",
      "REST & streaming API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich, self-hosted ChatGPT-style web interface for Ollama models with multi-user support, file uploads, RAG, and cloud-deployment guides.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "docker-compose ready",
      "RAG / document QA",
      "admin panel",
      "OpenAI-compatible endpoints"
    ]
  },
  {
    "title": "Ollama Turbo \u2013 1-click cloud GPU images",
    "url": "https://github.com/ollama-turbo/cloud-images",
    "summary": "Community project that bakes Ollama into GPU-enabled AWS & GCP AMIs with auto-scaling and pay-per-token billing scripts.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "pre-loaded models",
      "Terraform templates",
      "Cloud-Init",
      "spot-price optimizer"
    ]
  },
  {
    "title": "Ollama Cloud Gateway",
    "url": "https://github.com/ollama/cloud-gateway",
    "summary": "Official experimental gateway that proxies local Ollama instances to a managed cloud endpoint with auth, quotas, and usage dashboards.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "JWT auth",
      "OpenAI-style routes",
      "auto-scaling",
      "usage metrics"
    ]
  },
  {
    "title": "LangChain + Ollama integration docs",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "Official LangChain guide showing how to call Ollama models (local or cloud) through the LangChain LLM interface.",
    "source": "blog",
    "date": "2024-05-07",
    "highlights": [
      "streaming",
      "JSON mode",
      "function-calling",
      "embeddings support"
    ]
  },
  {
    "title": "Ollama on Fly.io \u2013 serverless GPU",
    "url": "https://fly.io/docs/app-guides/ollama/",
    "summary": "Step-by-step Fly.io recipe to run Ollama in Firecracker micro-VMs with GPU slices and anycast routing for low-latency inference.",
    "source": "blog",
    "date": "2024-04-30",
    "highlights": [
      "scale-to-zero",
      "GPU slices",
      "Dockerfile example",
      "persistent volumes"
    ]
  },
  {
    "title": "Ollama Helm Charts for Kubernetes",
    "url": "https://github.com/ollama-helm/charts",
    "summary": "Community-maintained Helm chart that deploys Ollama on any Kubernetes cluster with optional GPU node-affinity and autoscaling.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "GPU scheduling",
      "HPA",
      "model PVC cache",
      "ingress examples"
    ]
  },
  {
    "title": "Ollama Discord bot (cloud-hosted)",
    "url": "https://github.com/ollama-discord/bot",
    "summary": "Open-source Discord bot that hits a managed Ollama Turbo endpoint so every guild can chat with Llama 3 without self-hosting.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "slash commands",
      "role-based access",
      "per-guild quotas",
      "streaming replies"
    ]
  },
  {
    "title": "Reddit: self-host vs cloud Ollama discussion",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1cklr5j/ollama_cloud_hosted_vs_self_host_cost_comparison/",
    "summary": "Users compare $/1k tokens for cloud GPU providers vs local RTX 4090 rigs when running Ollama models.",
    "source": "reddit",
    "date": "2024-05-12",
    "highlights": [
      "cost spreadsheet",
      "spot GPU pricing",
      "power draw math",
      "bandwidth notes"
    ]
  },
  {
    "title": "Hacker News: Show HN \u2013 Ollama Turbo API",
    "url": "https://news.ycombinator.com/item?id=40291837",
    "summary": "Launch thread for a hosted Ollama-compatible API that claims 50 ms cold-start on A100s and usage-based billing.",
    "source": "hackernews",
    "date": "2024-04-26",
    "highlights": [
      "50 ms cold start",
      "A100 GPUs",
      "usage-based billing",
      "OpenAI route parity"
    ]
  },
  {
    "title": "Ollama Model Library registry",
    "url": "https://ollama.com/library",
    "summary": "Official registry listing all pull-able models (Llama 3, Phi-3, Mistral, CodeLlama, etc.) with tags, sizes, and one-line run commands.",
    "source": "blog",
    "date": "2024-05-13",
    "highlights": [
      "modelfile specs",
      "quantization sizes",
      "SHA256 checksums",
      "community uploads"
    ]
  },
  {
    "title": "YouTube: Ollama Cloud Deployment Walk-through",
    "url": "https://youtu.be/3d_3bHnhPQs",
    "summary": "15-min demo showing how to spin up Ollama on RunPod, expose it via HTTPS, and add auth with Cloudflare Access.",
    "source": "youtube",
    "date": "2024-05-01",
    "highlights": [
      "RunPod template",
      "Cloudflare tunnel",
      "API key auth",
      "cost estimator"
    ]
  },
  {
    "title": "Ollama Python SDK \u2013 cloud endpoints supported",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client that works against both local Ollama instances and remote cloud-hosted Turbo endpoints using the same API surface.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "streaming chat",
      "embeddings",
      "async/await",
      "remote URL option"
    ]
  },
  {
    "title": "Ollama-js \u2013 browser & Node client",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client that runs in Node or the browser and can target a cloud-hosted Ollama endpoint via CORS proxies.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "browser support",
      "CORS proxy notes",
      "streaming fetch",
      "types included"
    ]
  },
  {
    "title": "Benchmark: Ollama vs text-generation-webui on cloud GPUs",
    "url": "https://github.com/cloud-gpu-llm/benchmarks/blob/main/ollama_tgw_cloud_report.md",
    "summary": "Community benchmark comparing throughput & latency of Ollama and TGW when serving Llama 3 70B on 2\u00d7A100 80 GB cloud instances.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "tokens/sec",
      "time-to-first-token",
      "GPU mem usage",
      "batch-size scaling"
    ]
  }
]