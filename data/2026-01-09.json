[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: self-hosted LLM runner with a REST & CLI API that lets you pull, run and chat with 70+ open-source models (Llama 3, Mistral, Gemma, Phi \u2026) locally on macOS, Linux and Windows.",
    "source": "github",
    "date": "2024-06-01",
    "highlights": [
      "pull/run any GGUF model",
      "built-in model library",
      "REST/CLI API",
      "OpenAI-compatible endpoints",
      "GPU/CPU fallback"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "First-party Python client for Ollama; chat, generate, embed, list, pull, push models with a few lines of code.",
    "source": "github",
    "date": "2024-05-30",
    "highlights": [
      "sync & async APIs",
      "streaming support",
      "built-in embeddings",
      "PyPI: ollama"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript SDK for Node & browsers; same generate/chat/embed API surface as the Python client.",
    "source": "github",
    "date": "2024-05-30",
    "highlights": [
      "npm: ollama",
      "TypeScript types",
      "streaming",
      "ESM & CommonJS"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain adapter that turns any Ollama model into a LangChain LLM or embeddings provider.",
    "source": "github",
    "date": "2024-05-29",
    "highlights": [
      "pip: langchain-ollama",
      "chat & embeddings",
      "tool calling",
      "batch support"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama comparable to ChatGPT: folders, presets, RAG, multi-user, dark mode, mobile ready.",
    "source": "github",
    "date": "2024-05-31",
    "highlights": [
      "Docker one-liner",
      "OpenAI-compatible proxy",
      "document upload & RAG",
      "role-based access"
    ]
  },
  {
    "title": "Open WebUI (formerly Ollama WebUI)",
    "url": "https://github.com/open-webui/open-webui",
    "summary": "Successor repo to ollama-webui; adds plugins, voice input, multimodal vision, admin panel and pip install.",
    "source": "github",
    "date": "2024-06-02",
    "highlights": [
      "pip: open-webui",
      "plugin marketplace",
      "whisper voice",
      "vision models",
      "LDAP/OAuth"
    ]
  },
  {
    "title": "ollama4j \u2013 Java client for Ollama",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Idiomatic Java/Kotlin library exposing all Ollama endpoints with fluent builders and reactive streaming.",
    "source": "github",
    "date": "2024-05-25",
    "highlights": [
      "Maven Central",
      "Kotlin coroutines",
      "chat, generate, pull, list, delete"
    ]
  },
  {
    "title": "ollama-rb \u2013 Ruby gem",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Community-maintained Ruby wrapper for Ollama; supports streaming, embeddings and model management.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "gem install ollama",
      "Fiber-based streaming",
      "Rails-friendly"
    ]
  },
  {
    "title": "ollama-cli \u2013 Rust CLI",
    "url": "https://github.com/sammcj/ollama-cli",
    "summary": "Fast Rust CLI for chatting, pulling and benchmarking Ollama models; supports JSON mode and batch prompts.",
    "source": "github",
    "date": "2024-05-22",
    "highlights": [
      "cargo install ollama-cli",
      "syntax highlighting",
      "built-in bench"
    ]
  },
  {
    "title": "ollama-copilot \u2013 VS Code extension",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "Bring any Ollama model into VS Code as an inline coding assistant with autocomplete and chat sidebar.",
    "source": "github",
    "date": "2024-05-28",
    "highlights": [
      "inline completions",
      "chat panel",
      "custom prompts",
      "multi-model"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU autoscaling and PVC model cache.",
    "source": "github",
    "date": "2024-05-27",
    "highlights": [
      "Helm repo",
      "GPU nvidia-plugin",
      "model init container",
      "ingress"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama-docker-compose",
    "summary": "Official Docker Compose stacks for CPU & GPU setups plus examples with Open WebUI, LiteLLM, etc.",
    "source": "github",
    "date": "2024-05-30",
    "highlights": [
      "docker-compose.yml",
      "CUDA runtime",
      "Open WebUI bundled",
      "volume cache"
    ]
  },
  {
    "title": "ollama-litellm-proxy",
    "url": "https://github.com/BerriAI/litellm/tree/main/proxy_server#ollama",
    "summary": "LiteLLM proxy can expose any Ollama model as a drop-in OpenAI-compatible endpoint for production apps.",
    "source": "github",
    "date": "2024-05-26",
    "highlights": [
      "pip: litellm",
      "/v1/chat/completions",
      "load balancing",
      "key management"
    ]
  },
  {
    "title": "ollama-gui-tools (Electron desktop apps)",
    "url": "https://github.com/jmorganca/ollama-gui-tools",
    "summary": "Collection of lightweight Electron, Tauri and Flutter desktop front-ends for Ollama.",
    "source": "github",
    "date": "2024-05-21",
    "highlights": [
      "offline first",
      "macOS/Win/Linux builds",
      "system tray"
    ]
  },
  {
    "title": "ollama-rag \u2013 Document Q&A starter",
    "url": "https://github.com/ollama/ollama-rag",
    "summary": "Minimal Python repo showing how to combine Ollama embeddings + Chroma to build a private RAG chatbot.",
    "source": "github",
    "date": "2024-05-24",
    "highlights": [
      "PDF ingestion",
      "Chroma vector store",
      "streamlit UI",
      "pip install"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/ollama-discord/ollama-discord-bot",
    "summary": "Self-hostable Discord bot that brings any Ollama model into your server with slash commands and threads.",
    "source": "github",
    "date": "2024-05-23",
    "highlights": [
      "Docker image",
      "threading",
      "mod tools",
      "custom system prompts"
    ]
  },
  {
    "title": "ollama-function-calling",
    "url": "https://github.com/ollama/ollama-function-calling",
    "summary": "Experimental repo demonstrating JSON-mode tool use with Mistral & Llama 3 models via Ollama.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "pydantic schemas",
      "automatic retries",
      "OpenAI-style tools"
    ]
  },
  {
    "title": "ollama-obsidian-plugin",
    "url": "https://github.com/hinterdupfinger/ollama-obsidian",
    "summary": "Obsidian community plugin that lets you run local LLMs on your vault notes for summarization & Q&A.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "offline",
      "custom commands",
      "template prompts"
    ]
  },
  {
    "title": "ollama-nix \u2013 Nix flake",
    "url": "https://github.com/ollama-nix/ollama-nix",
    "summary": "Reproducible Nix flake for Ollama with CUDA and ROCm variants; includes module for NixOS.",
    "source": "github",
    "date": "2024-05-17",
    "highlights": [
      "cache.nixos.org",
      "ROCm support",
      "NixOS service"
    ]
  },
  {
    "title": "ollama-reddit-community",
    "url": "https://www.reddit.com/r/ollama/",
    "summary": "Active subreddit for sharing models, troubleshooting, integrations and showcasing Ollama projects.",
    "source": "reddit",
    "date": "2024-06-01",
    "highlights": [
      "show-and-tell",
      "model requests",
      "performance tips"
    ]
  }
]