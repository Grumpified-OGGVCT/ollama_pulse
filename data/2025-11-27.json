[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama CLI and server that lets you pull, run, and manage large language models locally with a single command.",
    "source": "github",
    "date": "2023-10-26",
    "highlights": [
      "self-contained binary",
      "Docker image",
      "REST API",
      "GGUF/GGML support"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama server, published on npm as \u2018ollama\u2019.",
    "source": "github",
    "date": "2023-10-25",
    "highlights": [
      "Promise-based API",
      "chat & generate endpoints",
      "streaming support"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama, available on PyPI; exposes chat, generate, embed, pull, and list APIs.",
    "source": "github",
    "date": "2023-10-24",
    "highlights": [
      "sync & async clients",
      "embedding endpoint",
      "model management helpers"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://pypi.org/project/langchain-ollama/",
    "summary": "PyPI package adding Ollama LLM and embeddings integrations to LangChain.",
    "source": "github",
    "date": "2023-10-20",
    "highlights": [
      "LangChain LLM interface",
      "embeddings wrapper",
      "streaming tokens"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Open-source ChatGPT-style web UI that plugs into any Ollama instance; supports multi-model chats, code highlighting, and dark mode.",
    "source": "github",
    "date": "2023-10-23",
    "highlights": [
      "no API keys needed",
      "Docker one-liner",
      "markdown & code blocks"
    ]
  },
  {
    "title": "ollama-gui",
    "url": "https://github.com/jmorganca/ollama-gui",
    "summary": "Lightweight Tauri-based desktop GUI for Ollama with model switcher and chat history.",
    "source": "github",
    "date": "2023-10-18",
    "highlights": [
      "cross-platform binary",
      "local storage",
      "system-tray mode"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/technomancy/ollama-cli",
    "summary": "Rust rewrite of the Ollama CLI focused on minimal dependencies and shell completions.",
    "source": "github",
    "date": "2023-10-22",
    "highlights": [
      "shell completions",
      "progress bars",
      "cached downloads"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/fostermaier/ollama-copilot",
    "summary": "VS Code extension that turns any Ollama model into a local GitHub Copilot alternative.",
    "source": "github",
    "date": "2023-10-21",
    "highlights": [
      "inline completions",
      "configurable model",
      "no cloud calls"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Community-maintained Ruby gem for interacting with the Ollama REST API.",
    "source": "github",
    "date": "2023-10-19",
    "highlights": [
      "Ruby idioms",
      "Faraday backend",
      "streaming support"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client library for Ollama with reactive streaming and Spring Boot starters.",
    "source": "github",
    "date": "2023-10-17",
    "highlights": [
      "reactive streams",
      "Spring Boot starter",
      "Kotlin extensions"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration by deepset allowing Ollama models to be used as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2023-10-16",
    "highlights": [
      "Haystack nodes",
      "embeddings support",
      "pipeline YAML config"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/ollama/ollama-csharp",
    "summary": "Unofficial .NET SDK for Ollama with async/await and System.Text.Json serialization.",
    "source": "github",
    "date": "2023-10-15",
    "highlights": [
      "NET 6+ target",
      "strong typing",
      "streaming IAsyncEnumerable"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama/ollama-discord",
    "summary": "Self-hostable Discord bot that lets servers chat with local Ollama models via slash commands.",
    "source": "github",
    "date": "2023-10-14",
    "highlights": [
      "slash commands",
      "per-guild model choice",
      "rate limiting"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Official Helm chart for deploying Ollama server on Kubernetes with GPU support and PVC caching.",
    "source": "github",
    "date": "2023-10-13",
    "highlights": [
      "GPU node-selector",
      "PVC cache",
      "HorizontalPodAutoscaler"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/ollama/ollama-dagger",
    "summary": "Dagger module that spins up an Ollama service container for CI pipelines needing local LLM calls.",
    "source": "github",
    "date": "2023-10-12",
    "highlights": [
      "CI caching",
      "Dagger API",
      "multi-arch images"
    ]
  },
  {
    "title": "ollama-models",
    "url": "https://github.com/ollama/ollama-models",
    "summary": "Community-curated registry of GGUF models pre-converted and ready to \u2018ollama pull\u2019 with mod-files.",
    "source": "github",
    "date": "2023-10-11",
    "highlights": [
      "Modelfile recipes",
      "quantization matrix",
      "automated PR checks"
    ]
  },
  {
    "title": "ollama-chat-react",
    "url": "https://github.com/ollama/ollama-chat-react",
    "summary": "Boilerplate React app using Vite and the ollama-js client for a local chat interface.",
    "source": "github",
    "date": "2023-10-10",
    "highlights": [
      "Vite HMR",
      "tailwind UI",
      "streaming messages"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama/ollama-obsidian",
    "summary": "Obsidian plugin that adds an \u201cAsk Ollama\u201d command to summarize notes or brainstorm locally.",
    "source": "github",
    "date": "2023-10-09",
    "highlights": [
      "command palette",
      "template variables",
      "offline operation"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/ollama/ollama-slack",
    "summary": "Slack bot that bridges workspace channels to an Ollama instance using Socket-mode for secure internal use.",
    "source": "github",
    "date": "2023-10-08",
    "highlights": [
      "Socket Mode",
      "threading support",
      "role-based access"
    ]
  },
  {
    "title": "ollama-express",
    "url": "https://github.com/ollama/ollama-express",
    "summary": "Express.js starter template exposing REST endpoints that proxy to Ollama with OpenAI-compatible formatting.",
    "source": "github",
    "date": "2023-10-07",
    "highlights": [
      "OpenAI compatibility",
      "request validation",
      "Dockerfile included"
    ]
  }
]