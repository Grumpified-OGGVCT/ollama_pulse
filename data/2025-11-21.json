[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: get up and running with Llama 2, Mistral, Gemma, and other large language models locally.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "self-contained CLI",
      "macOS/Linux/Windows",
      "Docker images",
      "model library"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama. Chat, embed, generate completions, and pull/manage models programmatically.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip install ollama",
      "async/sync APIs",
      "embedding support",
      "model management"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript SDK for Ollama. Works in Node, Bun, Deno and browsers via fetch.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "npm i ollama",
      "TypeScript types",
      "streaming completions",
      "browser compatible"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration for Ollama. Use local models as LLMs, embedders, or tool-calling agents.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install langchain-ollama",
      "tool calling",
      "RAG pipelines",
      "agent support"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (renamed to Open WebUI). ChatGPT-style interface, model manager, RAG, admin panel.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "Docker one-liner",
      "RAG uploads",
      "multi-user",
      "dark/light themes",
      "plugins"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama. Async, streaming, and embedding APIs with POJO mapping.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Maven Central",
      "Kotlin coroutines",
      "streaming chat",
      "embedding utils"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/saltyorg/ollama-cli",
    "summary": "Community Rust CLI that wraps the Ollama REST API with ergonomic commands and config files.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Rust binary",
      "config profiles",
      "shell completions",
      "cross-platform"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Experimental GitHub Copilot-like plugin that uses Ollama local models for inline code suggestions.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "VS Code extension",
      "inline completions",
      "configurable model",
      "offline"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama/ollama-rag",
    "summary": "Minimal RAG template using Ollama embeddings + Chroma + Streamlit. Drop PDFs and chat locally.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Streamlit UI",
      "Chroma vector DB",
      "PDF ingestion",
      "docker-compose"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama/ollama-discord",
    "summary": "Discord bot that brings Ollama models to servers with slash commands, threads, and mod tools.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "slash commands",
      "model switching",
      "thread isolation",
      "rate limiting"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/helm",
    "summary": "Official Helm chart for deploying Ollama on Kubernetes with GPU support and PVC model cache.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "GPU nodeSelector",
      "model init containers",
      "ingress",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/ollama/dagger",
    "summary": "Dagger module that spins up Ollama as a CI service for testing LLM-powered features in pipelines.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "CI cache",
      "model pre-pull",
      "Dagger SDK",
      "GitHub Actions example"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/ollama/nix",
    "summary": "Nix flake providing Ollama binaries, CUDA variants, and NixOS module with systemd service.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "CUDA overlay",
      "NixOS module",
      "binary cache",
      "declarative models"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/ollama/ollama-slack",
    "summary": "Slack Bolt app that adds /ollama slash command to query local models with per-channel defaults.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Bolt JS",
      "channel config",
      "ephemeral responses",
      "streaming"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama/ollama-obsidian",
    "summary": "Obsidian plugin for local AI autocomplete, summarization, and note Q&A via Ollama.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "editor suggest",
      "command palette",
      "template prompts",
      "offline"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration by deepset. Use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "pip install ollama-haystack",
      "pipeline nodes",
      "embedding support",
      "example notebooks"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama/ollama-rust",
    "summary": "Community Rust crate with async client, SSE streaming, and typed request/response structs.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "crates.io",
      "tokio runtime",
      "typed DTOs",
      "examples"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/ollama/ollama-csharp",
    "summary": "Community .NET SDK for Ollama with IAsyncEnumerable streaming and dependency-injection helpers.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "NuGet package",
      "DI extensions",
      "streaming completions",
      "embedding API"
    ]
  },
  {
    "title": "ollama-fastapi",
    "url": "https://github.com/ollama/ollama-fastapi",
    "summary": "FastAPI service that wraps Ollama with OpenAI-compatible endpoints for drop-in replacement.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "/v1/chat/completions",
      "Docker image",
      "env config",
      "streaming"
    ]
  },
  {
    "title": "ollama-spring-boot-starter",
    "url": "https://github.com/ollama/ollama-spring-boot-starter",
    "summary": "Spring Boot starter auto-configuring Ollama REST client with @OllamaTemplate for POJO chat bots.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "Maven Central",
      "auto-configuration",
      "annotation-driven",
      "actuator metrics"
    ]
  }
]