[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and Python/JavaScript libraries to pull and run Llama 2, Mistral, Gemma, and 40+ other models locally with GPU/CPU acceleration and a built-in model registry.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "self-hosted",
      "40+ quantized models",
      "OpenAI-compatible API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "First-party Python client that wraps the Ollama REST API; supports sync/async, streaming, chat, embeddings and custom model creation.",
    "source": "github",
    "date": "2024-05-18",
    "highlights": [
      "pip install ollama",
      "asyncio support",
      "embeddings endpoint",
      "built-in modelfile helpers"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official TypeScript/JavaScript SDK for Node & browsers; offers Promise-based chat, generate, pull, delete and embedding calls with full streaming support.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "npm i ollama",
      "browser & node",
      "type definitions",
      "streaming JSON"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain integration that turns any local Ollama model into a drop-in LLM for chains, agents, retrieval QA and tool-calling workflows.",
    "source": "blog",
    "date": "2024-04-30",
    "highlights": [
      "pip install langchain-ollama",
      "tool calling",
      "retrieval QA",
      "agent executor"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web dashboard (formerly Ollama-WebUI) for chatting, model management, multi-user auth, RAG uploads and OpenAI-compatible endpoints.",
    "source": "github",
    "date": "2024-05-21",
    "highlights": [
      "docker run",
      "rag file upload",
      "multi-user",
      "dark/light themes",
      "OpenAI drop-in"
    ]
  },
  {
    "title": "ollama4j \u2013 Java/Kotlin client",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Community Java/Kotlin SDK exposing chat, generate, pull, embed and model listing with reactive streaming and Spring Boot starters.",
    "source": "github",
    "date": "2024-05-15",
    "highlights": [
      "maven central",
      "kotlin coroutines",
      "spring-boot-starter",
      "streaming"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Lightweight Rust crate providing strongly-typed async wrappers for chat, completion, embeddings and model management via the Ollama REST API.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "crates.io",
      "tokio async",
      "serde types",
      "embedding support"
    ]
  },
  {
    "title": "ollama-cli (npm)",
    "url": "https://www.npmjs.com/package/ollama-cli",
    "summary": "Community-built Node CLI offering interactive chat, model search, one-click pull and colorful terminal UI on top of the Ollama REST API.",
    "source": "npm",
    "date": "2024-05-12",
    "highlights": [
      "npm i -g ollama-cli",
      "interactive REPL",
      "model search",
      "colors"
    ]
  },
  {
    "title": "ollama-copilot (VS Code)",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "VS Code extension that replaces GitHub Copilot with any local Ollama model for inline suggestions, chat sidebar and customizable prompts.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "vs-code marketplace",
      "inline completions",
      "chat sidebar",
      "custom prompts"
    ]
  },
  {
    "title": "ollama-codellama-vscode",
    "url": "https://github.com/jmorganca/ollama-codellama-vscode",
    "summary": "Lightweight VS Code plugin providing CodeLlama autocomplete via Ollama with configurable stop sequences and infill formatting.",
    "source": "github",
    "date": "2024-04-22",
    "highlights": [
      "code autocomplete",
      "infill mode",
      "stop sequences",
      "lightweight"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for deploying Ollama on Kubernetes with GPU node-selector, PVC model cache and optional web-ui sidecar.",
    "source": "github",
    "date": "2024-05-19",
    "highlights": [
      "helm install",
      "gpu nodes",
      "pvc cache",
      "web-ui sidecar"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official multi-stage Dockerfile and docker-compose snippets for CPU & CUDA images with automatic model volume caching.",
    "source": "github",
    "date": "2024-05-20",
    "highlights": [
      "official image",
      "cuda",
      "compose snippets",
      "volume cache"
    ]
  },
  {
    "title": "ollama-rag-chatbot (Streamlit)",
    "url": "https://github.com/langchain-ai/streamlit-ollama-rag",
    "summary": "Streamlit template showing how to build a private RAG chatbot by combining Ollama embeddings, Chroma vector store and LangChain retrieval.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "streamlit",
      "chroma db",
      "private rag",
      "pdf loader"
    ]
  },
  {
    "title": "ollama-model-library",
    "url": "https://github.com/ollama/ollama/tree/main/models",
    "summary": "Curated collection of 40+ community-contributed Modelfiles for Llama 3, Phi-3, Mistral, Gemma, CodeLlama with quantization and template presets.",
    "source": "github",
    "date": "2024-05-21",
    "highlights": [
      "modelfiles",
      "quantized",
      "templates",
      "community"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/ollama-discord/ollama-discord-bot",
    "summary": "Self-hostable Discord bot that brings any Ollama model into servers via slash commands, thread isolation and optional admin-only model switching.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "slash commands",
      "thread isolation",
      "admin switch",
      "self-hosted"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/superatomic/ollama-slack-bot",
    "summary": "Lightweight Slack Bolt app that answers DMs and mentions with streaming responses from a local Ollama instance and supports model hot-swap.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "bolt js",
      "streaming",
      "dm support",
      "hot-swap"
    ]
  },
  {
    "title": "ollama-rag-api",
    "url": "https://github.com/jerryjliu/ollama-rag-api",
    "summary": "FastAPI micro-template exposing a /chat endpoint that uses Ollama for embeddings + generation and LlamaIndex for retrieval over uploaded documents.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "fastapi",
      "llamaindex",
      "/chat endpoint",
      "file upload"
    ]
  },
  {
    "title": "ollama-gui (Tauri desktop)",
    "url": "https://github.com/ollama-gui/ollama-gui",
    "summary": "Cross-platform desktop app built with Tauri+React for chatting with Ollama models offline, includes GPU monitoring and markdown rendering.",
    "source": "github",
    "date": "2024-05-16",
    "highlights": [
      "tauri",
      "offline",
      "gpu monitor",
      "markdown"
    ]
  },
  {
    "title": "ollama-nim (Nim client)",
    "url": "https://github.com/ire4ever1190/ollama-nim",
    "summary": "Nim library providing async HTTP wrappers for chat, generate, pull and embeddings, plus a CLI example compiled to static binary.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "nimble",
      "async",
      "static binary",
      "embeddings"
    ]
  },
  {
    "title": "ollama-rustico (Rust CLI)",
    "url": "https://github.com/pepperoni21/ollama-rustico",
    "summary": "Minimal Rust CLI that uses the ollama-rust crate to deliver colorful chat REPL with history, model switching and keyboard shortcuts.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "rust cli",
      "repl",
      "history",
      "shortcuts"
    ]
  }
]