[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Ollama\u2019s official CLI and server for running Llama 2, Mistral, Gemma, and other models locally with one command.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "self-contained binary",
      "Docker image",
      "OpenAI-compatible API",
      "model library"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama; chat, generate, embed, pull, and manage models from any Python script.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pip install ollama",
      "async/await support",
      "embedding helpers",
      "streaming responses"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node & browsers; same generate/chat/embed API as the Python SDK.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "npm i ollama",
      "TypeScript types",
      "streaming fetch",
      "browser compatible"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/langchain-ollama",
    "summary": "LangChain integration letting you swap Ollama models into any LangChain chain or agent.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install langchain-ollama",
      "LLM & Embeddings classes",
      "tool-calling support"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama comparable to ChatGPT UI; chat history, multi-user, model manager.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "Docker one-liner",
      "markdown & code highlight",
      "RAG uploads",
      "dark mode"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama with synchronous, asynchronous, and reactive (RxJava) APIs.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Maven Central",
      "Spring Boot starter",
      "Kotlin coroutines",
      "streaming"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sugarforever/ollama-cli",
    "summary": "Interactive REPL and CLI wrapper around Ollama with conversation persistence and syntax highlighting.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "conversation threads",
      "/save /load",
      "syntax highlighting",
      "shell completions"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "VS Code extension that brings local Ollama models into GitHub Copilot-style inline suggestions.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "inline completions",
      "FIM templates",
      "configurable model",
      "no cloud needed"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama/ollama-rag",
    "summary": "Official example repo showing retrieval-augmented generation with Ollama embeddings and ChromaDB.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "Chroma integration",
      "PDF ingestion",
      "streaming answers",
      "step-by-step notebook"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama-helm/ollama-helm",
    "summary": "Helm chart to deploy Ollama server and models on Kubernetes with GPU & autoscaling support.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "GPU node-selector",
      "model pre-load",
      "HPA",
      "PVC caching"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-ollama",
    "summary": "Haystack integration by deepset; use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "pip install haystack-ollama",
      "YAML pipeline",
      "RAG ready",
      "batch embed"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/ollama-csharp",
    "summary": ".NET client for Ollama with .NET 8, Blazor demo, and built-in streaming JSON support.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "NuGet package",
      "Blazor sample",
      "async enumerable",
      "strong typing"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/srliao/ollama-discord",
    "summary": "Discord bot that lets a server chat with local Ollama models through slash commands and threads.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "slash commands",
      "per-guild model",
      "thread isolation",
      "typing indicator"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hintergrund/ollama-obsidian",
    "summary": "Obsidian plugin that adds \u201cAsk Ollama\u201d commands for summarizing notes and Q&A over your vault.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "command palette",
      "template variables",
      "local-only",
      "custom prompts"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/shykes/ollama-dagger",
    "summary": "Dagger module to run Ollama containers in CI pipelines for testing LLM-powered features.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "Dagger 0.9",
      "CI caching",
      "GPU passthrough",
      "model pre-seed"
    ]
  },
  {
    "title": "ollama-nim",
    "url": "https://github.com/ITotalJustice/ollama-nim",
    "summary": "Nim client for Ollama with compile-time API validation and native async/await.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "Nimble package",
      "zero deps",
      "static binaries",
      "streaming"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Rust crate providing strongly-typed async client for Ollama with Tokio and SSE streaming.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "crates.io",
      "Tokio runtime",
      "Serde schemas",
      "examples folder"
    ]
  },
  {
    "title": "ollama-models",
    "url": "https://github.com/ollama/ollama-models",
    "summary": "Community-curated Modelfile collection for CodeLlama, WizardMath, Zephyr, SQLCoder, etc.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pull-request workflow",
      "quantization matrix",
      "GPU memory notes",
      "license tags"
    ]
  },
  {
    "title": "ollama-gui-react",
    "url": "https://github.com/jmorganca/ollama-gui-react",
    "summary": "Minimal React front-end for chatting with Ollama via its HTTP API; useful as a starter template.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "Vite dev",
      "Tailwind CSS",
      "streaming messages",
      "Dockerfile"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/alexkim/ollama-slack",
    "summary": "Slack bot that listens in channels/DMs and responds using a configurable local Ollama model.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "socket-mode",
      "app mentions",
      "rate limiting",
      "model hot-swap"
    ]
  }
]