[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Get up and running with Llama 3.3, Mistral, Gemma 2, and other large language models locally.",
    "source": "github",
    "date": "2024-06-10",
    "highlights": [
      "self-hosted",
      "REST/CLI",
      "Docker image",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama. Chat with local LLMs using a simple, OpenAI-compatible API.",
    "source": "github",
    "date": "2024-06-05",
    "highlights": [
      "pip install ollama",
      "async support",
      "streaming",
      "embeddings"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript / TypeScript client for Ollama. Works in Node, Deno, Bun and browsers.",
    "source": "github",
    "date": "2024-06-07",
    "highlights": [
      "npm i ollama",
      "TypeScript types",
      "streaming",
      "browser compatible"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://pypi.org/project/langchain-ollama/",
    "summary": "LangChain integration package that adds Ollama LLM and embeddings support.",
    "source": "github",
    "date": "2024-06-01",
    "highlights": [
      "pip install langchain-ollama",
      "LLM & embeddings",
      "community provider"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich, self-hosted web interface for Ollama with chat UI, model management, and RAG.",
    "source": "github",
    "date": "2024-06-09",
    "highlights": [
      "Docker",
      "RAG uploads",
      "multi-user",
      "OpenAI-compat endpoints"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Unofficial Java/Kotlin client for Ollama with fluent API and Spring Boot starters.",
    "source": "github",
    "date": "2024-05-28",
    "highlights": [
      "Maven Central",
      "Spring Boot starter",
      "Kotlin DSL",
      "async"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Community-maintained Ruby gem for interacting with the Ollama API.",
    "source": "github",
    "date": "2024-05-30",
    "highlights": [
      "gem install ollama",
      "streaming",
      "Faraday backend",
      "Rails ready"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/salty-flower/ollama-cli",
    "summary": "Cross-platform TUI (terminal UI) for chatting with Ollama models written in Rust.",
    "source": "github",
    "date": "2024-06-02",
    "highlights": [
      "Rust binary",
      "keyboard shortcuts",
      "conversation history",
      "themes"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "VS Code extension that brings local Ollama models into the editor as a GitHub-Copilot-like assistant.",
    "source": "github",
    "date": "2024-06-03",
    "highlights": [
      "VS Code marketplace",
      "inline completions",
      "custom prompts",
      "free"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration by deepset allowing Ollama models to be used as generators or embedders.",
    "source": "github",
    "date": "2024-05-25",
    "highlights": [
      "pip install ollama-haystack",
      "RAG pipelines",
      "embeddings",
      "generator"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/ollama/ollama-cookbook",
    "summary": "Community recipes and examples: function-calling, RAG, fine-tuning, Docker Compose stacks, etc.",
    "source": "github",
    "date": "2024-06-08",
    "highlights": [
      "recipes",
      "function calling",
      "RAG",
      "Docker Compose",
      "fine-tune"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Helm chart to deploy Ollama on Kubernetes with GPU and autoscaling support.",
    "source": "github",
    "date": "2024-05-29",
    "highlights": [
      "Helm chart",
      "GPU nodes",
      "HPA",
      "persistent storage",
      "Ingress"
    ]
  },
  {
    "title": "ollama-chat",
    "url": "https://github.com/jmorganca/ollama-chat",
    "summary": "Minimal React chat app example consuming Ollama\u2019s OpenAI-compatible endpoints.",
    "source": "github",
    "date": "2024-05-31",
    "highlights": [
      "React",
      "streaming",
      "OpenAI-compat",
      "Vite",
      "Dockerfile"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama/ollama-rag",
    "summary": "Turn any PDF into a locally-run RAG chatbot using Ollama + Chroma + LangChain.",
    "source": "github",
    "date": "2024-06-04",
    "highlights": [
      "PDF ingestion",
      "Chroma DB",
      "LangChain",
      "Docker",
      "Gradio UI"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama/ollama-discord",
    "summary": "Discord bot that lets servers chat with local Ollama models using slash commands.",
    "source": "github",
    "date": "2024-06-06",
    "highlights": [
      "Discord.py",
      "slash commands",
      "streaming",
      "multi-model",
      "Docker"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/ollama/ollama-dagger",
    "summary": "Dagger module to spin up Ollama as a CI service for testing LLM-powered features.",
    "source": "github",
    "date": "2024-05-27",
    "highlights": [
      "Dagger module",
      "CI service",
      "caching",
      "GPU optional",
      "Go SDK"
    ]
  },
  {
    "title": "ollama-streamlit",
    "url": "https://github.com/ollama/ollama-streamlit",
    "summary": "Streamlit chat component template that connects to Ollama with session memory and model picker.",
    "source": "github",
    "date": "2024-06-01",
    "highlights": [
      "Streamlit",
      "session state",
      "model picker",
      "streaming",
      "Docker"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/ollama/ollama-nix",
    "summary": "Nix flake providing Ollama binaries and NixOS module for declarative GPU deployments.",
    "source": "github",
    "date": "2024-05-26",
    "highlights": [
      "Nix flake",
      "NixOS module",
      "declarative",
      "GPU support",
      "cache"
    ]
  },
  {
    "title": "ollama-mistral-inspector",
    "url": "https://github.com/ollama/ollama-mistral-inspector",
    "summary": "CLI tool to inspect and quantize Mistral-family models before serving them with Ollama.",
    "source": "github",
    "date": "2024-05-24",
    "highlights": [
      "quantize",
      "inspect weights",
      "GGUF",
      "Mistral",
      "CLI"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama/ollama-rust",
    "summary": "Unofficial Rust crate providing strongly-typed async bindings to the Ollama REST API.",
    "source": "github",
    "date": "2024-06-05",
    "highlights": [
      "crates.io",
      "async/await",
      "tokio",
      "typed DTOs",
      "examples"
    ]
  }
]