[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Go-based CLI and server that downloads, builds and runs Llama 2, Mistral, Gemma and 30+ other GGUF models locally with a built-in model registry and REST API.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "self-contained binary",
      "built-in model library",
      "OpenAI-compatible API",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://pypi.org/project/ollama/",
    "summary": "Official Python client (PyPI) that wraps the Ollama HTTP API; chat, generate, embed, pull, create and delete models with a few lines of code.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "async & sync APIs",
      "streaming support",
      "embeddings endpoint",
      "zero config"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "Official npm package giving Node & browsers typed methods for every Ollama endpoint including chat, generate, pull, push and embeddings.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "TypeScript definitions",
      "ESM & CommonJS",
      "streaming generators",
      "browser compatible"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "First-party LangChain integration providing OllamaChat, OllamaLLM and OllamaEmbeddings classes so any local model can drop into LangChain chains.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "LangChain partner package",
      "supports embeddings",
      "streaming",
      "function-calling"
    ]
  },
  {
    "title": "chromadb-ollama-embedding",
    "url": "https://github.com/chroma-core/chroma/tree/main/clients/python#ollama-embedding-provider",
    "summary": "Built-in Chroma provider that uses Ollama\u2019s `/api/embeddings` endpoint to generate local embeddings for vector collections without leaving your laptop.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "local embeddings",
      "no API keys",
      "same hardware as inference"
    ]
  },
  {
    "title": "ollama-webui (ollama-web/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI (SvelteKit) that talks to your local Ollama server; offers folders, prompts, code highlighting, model manager, multi-user and dark mode.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "docker image",
      "OpenAI-compatible fallback",
      "file upload",
      "voice input"
    ]
  },
  {
    "title": "ollama-cli (jmorganca/ollama-cli)",
    "url": "https://github.com/jmorganca/ollama-cli",
    "summary": "Simple interactive REPL wrapper around the ollama binary that adds readline history, syntax highlighting and prompt templates.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "REPL",
      "colored output",
      "template shortcuts",
      "single file Go"
    ]
  },
  {
    "title": "ollama-copilot (sammcj/ollama-copilot)",
    "url": "https://github.com/sammcj/ollama-copilot",
    "summary": "GitHub Copilot-style VS-Code extension that streams code completions from a local Ollama model instead of cloud services.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "VS-Code extension",
      "streaming completions",
      "configurable model",
      "zero telemetry"
    ]
  },
  {
    "title": "ollama-helm (terrycain/ollama-helm)",
    "url": "https://github.com/terrycain/ollama-helm",
    "summary": "Community Helm chart that deploys Ollama server and optional WebUI to Kubernetes with GPU node-selector and PVC support.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "GPU scheduling",
      "persistent storage",
      "ingress",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-docker (ollama/ollama-docker)",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official multi-stage Dockerfile producing a 50 MB image with CUDA and ROCm variants for headless container deployments.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "official image",
      "CUDA/ROCm",
      "rootless",
      "multi-arch"
    ]
  },
  {
    "title": "ollama-rag (ggerganov/ollama-rag)",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Minimal Python example showing how to build a RAG pipeline with Ollama embeddings and llama-cpp-python for local question-answering over PDFs.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "RAG example",
      "PDF parsing",
      "local embeddings",
      "FAISS index"
    ]
  },
  {
    "title": "ollama-discord-bot (ciroque/ollama-discord-bot)",
    "url": "https://github.com/ciroque/ollama-discord-bot",
    "summary": "Go-based Discord bot that adds a /chat slash command backed by any Ollama model; supports per-user history and admin whitelisting.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Discord bot",
      "slash commands",
      "user sessions",
      "docker"
    ]
  },
  {
    "title": "ollama-slack-bot (ankane/ollama-slack-bot)",
    "url": "https://github.com/ankane/ollama-slack-bot",
    "summary": "Lightweight Node Slack bot that listens for @ mentions and replies using Ollama; runs serverless on Deno Deploy with zero config.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "Slack integration",
      "Deno Deploy",
      "mentions",
      "serverless"
    ]
  },
  {
    "title": "ollama-experiments (mneedham/ollama-experiments)",
    "url": "https://github.com/mneedham/ollama-experiments",
    "summary": "Collection of Jupyter notebooks exploring function-calling, JSON mode, vision models and quantisation comparisons with Ollama.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "notebooks",
      "function calling",
      "vision",
      "benchmarks"
    ]
  },
  {
    "title": "ollama-llamaindex (run-llama/llama_index)",
    "url": "https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/llms/llama-index-llms-ollama",
    "summary": "LlamaIndex LLM and embeddings modules letting you plug Ollama models into any LlamaIndex RAG or agent pipeline.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "LlamaIndex integration",
      "agents",
      "RAG",
      "streaming"
    ]
  },
  {
    "title": "ollama-haystack (deepset-ai/haystack)",
    "url": "https://github.com/deepset-ai/haystack/tree/main/integrations/ollama",
    "summary": "Haystack 2.x integration providing OllamaGenerator and OllamaTextEmbedder components for building local NLP pipelines.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Haystack nodes",
      "pipelines",
      "embeddings",
      "local"
    ]
  },
  {
    "title": "ollama-obsidian (joethei/obsidian-ollama)",
    "url": "https://github.com/joethei/obsidian-ollama",
    "summary": "Obsidian plugin that adds a \u201cOllama: Generate\u201d command to summarise notes, brainstorm ideas or continue writing using a local model.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Obsidian plugin",
      "command palette",
      "templates",
      "offline"
    ]
  },
  {
    "title": "ollama-logseq (logseq/ollama-plugin)",
    "url": "https://github.com/logseq/ollama-plugin",
    "summary": "Logseq plugin that pipes selected blocks to Ollama for summarisation, translation or Q&A and writes the response back into your graph.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Logseq",
      "block selection",
      "bidirectional",
      "graph"
    ]
  },
  {
    "title": "ollama-minecraft (theolaa/ollama-minecraft)",
    "url": "https://github.com/theolaa/ollama-minecraft",
    "summary": "Spigot plugin that adds an in-game \u201c/ollama\u201d command letting players chat with any local model from inside Minecraft.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "Minecraft plugin",
      "in-game chat",
      "Spigot",
      "offline server"
    ]
  },
  {
    "title": "ollama-ray (ray-project/ray)",
    "url": "https://github.com/ray-project/ray/tree/master/python/ray/llm/examples/ollama",
    "summary": "Ray Serve example showing how to horizontally scale Ollama models behind a single endpoint for high-throughput serving.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Ray Serve",
      "autoscaling",
      "multi-model",
      "GPU pooling"
    ]
  }
]