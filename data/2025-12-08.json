[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official CLI and Python/JS libraries to pull and run Llama 2, Mistral, Gemma, etc. locally with one command.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "self-contained binary",
      "model registry",
      "REST & gRPC APIs",
      "macOS/Linux/Windows"
    ]
  },
  {
    "title": "ollama/ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official Node.js client for Ollama\u2014stream chat, embeddings, and pull models from JavaScript/TypeScript.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "npm install ollama",
      "Promise & async-iterator APIs",
      "TypeScript definitions"
    ]
  },
  {
    "title": "ollama/ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client and asyncio bindings; pip install ollama to chat, embed, or list local models.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "sync & async clients",
      "embedding helpers",
      "colab examples"
    ]
  },
  {
    "title": "jmorganca/ollama",
    "url": "https://pypi.org/project/ollama/",
    "summary": "PyPI landing page for ollama-python; same package as the official GitHub repo.",
    "source": "pypi",
    "date": "2024-05-12",
    "highlights": [
      "pip installable",
      "0.2.x releases",
      "supports Python 3.8+"
    ]
  },
  {
    "title": "ollama/ollama",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "npm package page for ollama-js; install with npm i ollama to get TypeScript-ready client.",
    "source": "npm",
    "date": "2024-05-10",
    "highlights": [
      "weekly >30k downloads",
      "zero dependencies",
      "ESM & CJS builds"
    ]
  },
  {
    "title": "ollama-webui/ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Open-source ChatGPT-style web UI for Ollama; docker run to get a full-featured interface.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "dark/light themes",
      "multi-model chats",
      "Docker image",
      "import/export"
    ]
  },
  {
    "title": "langchain-ai/langchain",
    "url": "https://python.langchain.com/docs/integrations/llms/ollama",
    "summary": "LangChain integration page showing how to use Ollama as an LLM backend with Python and JS.",
    "source": "blog",
    "date": "2024-05-09",
    "highlights": [
      "LLM & chat interface",
      "embedding support",
      "tool-calling examples"
    ]
  },
  {
    "title": "hwchase17/langchainjs",
    "url": "https://github.com/hwchase17/langchainjs/tree/main/examples/src/models/llm/ollama",
    "summary": "LangChain.js examples directory with Ollama streaming and chat completions.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "streaming demo",
      "structured output",
      "npm @langchain/community"
    ]
  },
  {
    "title": "embedchain/embedchain",
    "url": "https://github.com/embedchain/embedchain",
    "summary": "RAG framework that ships an Ollama app\u2014ingest docs and chat locally without OpenAI.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "pip install embedchain[ollama]",
      "built-in chunking",
      "Gradio UI"
    ]
  },
  {
    "title": "continuedev/continue",
    "url": "https://github.com/continuedev/continue",
    "summary": "VS Code & JetBrains plugin that adds local code-completion via Ollama models.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "inline autocomplete",
      "/edit commands",
      "config.yaml for models"
    ]
  },
  {
    "title": "richawo/llm-chain",
    "url": "https://github.com/richawo/llm-chain",
    "summary": "Rust crate providing a high-level API to run Ollama models in async pipelines.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Rust async",
      "Serde types",
      "streaming support"
    ]
  },
  {
    "title": "thmsmlr/llm-experiments",
    "url": "https://github.com/thmsmlr/llm-experiments",
    "summary": "Curated list of notebooks benchmarking Ollama vs. other local LLM servers.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "speed benchmarks",
      "memory usage",
      "GPU vs CPU"
    ]
  },
  {
    "title": "ollama-models/community",
    "url": "https://github.com/ollama-models",
    "summary": "Unofficial GitHub org hosting custom Modelfiles for CodeLlama, Zephyr, SQLCoder, etc.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Modelfile recipes",
      "quantization tips",
      "pull requests welcome"
    ]
  },
  {
    "title": "r/ollama",
    "url": "https://www.reddit.com/r/ollama/",
    "summary": "Subreddit with 9k+ members sharing tips, custom models, and UI projects for Ollama.",
    "source": "reddit",
    "date": "2024-05-14",
    "highlights": [
      "weekly Q&A",
      "showcase flair",
      "troubleshooting guides"
    ]
  },
  {
    "title": "Hacker News: Show HN: Ollama \u2013 Run Llama 2 locally with one command",
    "url": "https://news.ycombinator.com/item?id=36937834",
    "summary": "Launch thread with 400+ comments discussing performance, licensing, and GPU setup.",
    "source": "hackernews",
    "date": "2023-07-21",
    "highlights": [
      "M1/M2 benchmarks",
      "docker tips",
      "VRAM requirements"
    ]
  },
  {
    "title": "microsoft/autogen",
    "url": "https://github.com/microsoft/autogen/blob/main/notebook/agentchat_ollama.ipynb",
    "summary": "Microsoft AutoGen notebook demonstrating multi-agent conversations backed by Ollama.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "agent orchestration",
      "local models",
      "group chat example"
    ]
  },
  {
    "title": "AnubhavMadhav/ollama-spring-boot-starter",
    "url": "https://github.com/AnubhavMadhav/ollama-spring-boot-starter",
    "summary": "Spring Boot starter that autoconfigures Ollama REST client for Java/Kotlin apps.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "@OllamaTemplate",
      "auto-configuration",
      "Spring AI compatible"
    ]
  },
  {
    "title": "sugarforever/ollama-chat",
    "url": "https://github.com/sugarforever/ollama-chat",
    "summary": "Next.js chat app using ollama-js and Tailwind; deploy to Vercel in minutes.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "streaming UI",
      "chat history",
      "Dockerfile included"
    ]
  },
  {
    "title": "ollama/ollama-main",
    "url": "https://hub.docker.com/r/ollama/ollama",
    "summary": "Official Docker Hub page with multi-arch images for CPU and GPU (cuda) usage.",
    "source": "blog",
    "date": "2024-05-13",
    "highlights": [
      "docker-compose examples",
      "CUDA tags",
      "volume mounts"
    ]
  },
  {
    "title": "ggerganov/llama.cpp",
    "url": "https://github.com/ggerganov/llama.cpp/pull/1827",
    "summary": "PR discussion where llama.cpp added Ollama-compatible server mode for easy swapping.",
    "source": "github",
    "date": "2023-09-15",
    "highlights": [
      "same API surface",
      "binary size comparison",
      "community benchmarks"
    ]
  }
]