[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repository providing the CLI and server to pull, run, and manage large language models locally.",
    "source": "github",
    "date": "2023-06-26",
    "highlights": [
      "self-hosted LLM runtime",
      "Docker & binary distributions",
      "REST & Go API",
      "GGUF/GGML model support"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama API, usable in Node.js and browsers.",
    "source": "github",
    "date": "2023-10-11",
    "highlights": [
      "npm install ollama",
      "Promise-based API",
      "streaming responses",
      "TypeScript definitions"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client for Ollama; chat, generate, embed, pull, and manage models programmatically.",
    "source": "github",
    "date": "2023-10-11",
    "highlights": [
      "pip install ollama",
      "sync & async APIs",
      "embeddings support",
      "model management helpers"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package letting you use Ollama models as LLMs, chat, or embeddings inside LangChain pipelines.",
    "source": "github",
    "date": "2023-12-01",
    "highlights": [
      "pip install langchain-ollama",
      "unified LangChain interface",
      "streaming",
      "embeddings"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (chat, model management, multi-user, OpenAI-compatible endpoints).",
    "source": "github",
    "date": "2023-10-15",
    "highlights": [
      "Docker image available",
      "dark/light themes",
      "code syntax highlighting",
      "modelfile editor"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama providing sync/async APIs and Spring Boot starters.",
    "source": "github",
    "date": "2023-11-20",
    "highlights": [
      "Maven Central",
      "Spring Boot starter",
      "async streaming",
      "Kotlin extensions"
    ]
  },
  {
    "title": "chatd",
    "url": "https://github.com/Bin-Huang/chatd",
    "summary": "Lightweight desktop chat app that talks to any Ollama model via the local REST API.",
    "source": "github",
    "date": "2023-11-05",
    "highlights": [
      "Tauri-based",
      "cross-platform binaries",
      "auto-detects Ollama endpoint",
      "markdown support"
    ]
  },
  {
    "title": "ollama-models",
    "url": "https://github.com/jmorganca/ollama-models",
    "summary": "Community-curated Modelfiles for dozens of open-source LLMs ready to import into Ollama.",
    "source": "github",
    "date": "2023-09-18",
    "highlights": [
      "Llama 2, Mistral, CodeLlama, etc.",
      "one-click import",
      "parameter hints",
      "quantization options"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Minimal retrieval-augmented-generation example using Ollama embeddings and a local vector DB.",
    "source": "github",
    "date": "2023-12-03",
    "highlights": [
      "pure Python",
      "ChromaDB",
      "streaming answers",
      "easy PDF ingestion"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/salty-flower/ollama-cli",
    "summary": "Rust-based interactive CLI client with REPL, syntax highlighting, and command history.",
    "source": "github",
    "date": "2023-11-10",
    "highlights": [
      "Cargo install",
      "REPL mode",
      "ctrl-c graceful exit",
      "themes"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "VS Code extension that turns any Ollama model into a GitHub-Copilot-like inline helper.",
    "source": "github",
    "date": "2023-11-25",
    "highlights": [
      "inline completions",
      "configurable model",
      "status-bar indicator",
      "FIM templates"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/NixOS/nixpkgs/tree/master/pkgs/tools/misc/ollama",
    "summary": "Nix package and NixOS module for installing and running the Ollama server declaratively.",
    "source": "github",
    "date": "2023-10-30",
    "highlights": [
      "declarative GPU passthrough",
      "systemd service",
      "automatic model caching"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Helm chart to deploy Ollama on Kubernetes with optional GPU nodes and PVC model cache.",
    "source": "github",
    "date": "2023-11-12",
    "highlights": [
      "GPU scheduling",
      "PVC cache",
      "ingress support",
      "autoscaling"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/OllamaSharp",
    "summary": ".NET client library for Ollama with async streaming, chat, and embeddings support.",
    "source": "github",
    "date": "2023-11-28",
    "highlights": [
      "NuGet package",
      "async enumerable streaming",
      "strong naming",
      "DI-friendly"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/shykes/dagger-ollama",
    "summary": "Dagger module that spins up an Ollama service inside a CI pipeline for testing LLM features.",
    "source": "github",
    "date": "2023-12-05",
    "highlights": [
      "Dagger 0.9+",
      "cached model layers",
      "GitHub Actions example",
      "GPU optional"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rust",
    "summary": "Community Rust crate providing typed async bindings to the Ollama REST API.",
    "source": "github",
    "date": "2023-11-15",
    "highlights": [
      "crates.io",
      "tokio runtime",
      "serde schemas",
      "examples folder"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/iamashley/ollama-discord",
    "summary": "Self-hostable Discord bot that lets users chat with any Ollama model via slash commands.",
    "source": "github",
    "date": "2023-11-18",
    "highlights": [
      "slash commands",
      "per-guild model config",
      "typing indicator",
      "Docker image"
    ]
  },
  {
    "title": "ollama-slack",
    "url": "https://github.com/nerdneha/ollama-slack",
    "summary": "Slack Bolt app that brings Ollama models into channels with thread-aware conversations.",
    "source": "github",
    "date": "2023-11-22",
    "highlights": [
      "Socket Mode",
      "threading support",
      "app mentions",
      "rate limiting"
    ]
  },
  {
    "title": "ollama-elixir",
    "url": "https://github.com/valiantrabbit/ollama-elixir",
    "summary": "Elixir OTP client for Ollama with GenServer-backed connection pooling and streaming.",
    "source": "github",
    "date": "2023-11-30",
    "highlights": [
      "Hex package",
      "GenServer pool",
      "LiveView examples",
      "telemetry events"
    ]
  },
  {
    "title": "ollama-rustic",
    "url": "https://github.com/mdrokz/ollama-rustic",
    "summary": "Minimal Rust CLI to download and run Ollama models without Docker (static binary).",
    "source": "github",
    "date": "2023-12-01",
    "highlights": [
      "single static binary",
      "no runtime deps",
      "GPU auto-detection",
      "model caching"
    ]
  }
]