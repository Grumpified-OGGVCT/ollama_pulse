[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: get up and running with Llama 2, Mistral, Gemma, and other large language models locally.",
    "source": "github",
    "date": "2024-05-14",
    "highlights": [
      "self-contained LLM runner",
      "macOS/Linux/Windows binaries",
      "pull & run quantized models"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama: chat, generate, embed, pull, and manage local models with a few lines of code.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip install ollama",
      "async/await support",
      "built-in embedding helpers"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node and browsers; same API surface as the Python SDK.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "npm install ollama",
      "TypeScript definitions",
      "streaming responses"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/ollama",
    "summary": "LangChain integration package letting you swap Ollama models into any LangChain chain or agent.",
    "source": "github",
    "date": "2024-05-13",
    "highlights": [
      "pip install langchain-ollama",
      "ChatOllama & OllamaLLM classes",
      "callback support"
    ]
  },
  {
    "title": "Ollama Web UI",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI for Ollama (ChatGPT-style interface) with multi-user support, model manager, and REST proxy.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "Docker one-liner",
      "dark/light themes",
      "share conversation links"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama exposing synchronous and asynchronous APIs, embeddings, and model management.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Maven Central",
      "Spring Boot starters",
      "Kotlin coroutines"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Community-maintained Ruby gem for chatting, generating, and pulling models from an Ollama instance.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "gem install ollama",
      "Faraday HTTP backend",
      "streaming blocks"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sugarforever/ollama-cli",
    "summary": "Rust-based interactive CLI that wraps Ollama with readline history, syntax highlighting, and prompt templates.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "cargo install ollama-cli",
      "cross-platform",
      "configurable prompts"
    ]
  },
  {
    "title": "Ollama Copilot",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "VS Code extension bringing local Ollama models into inline suggestions and chat sidebar.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "open-source Copilot alternative",
      "inline completions",
      "custom model picker"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/llama-magic/ollama-cookbook",
    "summary": "Community recipes for running Ollama on Raspberry Pi, Kubernetes, AWS Lambda, and integrating with Home-Assistant.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "ARM builds",
      "Helm charts",
      "serverless functions"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-ollama",
    "summary": "Haystack integration to use Ollama models as generators or embedders in production RAG pipelines.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "pip install ollama-haystack",
      "native generator & embedder nodes",
      "batch support"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama-docker",
    "summary": "Official multi-arch Docker image with GPU (CUDA/ROCm) and CPU variants plus docker-compose examples.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "single-command GPU runtime",
      "compose stacks",
      "volume caching"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama-helm/ollama-helm",
    "summary": "Helm chart for deploying Ollama on Kubernetes with autoscaling, PVC model cache, and optional Ingress.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "GPU node selector",
      "HPA support",
      "configurable probes"
    ]
  },
  {
    "title": "Ollama Reddit Community",
    "url": "https://www.reddit.com/r/ollama/",
    "summary": "Active subreddit for sharing models, troubleshooting, and showcasing Ollama-based projects.",
    "source": "reddit",
    "date": "2024-05-14",
    "highlights": [
      "model recommendations",
      "performance tuning",
      "show-and-tell threads"
    ]
  },
  {
    "title": "Ollama on Hacker News",
    "url": "https://news.ycombinator.com/item?id=40012345",
    "summary": "Discussion thread covering Ollama\u2019s MIT license, performance benchmarks vs. llama.cpp, and roadmap.",
    "source": "hackernews",
    "date": "2024-05-01",
    "highlights": [
      "community benchmarks",
      "roadmap hints",
      "comparisons to llama.cpp"
    ]
  },
  {
    "title": "ollama-npm",
    "url": "https://www.npmjs.com/package/ollama",
    "summary": "Official npm package providing TypeScript-first client for Node and browsers with full Ollama REST coverage.",
    "source": "npm",
    "date": "2024-05-12",
    "highlights": [
      "zero dependencies",
      "ESM & CJS bundles",
      "typedoc docs"
    ]
  },
  {
    "title": "ollama on PyPI",
    "url": "https://pypi.org/project/ollama/",
    "summary": "Official Python package offering sync/async clients, embeddings, and model management for Ollama.",
    "source": "pypi",
    "date": "2024-05-10",
    "highlights": [
      ">=3.8 support",
      "Pydantic models",
      "built-in retry logic"
    ]
  },
  {
    "title": "ollama-streamlit",
    "url": "https://github.com/talk2much/ollama-streamlit",
    "summary": "Streamlit chat app that proxies to Ollama with session memory, markdown rendering, and model switcher.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "pip install streamlit-ollama",
      "session state memory",
      "easy theming"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/ollama-discord/ollama-discord",
    "summary": "Self-hostable Discord bot that brings local Ollama models into any server with slash commands and moderation hooks.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "Docker image",
      "role-based access",
      "conversation threads"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/hintergrund/ollama-obsidian",
    "summary": "Obsidian plugin for running local LLM prompts inside notes, auto-summarizing, and Q&A over your vault.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "Templater integration",
      "offline operation",
      "custom prompt files"
    ]
  }
]