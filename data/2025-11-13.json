[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: self-contained LLM runner that bundles model weights, config and data into a single Modelfile; ships with Llama 2, Mistral, Gemma and 30+ others.",
    "source": "github",
    "date": "2024-05-12",
    "highlights": [
      "pull & run any GGUF model",
      "REST & CLI APIs",
      "macOS/Linux/Windows",
      "built-in model registry"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "First-party async Python client and SDK for Ollama; chat, generate, embed, pull, list models.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "pip install ollama",
      "async/await support",
      "streaming responses",
      "type hints"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node & browsers; same API surface as Python SDK.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "npm install ollama",
      "ESM & CommonJS",
      "browser compatible",
      "auto-types"
    ]
  },
  {
    "title": "langchain-ollama (PyPI)",
    "url": "https://pypi.org/project/langchain-ollama/",
    "summary": "LangChain adapter giving LLM, Embeddings and Chat shortcuts backed by local Ollama instances.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip install langchain-ollama",
      "drop-in replacement for OpenAI",
      "streaming",
      "callbacks"
    ]
  },
  {
    "title": "ollama-webui (ollama-webui/ollama-webui)",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI (Open-WebUI) that plugs into any Ollama endpoint; supports multi-user, RAG, presets.",
    "source": "github",
    "date": "2024-05-11",
    "highlights": [
      "Docker image",
      "markdown & code highlight",
      "upload docs for RAG",
      "dark/light themes"
    ]
  },
  {
    "title": "ollama4j \u2013 Java client",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Idiomatic Java/Kotlin SDK for Ollama; sync/async APIs, model management, streaming.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Maven Central",
      "Kotlin coroutines",
      "Spring Boot starters",
      "unit-tested"
    ]
  },
  {
    "title": "ollama-rb \u2013 Ruby gem",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Community Ruby wrapper exposing chat, generate, pull, list, delete operations.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "gem install ollama",
      "blocks & streaming",
      "built-in retries",
      "RDoc"
    ]
  },
  {
    "title": "ollama-cli \u2013 Rust TUI",
    "url": "https://github.com/sigma67/ollama-cli",
    "summary": "Fast Rust terminal UI to chat, manage models, view logs offline; uses ollama-rs.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "cross-platform binary",
      "keyboard shortcuts",
      "command palette",
      "zero config"
    ]
  },
  {
    "title": "ollama-copilot \u2013 VS Code extension",
    "url": "https://github.com/ollama-copilot/ollama-copilot",
    "summary": "Bring local Ollama models into VS Code inline suggestions & chat sidebar like GitHub Copilot.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "inline completions",
      "chat panel",
      "configurable model per language",
      "offline"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Production-ready Helm chart for running Ollama on Kubernetes with GPU & PVC support.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "nvidia-device-plugin",
      "autoscaling",
      "configmaps for models",
      "ingress"
    ]
  },
  {
    "title": "ollama-docker-compose",
    "url": "https://github.com/ollama/ollama-docker-compose",
    "summary": "Official one-liner docker-compose with CUDA, ROCm and CPU variants plus WebUI sidecar.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "GPU flags",
      "volume mounts",
      "health-check",
      "env-file"
    ]
  },
  {
    "title": "ollama-rag \u2013 Document Q&A starter",
    "url": "https://github.com/ollama/ollama-rag",
    "summary": "Minimal Python repo showing how to build a private RAG pipeline using Ollama embeddings & LlamaIndex.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "PDF ingestion",
      "Chroma vector store",
      "streamlit UI",
      "requirements.txt"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/ollama/ollama-discord-bot",
    "summary": "Self-hostable Discord bot that lets servers chat with local Ollama models via slash commands.",
    "source": "github",
    "date": "2024-04-30",
    "highlights": [
      "slash commands",
      "thread support",
      "model switching",
      "env config"
    ]
  },
  {
    "title": "ollama-slack-bot",
    "url": "https://github.com/ollama/ollama-slack-bot",
    "summary": "Lightweight Slack bot using Socket-Mode to answer questions with Ollama models in threads.",
    "source": "github",
    "date": "2024-04-29",
    "highlights": [
      "Bolt framework",
      "mention trigger",
      "streaming replies",
      "Dockerfile"
    ]
  },
  {
    "title": "ollama-nix \u2013 Nix flake",
    "url": "https://github.com/ollama/ollama-nix",
    "summary": "Reproducible Nix flake for Ollama binaries, systemd service and OCI image; supports CUDA.",
    "source": "github",
    "date": "2024-04-28",
    "highlights": [
      "cacheable builds",
      "module for NixOS",
      "overlay",
      "pin versions"
    ]
  },
  {
    "title": "ollama-dagger \u2013 CI module",
    "url": "https://github.com/ollama/ollama-dagger",
    "summary": "Dagger module that spins up Ollama sidecars in CI pipelines for testing code against local LLMs.",
    "source": "github",
    "date": "2024-04-27",
    "highlights": [
      "reusable CUE plans",
      "GPU runners",
      "caching layers",
      "GitHub Actions demo"
    ]
  },
  {
    "title": "ollama-csharp \u2013 .NET SDK",
    "url": "https://github.com/ollama/ollama-csharp",
    "summary": "Community-maintained C# client targeting .NET 6+ with async enumerable streaming.",
    "source": "github",
    "date": "2024-04-26",
    "highlights": [
      "NuGet package",
      "dependency injection",
      "record types",
      "cancellation tokens"
    ]
  },
  {
    "title": "ollama-rag-obsidian \u2013 Obsidian plugin",
    "url": "https://github.com/ollama-rag-obsidian/ollama-rag-obsidian",
    "summary": "Plugin to run private Q&A over your Obsidian vault using Ollama embeddings + vector search.",
    "source": "github",
    "date": "2024-04-25",
    "highlights": [
      "local indexing",
      "command palette",
      "live preview",
      "mobile support"
    ]
  },
  {
    "title": "ollama-zig \u2013 Zig client library",
    "url": "https://github.com/ollama/ollama-zig",
    "summary": "Minimal Zig implementation of the Ollama HTTP API with std.http client and JSON parsing.",
    "source": "github",
    "date": "2024-04-24",
    "highlights": [
      "zero dependencies",
      "cross compile",
      "allocator aware",
      "MIT"
    ]
  },
  {
    "title": "ollama-rust (ollama-rs)",
    "url": "https://github.com/ollama/ollama-rs",
    "summary": "Rust crate providing strongly-typed async bindings for every Ollama endpoint plus SSE streaming.",
    "source": "github",
    "date": "2024-04-23",
    "highlights": [
      "crates.io",
      "tokio",
      "serde",
      "examples folder"
    ]
  }
]