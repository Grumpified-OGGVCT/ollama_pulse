[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "Official Ollama repo: self-hosted LLM runner with a built-in model library and simple CLI/API for running Llama 2, Mistral, Gemma, etc. on CPU/GPU.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "built-in model hub",
      "REST & Go API",
      "macOS/Linux/Windows",
      "one-line install"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python SDK for Ollama: sync/async clients, streaming, chat & embed endpoints, PyPI package kept in lock-step with core releases.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "asyncio support",
      "embeddings",
      "chat interface",
      "PyPI: ollama"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for Node & browsers; npm package ollama offers typed promises and streaming helpers.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "TypeScript",
      "streaming",
      "npm: ollama",
      "browser & node"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration providing Ollama LLM & embeddings components; install via langchain-ollama PyPI package.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "LangChain LLM",
      "embeddings",
      "PyPI: langchain-ollama"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich chat WebUI (formerly Ollama-WebUI) that plugs into local Ollama; offers folders, sharing, multimodal, RAG, admin panel.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "dark/light themes",
      "RAG uploads",
      "user roles",
      "Docker image"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama with fluent builders, streaming, chat, embeddings, model management; Maven Central artifact.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "Java fluent API",
      "streaming",
      "Maven Central",
      "Android ready"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Community Ruby gem wrapping Ollama REST endpoints; supports chat, generate, pull, delete, streaming.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Ruby gem",
      "streaming",
      "REST wrapper"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/sugarforever/ollama-cli",
    "summary": "Interactive CLI tool for chatting, pulling, and managing Ollama models with conversation history and markdown rendering.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "interactive chat",
      "history",
      "markdown",
      "npm global install"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "VS Code extension that turns local Ollama models into inline coding copilots with autocomplete and explain features.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "VS Code",
      "inline suggestions",
      "local copilot",
      "open-source"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/deepset-ai/haystack-integrations/tree/main/integrations/ollama",
    "summary": "Haystack integration by deepset-ai: use Ollama models as generators or embedders in Haystack pipelines.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Haystack nodes",
      "RAG pipelines",
      "embeddings"
    ]
  },
  {
    "title": "ollama-camunda",
    "url": "https://github.com/camunda-community-hub/ollama-camunda",
    "summary": "Community Camunda 8 connector that invokes Ollama models inside BPMN processes for human-task summarization, classification, etc.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "Camunda 8",
      "BPMN",
      "serverless connector"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama/tree/main/docker",
    "summary": "Official Docker image and compose samples for running Ollama server + GPU support (nvidia/cuda) and CPU-only variants.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "official image",
      "CUDA",
      "docker-compose",
      "ARM64/x86"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/otwld/ollama-helm",
    "summary": "Community Helm chart for deploying Ollama on Kubernetes with autoscaling, PVC, and GPU node-selector support.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Kubernetes",
      "Helm",
      "GPU nodes",
      "autoscale"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ggerganov/ollama-rag",
    "summary": "Lightweight RAG example using Ollama embeddings + Chroma vector DB and Streamlit UI to chat over your documents locally.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "RAG",
      "Chroma",
      "Streamlit",
      "local only"
    ]
  },
  {
    "title": "ollama-discord",
    "url": "https://github.com/solopasha/ollama-discord",
    "summary": "Self-host Discord bot that answers prompts using local Ollama models; supports slash commands, streaming, and model switching.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "Discord bot",
      "slash commands",
      "streaming",
      "role-based access"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/NixOS/nixpkgs/tree/master/pkgs/tools/misc/ollama",
    "summary": "Nix package and NixOS module for Ollama with automatic GPU driver configuration and systemd service.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "NixOS",
      "systemd",
      "GPU auto-detect",
      "declarative config"
    ]
  },
  {
    "title": "ollama-dagger",
    "url": "https://github.com/shykes/dagger-ollama",
    "summary": "Dagger module that spins up Ollama as a CI-sidecar for testing or generating docs using local LLMs inside pipelines.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "Dagger CI",
      "sidecar",
      "local LLM",
      "caching"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/pepperoni21/ollama-rs",
    "summary": "Rust crate providing strongly-typed async client for Ollama with chat, generate, pull, and embeddings endpoints.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "Rust",
      "async",
      "crates.io",
      "typed API"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/awaescher/OllamaSharp",
    "summary": ".NET library and CLI for Ollama with async streaming, chat, model management, and dependency injection helpers.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      ".NET",
      "NuGet",
      "streaming",
      "DI ready"
    ]
  },
  {
    "title": "ollama-model-library",
    "url": "https://github.com/ollama/ollama/tree/main/models",
    "summary": "Curated collection of Modelfile recipes (Llama 3, Phi-3, CodeLlama, etc.) maintained by Ollama team for one-click local runs.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "Modelfiles",
      "Llama 3",
      "CodeLlama",
      "community PRs"
    ]
  }
]