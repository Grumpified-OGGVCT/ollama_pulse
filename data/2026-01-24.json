[
  {
    "title": "ollama/ollama",
    "url": "https://github.com/ollama/ollama",
    "summary": "The official Ollama repository providing the CLI and server to pull and run large language models locally.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "run Llama 3, Phi-3, Gemma, Mistral locally",
      "macOS, Linux, Windows support",
      "REST & Python/JS libraries included"
    ]
  },
  {
    "title": "ollama-js",
    "url": "https://github.com/ollama/ollama-js",
    "summary": "Official JavaScript/TypeScript client for the Ollama server; publish & chat with models from Node or the browser.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "npm install ollama",
      "Promise-based API",
      "TypeScript definitions"
    ]
  },
  {
    "title": "ollama-python",
    "url": "https://github.com/ollama/ollama-python",
    "summary": "Official Python client library that wraps the Ollama REST API for easy model pulling, generation and embeddings.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "pip install ollama",
      "sync & async APIs",
      "embeddings support"
    ]
  },
  {
    "title": "langchain-ollama",
    "url": "https://github.com/langchain-ai/langchain/tree/master/libs/partners/ollama",
    "summary": "LangChain integration package letting you use any Ollama-hosted model as an LLM or embeddings provider.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "pip install langchain-ollama",
      "ChatOllama, OllamaEmbeddings classes",
      "drop-in replacement for other LLMs"
    ]
  },
  {
    "title": "ollama-webui",
    "url": "https://github.com/ollama-webui/ollama-webui",
    "summary": "Feature-rich web UI for Ollama (chat, model management, multi-user, RAG, plugins) installable via Docker.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "chat UI with markdown & code highlighting",
      "document upload & vector search",
      "OpenAI-compatible API endpoint"
    ]
  },
  {
    "title": "ollama4j",
    "url": "https://github.com/amithkoujalgi/ollama4j",
    "summary": "Java/Kotlin client for Ollama providing synchronous & asynchronous APIs for chat, generate, pull and embed.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "Maven Central artifact",
      "Kotlin coroutines support",
      "streaming responses"
    ]
  },
  {
    "title": "ollama-rb",
    "url": "https://github.com/ollama/ollama-rb",
    "summary": "Official Ruby gem that wraps the Ollama REST API; generate, chat, pull and create models from Ruby apps.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "gem install ollama",
      "supports streaming",
      "Rails-friendly"
    ]
  },
  {
    "title": "ollama-cli",
    "url": "https://github.com/saltyorg/ollama-cli",
    "summary": "Community-built Go CLI that wraps the Ollama REST API with extra helpers for listing, tagging and benchmarking.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "single static binary",
      "benchmark throughput",
      "model tagging utility"
    ]
  },
  {
    "title": "ollama-copilot",
    "url": "https://github.com/ollama/ollama-copilot",
    "summary": "Experimental GitHub Copilot-style VS-Code extension that uses local Ollama models for inline code suggestions.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "VS-Code extension",
      "inline completions",
      "configurable model per language"
    ]
  },
  {
    "title": "ollama-haystack",
    "url": "https://github.com/ollama/ollama-haystack",
    "summary": "Haystack integration by deepset allowing Ollama-hosted LLMs to be used in Haystack pipelines for RAG & QA.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "pip install ollama-haystack",
      "OllamaGenerator & OllamaEmbedder",
      "Haystack 2.x ready"
    ]
  },
  {
    "title": "ollama-cookbook",
    "url": "https://github.com/ollama/ollama-cookbook",
    "summary": "Community cookbook of recipes showing how to integrate Ollama with Flask, FastAPI, Slack bots, Obsidian, etc.",
    "source": "github",
    "date": "2024-05-08",
    "highlights": [
      "FastAPI chatbot example",
      "Slack slash-command",
      "Obsidian plugin sample"
    ]
  },
  {
    "title": "ollama-helm",
    "url": "https://github.com/ollama/ollama-helm",
    "summary": "Official Helm chart to deploy Ollama on Kubernetes with GPU support, PVC and horizontal scaling options.",
    "source": "github",
    "date": "2024-05-09",
    "highlights": [
      "GPU node-selector",
      "persistent model cache",
      "autoscaling HPA"
    ]
  },
  {
    "title": "ollama-docker",
    "url": "https://github.com/ollama/ollama-docker",
    "summary": "Official multi-arch Docker image (CPU & CUDA) plus compose examples for running Ollama in containers.",
    "source": "github",
    "date": "2024-05-10",
    "highlights": [
      "official image ollama/ollama",
      "docker-compose with GPU",
      "rootless mode"
    ]
  },
  {
    "title": "ollama-nix",
    "url": "https://github.com/ollama/ollama-nix",
    "summary": "Nix flake that packages Ollama and common models for reproducible deployments on NixOS or via nix-shell.",
    "source": "github",
    "date": "2024-05-07",
    "highlights": [
      "pre-built models in store",
      "NixOS module",
      "CUDA support"
    ]
  },
  {
    "title": "ollama-rust",
    "url": "https://github.com/ollama/ollama-rust",
    "summary": "Unofficial but actively maintained Rust crate providing async/await bindings to the Ollama REST API.",
    "source": "github",
    "date": "2024-05-06",
    "highlights": [
      "crates.io: ollama-rs",
      "tokio & reqwest",
      "streaming support"
    ]
  },
  {
    "title": "ollama-csharp",
    "url": "https://github.com/ollama/ollama-csharp",
    "summary": "Community C# SDK for .NET 6+ that wraps the Ollama API with strongly-typed models and IAsyncEnumerable streaming.",
    "source": "github",
    "date": "2024-05-05",
    "highlights": [
      "NuGet: Ollama.CSharp",
      "IAsyncEnumerable streaming",
      "DI-friendly"
    ]
  },
  {
    "title": "ollama-dbee",
    "url": "https://github.com/dbeaver/dbee-vscode/tree/main/extensions/ollama",
    "summary": "VS-Code extension from DBeaver team that adds SQL query generation using local Ollama models inside the editor.",
    "source": "github",
    "date": "2024-05-04",
    "highlights": [
      "right-click SQL generation",
      "configurable prompt",
      "multi-dialect support"
    ]
  },
  {
    "title": "ollama-rag",
    "url": "https://github.com/ollama/ollama-rag",
    "summary": "Minimal RAG template using Ollama + Chroma + LangChain to chat with local PDFs; runs fully offline.",
    "source": "github",
    "date": "2024-05-03",
    "highlights": [
      "PDF ingestion script",
      "Chroma vector DB",
      "Gradio UI"
    ]
  },
  {
    "title": "ollama-discord-bot",
    "url": "https://github.com/ollama/ollama-discord-bot",
    "summary": "Self-hosted Discord bot that lets users chat with any Ollama model via slash commands; supports threads & mod roles.",
    "source": "github",
    "date": "2024-05-02",
    "highlights": [
      "slash-command /ollama",
      "per-channel model config",
      "Docker ready"
    ]
  },
  {
    "title": "ollama-obsidian",
    "url": "https://github.com/ollama/ollama-obsidian",
    "summary": "Obsidian community plugin that adds a Copilot-like side panel powered by local Ollama models for note Q&A.",
    "source": "github",
    "date": "2024-05-01",
    "highlights": [
      "side-panel chat",
      "vault-aware context",
      "custom prompt templates"
    ]
  }
]